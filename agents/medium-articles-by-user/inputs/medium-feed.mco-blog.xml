<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Marc Cohen on Medium]]></title>
        <description><![CDATA[Stories by Marc Cohen on Medium]]></description>
        <link>https://medium.com/@mco-blog?source=rss-14a828f24ca2------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*uy9EUnfgReMHwBs7WBb3Jg.jpeg</url>
            <title>Stories by Marc Cohen on Medium</title>
            <link>https://medium.com/@mco-blog?source=rss-14a828f24ca2------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 15:27:42 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@mco-blog/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Colab Data Visualizations Made Easy]]></title>
            <link>https://medium.com/google-colab/colab-data-visualizations-made-easy-5e1918e5234e?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/5e1918e5234e</guid>
            <category><![CDATA[matplotlib]]></category>
            <category><![CDATA[data-visualization]]></category>
            <category><![CDATA[colab]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Tue, 18 Jul 2023 16:40:52 GMT</pubDate>
            <atom:updated>2023-07-18T16:40:52.728Z</atom:updated>
            <content:encoded><![CDATA[<iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FnW4xGtbQ41E%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DnW4xGtbQ41E&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FnW4xGtbQ41E%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/9ebff7a2cb35faac7a83cd839708b013/href">https://medium.com/media/9ebff7a2cb35faac7a83cd839708b013/href</a></iframe><p>If you’ve never found visualizing your data challenging, you can stop reading now. For everyone else, the Google Colab team is excited to announce automated generation of plots from Pandas DataFrames! One click gets you a palette of suggested plots for your DataFrame, and clicking on any plot inserts the corresponding code into your notebook.</p><p>To get the auto-plotting button to display, run a code cell which finishes with a line that lists an existing DataFrame, which will then display this icon on the bottom right of your cell output:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/100/1*FapK0kzNda6mB09LqCxi-Q.png" /></figure><p>If you find a generated plot useful, the auto-plotting feature can provide the the supporting code and even insert it directly into a new cell in your notebook, allowing you to save and configure the plot to your liking.</p><p>Here’s a <a href="https://colab.research.google.com/drive/1ah_R0EOXmTd1969ut-2A5iRw0y2bPvcu?usp=sharing">sample notebook</a> you can use to try the feature for yourself. Let us know how you’re using this feature in the comments below and keep on visualizing!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5e1918e5234e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/colab-data-visualizations-made-easy-5e1918e5234e">Colab Data Visualizations Made Easy</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Two New Ways to Manage Cell Execution]]></title>
            <link>https://medium.com/google-colab/two-new-ways-to-manage-cell-execution-fbad61b40882?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/fbad61b40882</guid>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Thu, 06 Jul 2023 14:24:34 GMT</pubDate>
            <atom:updated>2023-07-06T14:24:34.267Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/970/1*qCv__p3EsHG_DZSvmLeRZQ.png" /></figure><p>We’ve recently introduced two new features to make it easier to control how and when to run your code cells:</p><ol><li>The first feature gives you the ability to run a collection of cells directly from the notebook table of contents, via a “Run cells in section” option in the context menu (under the triple dots), as shown in the image above.</li><li>The second feature gives you the ability to assign certain cells as pre-requisites for a given notebook. This is manifested via an option in the notebook settings dialog (under the Edit menu) to “Automatically run the first cell or section” when opening a notebook (see image below). If your notebook starts with a single cell, that cell will be auto-executed whenever your notebook is opened. If the notebook starts with a section, the entire first section of cells will be auto-executed on notebook open.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*36XCOLCwYdLIzcDRuibxIA.png" /></figure><p>Hope you find these useful — let us know in the comments how you’re using them!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fbad61b40882" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/two-new-ways-to-manage-cell-execution-fbad61b40882">Two New Ways to Manage Cell Execution</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Noteworthy Notebooks #3 — Analyzing a Bank Failure with Colab]]></title>
            <link>https://medium.com/google-colab/noteworthy-notebooks-3-analyzing-a-bank-failure-with-colab-d23b372de313?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/d23b372de313</guid>
            <category><![CDATA[economics]]></category>
            <category><![CDATA[colab]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Mon, 08 May 2023 18:11:25 GMT</pubDate>
            <atom:updated>2023-05-08T18:13:59.470Z</atom:updated>
            <content:encoded><![CDATA[<h3>Noteworthy Notebooks #3 — Analyzing a Bank Failure with Colab</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*73G6L52YqR6vgz3K" /><figcaption>Photo by <a href="https://unsplash.com/@wildbook?utm_source=medium&amp;utm_medium=referral">Dmitry Demidko</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>A well crafted notebook is a kind of living textbook and this is a great example. Professor <a href="https://www.linkedin.com/in/ashwin2rao/">Ashwin Rao</a> of Stanford University has authored a fascinating Colab notebook explaining, at a high level, why Silicon Valley Bank (SVB) failed.</p><p>Due to the nature of it’s clientele (largely tech startups), SVB enjoyed a strong cash position and invested much of that portfolio in short and long term bonds. Unfortunately, with post-pandemic rising interest rates, bond prices plunged, drastically reducing the value of those investments while, at the same time, increasing interest rates the bank owed to consumers and institutions <em>above</em> the level they earned on their own investments.</p><p>There were other factors involved — Dr. Rao’s analysis is limited to the economic and pricing dynamics underlying this story. <strong>This notebook will teach you how bonds work and why, in the investment world, there’s no such thing as a sure thing.</strong></p><p><a href="https://colab.sandbox.google.com/drive/15uxrAeCCL327kWH9N0X-ogKwf2zErjP5">Check out the notebook here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d23b372de313" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/noteworthy-notebooks-3-analyzing-a-bank-failure-with-colab-d23b372de313">Noteworthy Notebooks #3 — Analyzing a Bank Failure with Colab</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[10 Minutes to Pandas — now in Colab!]]></title>
            <link>https://medium.com/google-colab/10-minutes-to-pandas-now-in-colab-a7a629a630dc?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/a7a629a630dc</guid>
            <category><![CDATA[pandas]]></category>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Tue, 02 May 2023 20:04:53 GMT</pubDate>
            <atom:updated>2023-05-03T07:34:54.825Z</atom:updated>
            <content:encoded><![CDATA[<h3>10 Minutes to Pandas — in Colab!</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OAY2-eKeFD6DW6AK" /><figcaption>Photo by <a href="https://unsplash.com/@yosuke_ota?utm_source=medium&amp;utm_medium=referral">Yosuke Ota</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p><a href="http://pandas.pydata.org">Pandas</a> is a popular open source Python package for data science, data engineering, analytics, and machine learning. It’s built on top of <a href="https://numpy.org/">NumPy</a>, which provides efficient support for numerical computation on multi-dimensional arrays.</p><p>The Pandas project offers a helpful introductory tutorial called <a href="https://pandas.pydata.org/docs/user_guide/10min.html"><em>10 Minutes to Pandas</em></a> but it’s a read-only document. I like to learn by doing so I’ve taken the liberty of porting the ten minute Pandas tutorial to Colab so now you can enjoy an interactive version of this popular material.</p><p><a href="https://colab.research.google.com/drive/1_n1RZx2maSeC9w2iy2CsMb8hq2arG6kW">Here’s the notebook</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a7a629a630dc" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/10-minutes-to-pandas-now-in-colab-a7a629a630dc">10 Minutes to Pandas — now in Colab!</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Colab + BigQuery — Perfect Together]]></title>
            <link>https://medium.com/google-colab/colab-bigquery-perfect-together-827152026eb?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/827152026eb</guid>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Thu, 12 Jan 2023 08:47:16 GMT</pubDate>
            <atom:updated>2023-01-12T08:47:16.425Z</atom:updated>
            <content:encoded><![CDATA[<h3>Colab + BigQuery — Perfect Together</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L7hBvd_u0u7FUzXO" /><figcaption>Photo by <a href="https://unsplash.com/@benjaminlehman?utm_source=medium&amp;utm_medium=referral">benjamin lehman</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Have you heard about Google BigQuery? It’s hard to summarize all of the advantages in one sentence but I’m going to try anyway: BigQuery is a fully managed, cloud-native data warehouse that enables incredibly fast SQL queries using the processing power of Google’s infrastructure.</p><p>If you haven’t had an opportunity to try BigQuery, today’s notebook, courtesy of Google Developer Advocate <a href="https://twitter.com/alokpattani">Alok Pattani</a>, is a treat. In Alok’s notebook, you’ll learn how to use BigQuery to perform some basic data science tasks, including:</p><ul><li>setting up Colab and Google BigQuery within Colab</li><li>reading data from BigQuery into Colab</li><li>using Python data science tools to do some analysis/curve fitting</li><li>creating some interactive outputs</li><li>using Python functionality “on top” of BigQuery to scale analysis</li><li>writing some analysis results back into BigQuery</li></ul><p>In addition to mind-bending performance, BigQuery is scalable, serverless, cost effective, secure, and plays well with other Google Cloud services.</p><p><a href="https://colab.research.google.com/drive/1hSI1BXyCyj7viRpp1GFZqkU1qtBUd0g1?authuser=0"><strong>Run the notebook</strong></a><strong> and see how easy it is to use a world class data warehouse from the free, zero-friction Colab environment.</strong></p><figure><a href="https://colab.research.google.com/drive/1hSI1BXyCyj7viRpp1GFZqkU1qtBUd0g1?authuser=0"><img alt="" src="https://cdn-images-1.medium.com/max/354/0*kazeScHysb3JCHmL.png" /></a></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=827152026eb" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/colab-bigquery-perfect-together-827152026eb">Colab + BigQuery — Perfect Together</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Make your dataframes come alive!]]></title>
            <link>https://medium.com/google-colab/make-your-dataframes-come-alive-924c12e2e1d6?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/924c12e2e1d6</guid>
            <category><![CDATA[data-visualization]]></category>
            <category><![CDATA[exploratory-data-analysis]]></category>
            <category><![CDATA[dataframes]]></category>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[pandas]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Thu, 08 Dec 2022 18:22:54 GMT</pubDate>
            <atom:updated>2022-12-08T19:03:02.319Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*VrsGUiLvE-VQcr6VYu3sGQ.gif" /><figcaption>interactive table in action</figcaption></figure><p><a href="https://pandas.pydata.org/">Pandas</a> is a wonderful tool for analyzing data in Python but wouldn’t it be nice if your dataframes could be a bit more interactive? Imagine being able to easily sort and search your data without having to write additional code. Well, in Colab that feature is just a mouse click away.</p><p>Let’s load a dataframe with some heart disease prediction data and display the first few rows:</p><pre>import pandas as pd<br>data = pd.read_csv(&quot;https://raw.githubusercontent.com/kb22/Heart-Disease-Prediction/master/dataset.csv&quot;)<br>data.head()</pre><p>As expected, Pandas renders the table nicely, but it also includes a little magic wand, as highlighted here:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vg7SOANjbPukmzQHLPBXuA.png" /></figure><p>Clicking the wand re-renders the dataframe as an interactive table, like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YDzdBcqfqBffJr2-dRpRxg.png" /></figure><p>Now you can dynamically explore the data, for example, you can:</p><ul><li>reorder the rows by clicking on a column header</li><li>filter the data based on arbitrarily ranges on one or more columns</li><li>copy your data to the clipboard in csv, markdown, or json formats</li></ul><p>You can also skip the mouse click and automatically render your dataframes in this style by default, by running this code near the top of your notebook:</p><pre>from google.colab import data_table<br>data_table.enable_dataframe_formatter()</pre><p>This feature can make your exploratory data analysis a bit more efficient and fun — give it a try!</p><p>Learn more about interactive tables in <a href="https://colab.research.google.com/notebooks/data_table.ipynb">this notebook</a> or see them live in this <a href="https://www.youtube.com/watch?v=rNgswRZ2C1Y">excellent video</a> by my colleague Nate.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=924c12e2e1d6" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/make-your-dataframes-come-alive-924c12e2e1d6">Make your dataframes come alive!</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Noteworthy Notebooks #2 — Winning Wordle]]></title>
            <link>https://medium.com/google-colab/noteworthy-notebooks-2-winning-wordle-40793f94a054?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/40793f94a054</guid>
            <category><![CDATA[wordle]]></category>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[puzzles-and-games]]></category>
            <category><![CDATA[fun]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Fri, 02 Dec 2022 17:13:02 GMT</pubDate>
            <atom:updated>2022-12-08T20:39:17.398Z</atom:updated>
            <content:encoded><![CDATA[<h3>Noteworthy Notebooks #2 — Winning Wordle</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6_BOQV8urFUo2LB3" /><figcaption>Photo by <a href="https://unsplash.com/@nhuenerfuerst?utm_source=medium&amp;utm_medium=referral">Nils Huenerfuerst</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Today’s featured notebook answers three interesting questions about the viral word game sensation <a href="https://www.nytimes.com/games/wordle/index.html">Wordle</a>:</p><ul><li>If I win in two guesses, am I good or lucky?</li><li>What is a guaranteed-winning strategy that I can easily memorize?</li><li>What is a strategy that minimizes the number of guesses?</li></ul><p>This one comes from a delightful collection of Python notebooks by <a href="https://norvig.com/">Peter Norvig</a>. Check out Peter’s <a href="https://github.com/norvig/pytudes">pytudes</a> site for more interesting and fun puzzle solving and learning exercises, all of which can be run on Colab.</p><p>Even if you’ve never written a line of Python, experimenting with this notebook will give you a feeling for how you can solve tricky problems using software.</p><p>Here’s the notebook:</p><figure><a href="https://colab.research.google.com/github/norvig/pytudes/blob/main/ipynb/Wordle.ipynb"><img alt="" src="https://cdn-images-1.medium.com/max/354/1*tZ-G83Pjz0Ai1OYu3EtJpw.png" /></a></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=40793f94a054" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/noteworthy-notebooks-2-winning-wordle-40793f94a054">Noteworthy Notebooks #2 — Winning Wordle</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using BERT Models in TensorFlow]]></title>
            <link>https://medium.com/google-colab/using-bert-models-in-tensorflow-213c25eaacc4?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/213c25eaacc4</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[tensorflow]]></category>
            <category><![CDATA[bert]]></category>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[nlp]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Tue, 22 Nov 2022 21:51:14 GMT</pubDate>
            <atom:updated>2022-11-22T22:10:44.254Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Tensorflow Hub makes it easier than ever to use BERT models with preprocessing. Try it in </em><a href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb"><em>Colab</em></a>!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PNBAcndyeNYBEa5m2c2ZHA.png" /></figure><p><a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">BERT</a> and other <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> encoder architectures have been very successful in natural language processing (NLP) for computing vector space representations of text, both in advancing the state of the art in academic benchmarks as well as in large-scale applications like <a href="https://blog.google/products/search/search-language-understanding-bert/">Google Search</a>. BERT has been available for TensorFlow since it was created, but originally relied on non-TensorFlow Python code to transform raw text into model inputs.</p><p>Nowadays, we can use BERT entirely within TensorFlow, thanks to pre-trained encoders and matching text preprocessing models available on <a href="https://tfhub.dev/google/collections/bert/1">TensorFlow Hub</a>. BERT in TensorFlow can now run on text inputs with just a few lines of code:</p><figure><img alt="An animation of the preprocessing model that makes it easy for you to input text into BERT (described below)." src="https://cdn-images-1.medium.com/max/1024/0*LV47KzAnYPxunTCF.gif" /></figure><pre># Load BERT and the preprocessing model from TF Hub.<br>preprocess = hub.load(&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1&#39;)<br>encoder = hub.load(&#39;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3&#39;)<br># Use BERT on a batch of raw text inputs.<br>input = preprocess([&#39;Batch of inputs&#39;, &#39;TF Hub makes BERT easy!&#39;, &#39;More text.&#39;])<br>pooled_output = encoder(input)[&quot;pooled_output&quot;]<br>print(pooled_output)<br>tf.Tensor(<br>[[-0.8384154  -0.26902363 -0.3839138  ... -0.3949695  -0.58442086  0.8058556 ]<br> [-0.8223734  -0.2883956  -0.09359277 ... -0.13833837 -0.6251748   0.88950026]<br> [-0.9045408  -0.37877116 -0.7714909  ... -0.5112085  -0.70791864  0.92950743]],<br>shape=(3, 768), dtype=float32)</pre><p>These encoder and preprocessing models have been built with <a href="https://github.com/tensorflow/models/tree/master/official/nlp">TensorFlow Model Garden</a>’s NLP library and exported to TensorFlow Hub in the <a href="https://www.tensorflow.org/hub/tf2_saved_model">SavedModel format</a>. Under the hood, preprocessing uses TensorFlow ops from the <a href="https://blog.tensorflow.org/2019/06/introducing-tftext.html">TF.text</a> library to do the tokenization of input text — allowing you to build your own TensorFlow model that goes from raw text inputs to prediction outputs without Python in the loop. This accelerates the computation, removes boilerplate code, is less error prone, and enables the serialization of the full text-to-outputs model, making BERT easier to serve in production.</p><p>To show in more detail how these models can help you, check out these tutorials:</p><ul><li>The <a href="https://www.tensorflow.org/tutorials/text/classify_text_with_bert">beginner</a> tutorial solves a <em>sentiment analysis</em> task and doesn’t need any special customization to achieve great model quality. It’s the easiest way to use BERT and a preprocessing model.</li><li>The <a href="https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu">advanced</a> tutorial solves NLP classification tasks from the <a href="http://gluebenchmark.com/">GLUE benchmark</a>, running on a TPU. It also shows how to use the preprocessing model in situations where you need multi-segment input.</li></ul><h4>Choosing a BERT model</h4><p>BERT models are pre-trained on a large corpus of text (for example, an archive of Wikipedia articles) using self-supervised tasks like predicting words in a sentence from the surrounding context. This type of training allows the model to learn a powerful representation of the semantics of the text without needing labeled data. However, it also takes significant computing resources to train — 4 days on 16 TPUs (as reported in the 2018 <a href="https://arxiv.org/abs/1810.04805">BERT paper</a>). Fortunately, after completing this expensive pre-training, we can reuse this rich representation for many different tasks.</p><p>TensorFlow Hub offers a variety of BERT and BERT-like models:</p><ul><li>Eight <a href="https://tfhub.dev/google/collections/bert/1">BERT models</a> come with the trained weights released by the original BERT authors.</li><li>24 <a href="https://tfhub.dev/google/collections/bert/1">Small BERTs</a> have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.</li><li><a href="https://tfhub.dev/google/collections/albert/1">ALBERT</a>: these are four different sizes of “A Lite BERT” that reduces model size (but not computation time) by sharing parameters between layers.</li><li>The 8 <a href="https://tfhub.dev/google/collections/experts/bert/1">BERT Experts</a> all have the same BERT architecture and size but offer a choice of different pre-training domains and intermediate fine-tuning tasks, to align more closely with the target task.</li><li><a href="https://tfhub.dev/google/collections/electra/1">Electra</a> has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).</li><li>BERT with Talking-Heads Attention and Gated GELU [<a href="https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1">base</a>, <a href="https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1">large</a>] has two improvements to the core of the Transformer architecture.</li><li><a href="https://tfhub.dev/tensorflow/lambert_en_uncased_L-24_H-1024_A-16/1">Lambert</a> has been trained with the LAMB optimizer and several techniques from RoBERTa.</li><li><a href="https://tfhub.dev/google/MuRIL/1">MuRIL</a> is Multilingual Representations for Indian Languages, pre-trained on 17 Indian languages (including English), and their transliterated counterparts.</li><li>MobileBERT (<a href="https://tfhub.dev/tensorflow/mobilebert_en_uncased_L-24_H-128_B-512_A-4_F-4_OPT/1">english</a>, <a href="https://tfhub.dev/tensorflow/mobilebert_multi_cased_L-24_H-128_B-512_A-4_F-4_OPT/1">multilingual</a>), is a thin version of BERT, trained through distillation from a teacher BERT model on the Wikipedia, BooksCorpus.</li><li>… and more to come.</li></ul><p>These models are BERT encoders. The links above take you to their documentation on TF Hub, which refers to the preprocessing model to use with each of them.</p><p>Visit the model pages to learn more about the different applications targeted by each model. Thanks to their common interface, it’s easy to experiment and compare the performance of different encoders on your specific task by changing the URLs of the encoder model and its preprocessing.</p><h4>The Preprocessing Model</h4><p>For each BERT encoder, there’s a matching preprocessing model, which transforms raw text to numeric input tensors expected by the encoder, using the TF.text library. Unlike preprocessing with pure Python, this logic can be part of a TensorFlow model served directly from text inputs. Each preprocessing model from TF Hub is already configured with a vocabulary and its associated text normalization logic and needs no further set-up.</p><p>We’ve already seen the simplest way of using the preprocessing model above. Let’s look again more closely:</p><pre>preprocess = hub.load(&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1&#39;)<br>input = preprocess([&quot;This is an amazing movie!&quot;])<br> <br>{&#39;input_word_ids&#39;: &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=<br>  array([[ 101, 2023, 2003, 2019, 6429, 3185,  999,  102,    0,  ...]])&gt;,<br> &#39;input_mask&#39;: &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=<br>  array([[   1,    1,    1,    1,    1,    1,    1,    1,    0,  ...,]])&gt;,<br> &#39;input_type_ids&#39;: &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=<br>  array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,  ...,]])&gt;}</pre><p>Calling preprocess()transforms raw text inputs into a fixed-length input sequence for the BERT encoder. You can see that it consists of a tensor input_word_ids with numerical ids for each tokenized input, including start, end and padding tokens, plus two auxiliary tensors: an input_mask (that tells non-padding from padding tokens) and input_type_ids for each token (that can distinguish multiple text segments per input, which we will discuss below).</p><p>The same preprocessing SavedModel also offers a second, more fine-grained API, which supports putting one or two distinct text segments into one input sequence for the encoder. Let’s look at a sentence entailment task, in which BERT is used to predict if a premise entails a hypothesis or not:</p><pre>text_premises = [&quot;The fox jumped over the lazy dog.&quot;,<br>                 &quot;Good day.&quot;]<br>tokenized_premises = preprocess.tokenize(text_premises)<br> <br>&lt;tf.RaggedTensor<br>  [[[1996], [4419], [5598], [2058], [1996], [13971], [3899], [1012]],<br>  [[2204], [2154], [1012]]]&gt;<br> <br> <br>text_hypotheses = [&quot;The dog was lazy.&quot;,  # Entailed.<br>                   &quot;Axe handle!&quot;]        # Not entailed.<br>tokenized_hypotheses = preprocess.tokenize(text_hypotheses)<br> <br>&lt;tf.RaggedTensor<br>  [[[1996], [3899], [2001], [13971], [1012]],<br>  [[12946], [5047], [999]]]&gt;</pre><p>The result of each tokenization is a <a href="https://www.tensorflow.org/guide/ragged_tensor">RaggedTensor</a> of numeric token ids, representing each of the text inputs in full. If some pairs of premise and hypothesis are too long to fit within the <em>seq_length</em> for BERT inputs in the next step, you can do additional preprocessing here, such as trimming the text segment or splitting it into multiple encoder inputs.</p><p>The tokenized input then gets packed into a fixed-length input sequence for the BERT encoder:</p><pre>encoder_inputs = preprocess.bert_pack_inputs(<br>   [tokenized_premises, tokenized_hypotheses],<br>   seq_length=18)  # Optional argument, defaults to 128.<br> <br>{&#39;input_word_ids&#39;: &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=<br>  array([[  101,  1996,  4419,  5598,  2058,  1996, 13971,  3899,  1012,<br>            102,  1996,  3899,  2001, 13971,  1012,   102,     0,     0],<br>         [  101,  2204,  2154,  1012,   102, 12946,  5047,   999,   102,<br>              0,     0,     0,     0,     0,     0,     0,     0,     0]])&gt;,<br> &#39;input_mask&#39;: &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=<br>  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],<br>         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&gt;,<br> &#39;input_type_ids&#39;: &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=<br>  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],<br>         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&gt;}</pre><p>The result of packing is the already-familiar dict of <em>input_word_ids</em>, <em>input_mask</em> and <em>input_type_ids</em> (which are 0 and 1 for the first and second input, respectively). All outputs have a common <em>seq_length</em> (128 by default). Inputs that would exceed <em>seq_length</em> are truncated to approximately equal sizes during packing.</p><h4>Accelerating model training</h4><p>TensorFlow Hub provides BERT encoder and preprocessing models as separate pieces to enable accelerated training, especially on TPUs.</p><p>Tensor Processing Units (TPUs) are Google’s custom-developed accelerator hardware that excel at large scale machine learning computations such as those required to fine-tune BERT. TPUs operate on dense Tensors and expect that variable-length data like strings has already been transformed into fixed-size Tensors by the host CPU.</p><p>The split between the BERT encoder model and its associated preprocessing model enables distributing the encoder fine-tuning computation to TPUs as part of model training, while the preprocessing model executes on the host CPU. The preprocessing computation can be run asynchronously on a dataset using tf.data.Dataset.map() with dense outputs ready to be consumed by the encoder model on the TPU. Asynchronous preprocessing like this can improve performance with other accelerators as well.</p><p>Our <a href="https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu">advanced</a> BERT tutorial can be run in a Colab runtime that uses a TPU worker and demonstrates this end-to-end.</p><h3>Summary</h3><p>TensorFlow Hub makes available a large <a href="https://tfhub.dev/google/collections/transformer_encoders_text/1">collection</a> of pre-trained BERT encoders and text preprocessing models that are easy to use in just a few lines of code.</p><p>Take a look at our interactive <a href="https://www.tensorflow.org/tutorials/text/classify_text_with_bert">beginner</a> and <a href="https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu">advanced</a> tutorials to learn more about how to use the models for sentence and sentence-pair classification.</p><p><em>Adapted from a </em><a href="https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html"><em>Tensorflow Blog article</em></a><em> by Arno Eigenwillig and </em><a href="https://twitter.com/gusthema"><em>Luiz GUStavo Martins</em></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=213c25eaacc4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/using-bert-models-in-tensorflow-213c25eaacc4">Using BERT Models in TensorFlow</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Welcome to the official Google Colab blog!]]></title>
            <link>https://medium.com/google-colab/welcome-to-the-official-google-colab-blog-3a6d10f50d36?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/3a6d10f50d36</guid>
            <category><![CDATA[announcements]]></category>
            <category><![CDATA[colab]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Fri, 11 Nov 2022 12:24:07 GMT</pubDate>
            <atom:updated>2022-12-08T20:38:51.429Z</atom:updated>
            <content:encoded><![CDATA[<p><strong>Click the Follow button</strong> to stay informed about product announcements, best practices, and featured notebooks. And let us know if you have a story or a notebook you’d like to share with our community.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3a6d10f50d36" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/welcome-to-the-official-google-colab-blog-3a6d10f50d36">Welcome to the official Google Colab blog!</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Featured Notebook #1 — Digit Classifier Using Keras and TensorFlow]]></title>
            <link>https://medium.com/google-colab/featured-notebook-1-digit-classifier-using-keras-and-tensorflow-e088e05c6ad2?source=rss-14a828f24ca2------2</link>
            <guid isPermaLink="false">https://medium.com/p/e088e05c6ad2</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[keras]]></category>
            <category><![CDATA[colab]]></category>
            <category><![CDATA[tensorflow]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Marc Cohen]]></dc:creator>
            <pubDate>Wed, 09 Nov 2022 22:10:57 GMT</pubDate>
            <atom:updated>2022-11-23T19:05:57.370Z</atom:updated>
            <content:encoded><![CDATA[<h3>Noteworthy Notebooks #1 — Digit Classifier Using Keras and TensorFlow.js</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bowDah0wnT3lCzWOn8PYDQ.gif" /></figure><p>This lab trains and evaluates a handwritten digit classification model using the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>. It uses a Graphical Processing Unit (GPU) to speed up training and includes an interactive component that lets you test your model by drawing your own digits right inside this notebook.</p><p>Here’s what we’ll do in this lab:</p><ul><li>Train an ML model on a GPU</li><li>Test our model interactively using TensorFlow.js</li><li>Export our model for use in JavaScript and run an interactive web app to test the model interactively, with hand-drawn digits (as depicted in the animated gif above).</li></ul><p><strong>Here’s the notebook: </strong><a href="http://mco.fyi/mllab"><strong>mco.fyi/mllab</strong></a><strong>.</strong></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e088e05c6ad2" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/featured-notebook-1-digit-classifier-using-keras-and-tensorflow-e088e05c6ad2">Featured Notebook #1 — Digit Classifier Using Keras and TensorFlow</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>