
== Article 1
* Title: 'Tech Conferences'
* Author: 'Kelsey Hightower'
* URL: 'https://medium.com/@kelseyhightower/tech-conferences-84d67cf994f6?source=rss-9e783a6f12f6------2'
* PublicationDate: 'Mon, 23 Jan 2017 05:29:59 GMT'
* Categories: public-speaking, conference

The AttendeeI’ve gone to my fair share of conferences as an attendee, and nothing is more exciting than spending time with people who share your passion for technology. These are our tribes, and when we’re brought together, it’s amazing.But for me it did not start out that way. I can remember going to conferences alone, hoping to join the party, only to find that conferences can be a bit cliquish, composed of subgroups that roll in tight circles. My first taste of feeling like an outcast came during a conference lunch; I felt like the kid who’s just transferred to a new school halfway into the school year. I was without my support system, no one knew who I was, and all the cool kids seemed to be having tons of fun without me.Over time I learned to just jump in, work the room, listen in on conversations I found interesting, and then break the ice with a question. Turns out, most conference goers love to talk, and if you ask a somewhat relevant question, people will talk to you for hours. I took the power of nerd sniping and used it for good.After a few events, I came to understand that most people are nice, welcoming, and willing to talk to you — but it was up to me to jump in. With a little experience under my belt, I now seek out people who look like my formerly shy self and make them feel welcome, mainly by asking questions and listening. I don’t wait for my chance to respond; I actually listen, and I learn from them. It’s amazing how much people at tech conferences have to share, even if they have less experience in the conference topic than you.The SpeakerI started speaking at conferences about five years ago. I got my start speaking at a local Python meetup in Atlanta, where I gave a talk comparing the implementation of list comprehensions in Python and Haskell. I chose such a complex topic because I felt I had to look smart. This is something I would later learn is unnecessary; some of the best presentations I’ve seen come from beginners sharing something they’re excited about.After a few more meetups, I built up the courage to submit my first talk proposal to PuppetConf 2011, “Streamlining Workflows With Puppet Faces”. The talk was a success! I overcame my fear of public speaking, despite sweaty palms and butterflies, and got a few laughs along the way. Shortly after that talk, I landed a job at Puppet Labs. I still consider that talk one of the best interviews I’ve ever given.After PuppetConf, I went on to speak at a few more meetups and small conferences. As a speaker, I felt I had finally made it. But in some cases, I felt I had landed talks as the token black guy. To be honest, I didn’t care — opportunity is opportunity — though I do worry about the speakers that were rejected in the name of inclusion. Nevertheless, I figured I owed it to the community, my community and to the rejected speakers, to not only accept the speaking slot, but to go out there and earn it.I’ve largely overcome my fears around submitting a CFP (Call for Papers), but I’ll never forget what it’s like to feel unqualified, or believe that an event won’t accept an outsider from an underrepresented group. Even to this day, when I see a CFP pop up, the first thing I do is check out the speakers from previous years. If I don’t see someone who looks like me, I immediately assume I’ll be fighting an uphill battle, and start questioning what makes me so special that I’ll be the first black person given an opportunity to speak at their conference. Representation matters, and it’s the main reason I accept so many speaking opportunities: I want to be the reflection some people are looking for.The Conference ChairTwo years ago I was approached by O’Reilly, and was offered an opportunity to become a co-chair for OSCON 2016. After disconnecting from the initial call with Gina Blaber, the vice president of conferences, I did a few victory laps around my apartment, thinking,“I have just received one of the highest honors a member of the tech community could ever receive!” I’d been entrusted to help shape what continues to be a legendary event, not just for open source, but for all the communities born from it.Rachel Roumeliotis, who I consider to be the Queen of OSCON, showed me the ropes and taught me everything I know about being a conference chair. Not only did Rachel teach me how to sort and rank talk submissions, she taught me how to think about each talk in relation to the entire conference. She taught me how to be a professional conference contributor.I’ve also had the pleasure of working with Scott Hanselman as my OSCON co-chair. In short, Scott is extra dope, and is the biggest advocate for inclusion I know. Scott doesn’t go around giving out speaking slots, but goes above and beyond to ensure no one gets overlooked. Scott taught me how to dig deeper and find those hidden gems that reviewers often miss. Not only did we end up giving new speakers a chance, we gave attendees something new, something refreshing, and new faces to attach it to.I’m also fortunate to co-chair KubeCon this year, and to get a chance to incorporate what I’ve learned from OSCON. So far, the most challenging aspect of being a conference chair is rejecting talk submissions. I know how much time it takes to put together a CFP, and how much courage it can take to actually submit it, so it pains me to have to reject even a single talk. As a conference chair, you get to analyze every CFP, and you quickly realize that many submissions overlap, so great proposals often have to compete with each other. This is actually a good thing, but then you need to determine how to break ties. Do you optimize for new speakers, inclusion, and balance, or do the big names always win?Some may suggest a blind CFP review and selection process. I can’t see how that works, unless your CFP attracts a balanced set of submissions from all groups. One way to foster inclusion is to encourage underrepresented groups to submit a CFP, and more importantly, offer assistance for putting together the best proposal possible. It also helps to keep in mind that gender is not the only form of inclusion. Some claim success because the number of female speakers at their event reaches some percentage slightly greater than terrible; this is a bit short-sighted, and leaves out many other underrepresented groups. But beyond inclusion, remember that content is king. Most people go to conferences to learn something, so the talks must meet that expectation, even if your goals for inclusion fall short.I’m still pretty new to this whole conference chair thing, and so far I have learned there’s no one-size-fits-all when it comes to conferences. Fairness requires hard work from the organizers, rooted in respect for both the speaker and attendee — and putting together a tech conference give us the opportunity to do just that.

== Article 2
* Title: '12 Fractured Apps'
* Author: 'Kelsey Hightower'
* URL: 'https://medium.com/@kelseyhightower/12-fractured-apps-1080c73d481c?source=rss-9e783a6f12f6------2'
* PublicationDate: 'Sun, 13 Dec 2015 22:33:00 GMT'
* Categories: docker, devops

Over the years I’ve witnessed more and more people discover the 12 Fact0r App manifesto and start implementing many of the suggestions outlined there. This has led to applications that are far easier to deploy and manage. However practical examples of 12 Factor were a rare sight to see in the wild.Once Docker hit the scene the benefits of the 12 Factor App (12FA) really started to shine. For example, 12FA recommends that logging should be done to stdout and be treated as an event stream. Ever run the docker logs command? That’s 12FA in action!12FA also suggests applications should use environment variables for configuration. Again Docker makes this trivial by providing the ability to set env vars programmatically when creating containers.Docker and 12 factor apps are a killer combo and offer a peek into the future of application design and deployment.Docker also makes it somewhat easy to “lift and shift” legacy applications to containers. I say “somewhat” because what most people end up doing is treating Docker containers like VMs, resulting in 2GB container images built on top of full blown Linux distros.Unfortunately legacy applications, including the soon-to-be-legacy application you are working on right now, have many shortcomings, especially around the startup process. Applications, even modern ones, make too many assumptions and do very little to ensure a clean startup. Applications that require an external database will normally initialize the database connection during startup. However, if that database is unreachable, even temporarily, many applications will simply exit. If you’re lucky you might get an error message and non-zero exit code to aid in troubleshooting.Many of the applications that are being packaged for Docker are broken in subtle ways. So subtle people would not call them broken, it’s more like a hairline fracture — it works but hurts like hell when you use them.This kind of application behavior has forced many organizations into complex deployment processes and contributed to the rise of configuration management tools like Puppet or Ansible. Configuration management tools can solve the “missing” database problem by ensuring the database is started before the applications that depend on it. This is nothing more then a band-aid covering up the larger problem. The application should simply retry the database connection, using some sort of backoff, and log errors along the way. At some point either the database will come online, or your company will be out of business.Another challenge for applications moving to Docker is around configuration. Many applications, even modern ones, still rely on local, on-disk, configuration files. It’s often suggested to simply build new “deployment” containers that bundle the configuration files in the container image.Don’t do this.If you go down this road you will end up with an endless number of container images named something like this:application-v2–prod-01022015application-v2-dev-02272015You’ll soon be in the market for a container image management tool.The move to Docker has given people the false notion they no longer need any form of configuration management. I tend to agree, there is no need to use Puppet, Chef, or Ansible to build container images, but there is still a need to manage runtime configuration settings.The same logic used to do away with configuration management is often used to avoid all init systems in favor of the docker run command.To compensate for the lack of configuration management tools and robust init systems, Docker users have turned to shell scripts to mask application shortcomings around initial bootstrapping and the startup process.Once you go all in on Docker and refuse to use tools that don’t bear the Docker logo you paint yourself into a corner and start abusing Docker.Example ApplicationThe remainder of this post will utilize an example program to demonstrate a few common startup tasks preformed by a typical application. The example application performs the following tasks during startup:Load configuration settings from a JSON encoded config fileAccess a working data directoryEstablish a connection to an external mysql databasepackage mainimport (    "database/sql"    "encoding/json"    "fmt"    "io/ioutil"    "log"    "net"    "os"    _ "github.com/go-sql-driver/mysql")var (    config Config    db     *sql.DB)type Config struct {    DataDir string `json:"datadir"`    // Database settings.    Host     string `json:"host"`    Port     string `json:"port"`    Username string `json:"username"`    Password string `json:"password"`    Database string `json:"database"`}func main() {    log.Println("Starting application...")    // Load configuration settings.    data, err := ioutil.ReadFile("/etc/config.json")    if err != nil {        log.Fatal(err)    }    if err := json.Unmarshal(data, &amp;config); err != nil {        log.Fatal(err)    }    // Use working directory.    _, err = os.Stat(config.DataDir)    if err != nil {        log.Fatal(err)    }    // Connect to database.    hostPort := net.JoinHostPort(config.Host, config.Port)    dsn := fmt.Sprintf("%s:%s@tcp(%s)/%s?timeout=30s",        config.Username, config.Password, hostPort, config.Database)    db, err = sql.Open("mysql", dsn)    if err != nil {        log.Fatal(err)    }    if err := db.Ping(); err != nil {        log.Fatal(err)    }}The complete source code of the example program is available on GitHub.As you can see there’s nothing special here, but if you look closely you can see this application will only startup under specific conditions, which we’ll call the happy path. If the configuration file or working directory is missing, or the database is not available during startup, the above application will fail to start. Let’s deploy the example application via Docker and examine this first hand.Build the application using the go build command:$ GOOS=linux go build -o app .Create a Docker image using the following Dockerfile:FROM scratchMAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;COPY app /appENTRYPOINT ["/app"]All I’m doing here is copying the application binary into place. This container image will use the scratch base image, resulting in a minimal Docker image suitable for deploying our application. Remember, ship artifacts not build environments.Create the Docker image using the docker build command:$ docker build -t app:v1 .Finally, create a Docker container from the app:v1 Docker image using the docker run command:$ docker run --rm app:v12015/12/13 04:00:34 Starting application...2015/12/13 04:00:34 open /etc/config.json: no such file or directoryLet the pain begin! Right out of the gate I hit the first startup problem. Notice the application fails to start because of the missing /etc/config.json configuration file. I can fix this by bind mounting the configuration file at runtime:$ docker run --rm \  -v /etc/config.json:/etc/config.json \  app:v12015/12/13 07:36:27 Starting application...2015/12/13 07:36:27 stat /var/lib/data: no such file or directoryAnother error! This time the application fails to start because the /var/lib/data directory does not exist. I can easily work around the missing data directory by bind mounting another host dir into the container:$ docker run --rm \  -v /etc/config.json:/etc/config.json \  -v /var/lib/data:/var/lib/data \  app:v12015/12/13 07:44:18 Starting application...2015/12/13 07:44:48 dial tcp 203.0.113.10:3306: i/o timeoutNow we are making progress, but I forgot to configure access to the database for this Docker instance.This is the point where some people start suggesting that configuration management tools should be used to ensure that all these dependencies are in place before starting the application. While that works, it’s pretty much overkill and often the wrong approach for application-level concerns.I can hear the silent cheers from hipster “sysadmins” sipping on a cup of Docker Kool-Aid eagerly waiting to suggest using a custom Docker entrypoint to solve our bootstrapping problems.Custom Docker entrypoints to the rescueOne way to address our startup problems is to create a shell script and use it as the Docker entrypoint in place of the actual application. Here’s a short list of things we can accomplish using a shell script as the Docker entrypoint:Generate the required /etc/config.json configuration fileCreate the required /var/lib/data directoryTest the database connection and block until it’s availableThe following shell script tackles the first two items by adding the ability to use environment variables in-place of the /etc/config.json configuration file and creating the missing /var/lib/data directory during the startup process. The script executes the example application as the final step, preserving the original behavior of starting the application by default.#!/bin/shset -edatadir=${APP_DATADIR:="/var/lib/data"}host=${APP_HOST:="127.0.0.1"}port=${APP_PORT:="3306"}username=${APP_USERNAME:=""}password=${APP_PASSWORD:=""}database=${APP_DATABASE:=""}cat &lt;&lt;EOF &gt; /etc/config.json{  "datadir": "${datadir}",  "host": "${host}",  "port": "${port}",  "username": "${username}",  "password": "${password}",  "database": "${database}"}EOFmkdir -p ${APP_DATADIR}exec "/app"The Docker image can now be rebuilt using the following Dockerfile:FROM alpine:3.1MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;COPY app /appCOPY docker-entrypoint.sh /entrypoint.shENTRYPOINT ["/entrypoint.sh"]Notice the custom shell script is copied into the Docker image and used as the entrypoint in place of the application binary.Build the app:v2 Docker image using the docker build command:$ docker build -t app:v2 .Now run it:$ docker run --rm \  -e "APP_DATADIR=/var/lib/data" \  -e "APP_HOST=203.0.113.10" \  -e "APP_PORT=3306" \  -e "APP_USERNAME=user" \  -e "APP_PASSWORD=password" \  -e "APP_DATABASE=test" \  app:v22015/12/13 04:44:29 Starting application...The custom entrypoint is working. Using only environment variables we are now able to configure and run our application.But why are we doing this?Why do we need to use such a complex wrapper script? Some will say it’s much easier to write this functionality in shell then doing it in the app. But the cost is not only in managing shell scripts. Notice the other difference between the v1 and v2 Dockerfiles?FROM alpine:3.1The v2 Dockerfile uses the alpine base image to provide a scripting environment, while small, it does double the size of our Docker image:$ docker imagesREPOSITORY  TAG  IMAGE ID      CREATED      VIRTUAL SIZEapp         v2   1b47f1fbc7dd  2 hours ago  10.99 MBapp         v1   42273e8664d5  2 hours ago  5.952 MBThe other drawback to this approach is the inability to use a configuration file with the image. We can continue scripting and add support for both the configuration file and env vars, but this is just going down the wrong path, and it will come back to bite us at some point when the wrapper script gets out of sync with the application.There is another way to fix this problem.Programming to the rescueYep, good old fashion programming. Each of the issues being addressed in the docker-entrypoint.sh script can be handled directly by the application.Don’t get me wrong, using an entrypoint script is ok for applications you don’t have control over, but when you rely on custom entrypoint scripts for applications you write, you add another layer of complexity to the deployment process for no good reason.Config files should be optionalThere is absolutely no reason to require a configuration file after the 90s. I would suggest loading the configuration file if it exists, and falling back to sane defaults. The following code snippet does just that.// Load configuration settings.data, err := ioutil.ReadFile("/etc/config.json")// Fallback to default values.switch {    case os.IsNotExist(err):        log.Println("Config file missing using defaults")        config = Config{            DataDir: "/var/lib/data",            Host: "127.0.0.1",            Port: "3306",            Database: "test",        }    case err == nil:        if err := json.Unmarshal(data, &amp;config); err != nil {            log.Fatal(err)        }    default:        log.Println(err)}Using env vars for configThis is one of the easiest things you can do directly in your application. In the following code snippet env vars are used to override configuration settings.log.Println("Overriding configuration from env vars.")if os.Getenv("APP_DATADIR") != "" {    config.DataDir = os.Getenv("APP_DATADIR")}if os.Getenv("APP_HOST") != "" {    config.Host = os.Getenv("APP_HOST")}if os.Getenv("APP_PORT") != "" {    config.Port = os.Getenv("APP_PORT")}if os.Getenv("APP_USERNAME") != "" {    config.Username = os.Getenv("APP_USERNAME")}if os.Getenv("APP_PASSWORD") != "" {    config.Password = os.Getenv("APP_PASSWORD")}if os.Getenv("APP_DATABASE") != "" {    config.Database = os.Getenv("APP_DATABASE")}Manage the application working directoriesInstead of punting the responsibility of creating working directories to external tools or custom entrypoint scripts your application should manage them directly. If they are missing create them. If that fails be sure to log an error with the details:// Use working directory._, err = os.Stat(config.DataDir)if os.IsNotExist(err) {    log.Println("Creating missing data directory", config.DataDir)    err = os.MkdirAll(config.DataDir, 0755)}if err != nil {    log.Fatal(err)}Eliminate the need to deploy services in a specific orderDo not require anyone to start your application in a specific order. I’ve seen too many deployment guides warn users to deploy an application after the database because the application would fail to start.Stop doing this. Here’s how:$ docker run --rm \  -e "APP_DATADIR=/var/lib/data" \  -e "APP_HOST=203.0.113.10" \  -e "APP_PORT=3306" \  -e "APP_USERNAME=user" \  -e "APP_PASSWORD=password" \  -e "APP_DATABASE=test" \  app:v32015/12/13 05:36:10 Starting application...2015/12/13 05:36:10 Config file missing using defaults2015/12/13 05:36:10 Overriding configuration from env vars.2015/12/13 05:36:10 Creating missing data directory /var/lib/data2015/12/13 05:36:10 Connecting to database at 203.0.113.10:33062015/12/13 05:36:40 dial tcp 203.0.113.10:3306: i/o timeout2015/12/13 05:37:11 dial tcp 203.0.113.10:3306: i/o timeoutNotice in the above output that I’m not able to connect to the target database running at 203.0.113.10.After running the following command to grant access to the “mysql” database:$ gcloud sql instances patch mysql \  --authorized-networks "203.0.113.20/32"The application is able to connect to the database and complete the startup process.2015/12/13 05:37:43 dial tcp 203.0.113.10:3306: i/o timeout2015/12/13 05:37:46 Application started successfully.The code to make this happen looks like this:// Connect to database.hostPort := net.JoinHostPort(config.Host, config.Port)log.Println("Connecting to database at", hostPort)dsn := fmt.Sprintf("%s:%s@tcp(%s)/%s?timeout=30s",    config.Username, config.Password, hostPort, config.Database)db, err = sql.Open("mysql", dsn)if err != nil {    log.Println(err)}var dbError errormaxAttempts := 20for attempts := 1; attempts &lt;= maxAttempts; attempts++ {    dbError = db.Ping()    if dbError == nil {        break    }    log.Println(dbError)    time.Sleep(time.Duration(attempts) * time.Second)}if dbError != nil {    log.Fatal(dbError)}Nothing fancy here. I’m simply retrying the database connection and increasing the time between each attempt.Finally, we wrap up the startup process with a friendly log message that the application has started correctly. Trust me, your sysadmin will thank you.log.Println("Application started successfully.")SummaryEverything in this post is about improving the deployment process for your applications, specifically those running in a Docker container, but these ideas should apply almost anywhere. On the surface it may seem like a g

== Article 3
* Title: 'Building Docker Images for Static Go Binaries'
* Author: 'Kelsey Hightower'
* URL: 'https://medium.com/@kelseyhightower/optimizing-docker-images-for-static-binaries-b5696e26eb07?source=rss-9e783a6f12f6------2'
* PublicationDate: 'Thu, 14 Aug 2014 14:52:28 GMT'
* Categories: 

Building applications in Go enables the ability to easily produce statically linked binaries free of external dependencies. Statically linked binaries are much larger than their dynamic counterparts, but often weigh in at less than 10 MB for most real-world applications. The reason for such large binary sizes are because everything, including the Go runtime, is included in the binary. The large binary size is a tradeoff I’m willing to make since I gain the ability to deploy applications by copying a single binary into place and executing it.So I can’t help but ask, If the process of building and deploying static binaries is so easy, why would I want to bring Docker into the mix? Well, Docker does offer the convenience of a standardized packaging format that makes it easy to share, discover, and install applications. I see Docker images being similar to rpms in concept, with Docker images having the advantage of packaging up my entire application in a single artifact. Hmm… this sounds familiar.Deploying applications with Docker brings the benefits of Linux containers. I essentially gain the ability to leverage Linux namespaces and cgroups for free.After being sold on the benefits of using Docker for Go apps, I started building containers for them. I was a bit surprised by the initial results. To help illustrate my journey I’ll use an example application called Contributors. The Contributors app is a simple web frontend that lists the contributors, along with their avatar photos, for a given GitHub repository.I, like many people, followed examples for building Docker images using a Dockerfile that looks something like this:FROM google/debian:wheezyMAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;RUN apt-get update -y &amp;&amp; apt-get install —no-install-recommends -y -q curl build-essential ca-certificates git mercurial # Install Go# Save the SHA1 checksum from http://golang.org/dlRUN echo '9f9dfcbcb4fa126b2b66c0830dc733215f2f056e go1.3.src.tar.gz' &gt; go1.3.src.tar.gz.sha1RUN curl -O -s https://storage.googleapis.com/golang/go1.3.src.tar.gzRUN sha1sum —check go1.3.src.tar.gz.sha1RUN tar -xzf go1.3.src.tar.gz -C /usr/localENV PATH /usr/local/go/bin:$PATHENV GOPATH /gopathRUN cd /usr/local/go/src &amp;&amp; ./make.bash —no-clean 2&gt;&amp;1WORKDIR /gopath/src/github.com/kelseyhightower/contributors# Build the Contributors applicationRUN mkdir -p /gopath/src/github.com/kelseyhightower/contributorsADD . /gopath/src/github.com/kelseyhightower/contributorsRUN CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags '-w' .RUN cp contributors /contributorsENV PORT 80EXPOSE 80ENTRYPOINT ["/contributors"]The above Dockerfile combines the Docker image creation process with the application build process. While this workflow has its advantages I consider it unnecessary when working with statically linked binaries. I personally build my applications in CI and save the resulting binaries as artifacts of the build, which allows me to package my application in any format including a Docker image, rpm, or tarball.Another drawback to building the application during the image creation process is the size of the resulting Docker image. The above Dockerfile produces a Docker image that checks in around 500 MB in size. Earlier I mentioned that sized does not matter, but at almost 500 MB we better start paying attention.Why is the Docker image so big?Building the Contributor app as part of the image creation process means we must set up a working build environment including all the tools required to download and build the Go runtime. This requirement locks us into choosing a base image capable of installing all those extra bits, and that base image usually starts around the 100 MB range. Then every additional RUN command in the Dockerfile increases the overall file size of the image. Keep in mind I’m doing all of this so I can produce the same binary built by my CI system.While I could add logic to the Dockerfile to clean up unneeded files I decided to keep the application build process in CI, and treat the Docker image as another target for packaging. At this point I was able to shrink my Dockerfile to the following:FROM google/debian:wheezyMAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;ADD contributors contributorsENV PORT 80EXPOSE 80ENTRYPOINT ["/contributors"]Notice the addition of the ADD command which adds the contributors binary to the image. The build process now goes like this:CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags '-w' .docker build -t kelseyhightower/contributors .I’m now working with a much smaller Docker image, but at 120 MB the image is still too big. Especially since our binary doesn’t require any files in the image. The easy solution is to use a smaller base image such as busybox, considered tiny at 5 MB, or the absolute smallest image available, the scratch image:FROM scratchMAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;ADD contributors contributorsENV PORT 80EXPOSE 80ENTRYPOINT ["/contributors"]Normally the scratch image will not work for typical Go applications because they are often built with cgo enabled, which results in a binary that dynamically links to a few dependencies. Also, some Go applications make external calls to SSL endpoints, which will fail with the following error when running from the scratch image:x509: failed to load system roots and no roots providedThe reason for this is that on Linux systems the tls package reads the root CA certificates from /etc/ssl/certs/ca-certificates.crt, which is missing from the scratch image. The Contributors app gets around this problem by bundling a copy of the root CA certificates and configuring outbound calls to use them.Bundling of the actual root CA certificates is pretty straightforward. The Contributor app takes a cert bundle, /etc/ssl/certs/ca-certificates.crt, from CoreOS Linux and assigns the text to a global variable named pemCerts. Then in the main init() the Contributor app initializes a cert pool and configures a HTTP client to use it.func init() {    pool = x509.NewCertPool()    pool.AppendCertsFromPEM(pemCerts)    client = &amp;http.Client{        Transport: &amp;http.Transport{            TLSClientConfig: &amp;tls.Config{RootCAs: pool},        },    }}From this point on all calls using the new HTTP client will work with SSL end-points. Checkout the source code for more details on how the the root CA certificates are wired up.Using the updated Dockerfile and re-running the build process I end up with a Docker image that is slightly larger than the Contributor app binary. We are now down to a 6MB Docker image.At this point I have a Docker image optimized for size that is ready to run and share with others.docker run -d -P kelseyhightower/contributors
