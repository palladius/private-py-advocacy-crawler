<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Guillaume Laforge on Medium]]></title>
        <description><![CDATA[Stories by Guillaume Laforge on Medium]]></description>
        <link>https://medium.com/@glaforge?source=rss-431147437aeb------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/0*Fbu6IxbH-XbgjDml.jpeg</url>
            <title>Stories by Guillaume Laforge on Medium</title>
            <link>https://medium.com/@glaforge?source=rss-431147437aeb------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 03 Jul 2024 16:03:17 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@glaforge/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Let’s make Gemini Groovy!]]></title>
            <link>https://medium.com/google-cloud/lets-make-gemini-groovy-7481ac6add04?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/7481ac6add04</guid>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[groovy]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[langchain4j]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 03 Jun 2024 00:00:24 GMT</pubDate>
            <atom:updated>2024-06-03T15:07:35.141Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oNLhGcpHQkmBupU3.jpg" /></figure><p>The happy users of <a href="https://gemini.google.com/advanced">Gemini Advanced</a>, the powerful AI web assistant powered by the Gemini model, can execute some Python code, thanks to a built-in Python interpreter. So, for math, logic, calculation questions, the assistant can let Gemini invent a Python script, and execute it, to let users get a more accurate answer to their queries.</p><p>But wearing my <a href="https://groovy-lang.org/">Apache Groovy</a> hat on, I wondered if I could get Gemini to invoke some Groovy scripts as well, for advanced math questions!</p><h3>LangChain4j based approach</h3><p>As usual, my tool of choice for any LLM problem is the powerful <a href="https://docs.langchain4j.dev/">LangChain4j</a> framework! Interestingly, there are already some code engine integrations,</p><ul><li>a <a href="https://www.graalvm.org/latest/reference-manual/polyglot-programming/">GraalVM Polyglot Truffle</a> engine, that can execute Python and JavaScript code,</li><li>a <a href="https://judge0.com/">Judge0</a> engine that uses the Judge0 online code execution system, which also supports Groovy!</li></ul><p>I haven’t tried Judge0 yet, as I saw it was supporting Groovy 3 only, and not yet Groovy 4. But for math or logic questions, Groovy 3 is just fine anyway. Instead, I wanted to explore how to create my own Groovy interpreter!</p><p>In the following experiment, I’m going to use the <a href="https://deepmind.google/technologies/gemini/">Gemini</a> model, because it supports <em>function calling</em>, which means we can instruct the model that it can use some tools when needed.</p><p>Let’s walk through this step by step.</p><p>First, I instantiate a Gemini chat model:</p><pre>var model = VertexAiGeminiChatModel.builder()<br>    .project(&quot;MY_GCP_PROJECT_ID&quot;)<br>    .location(&quot;us-central1&quot;)<br>    .modelName(&quot;gemini-1.5-flash-001&quot;)<br>    .maxRetries(1)<br>    .build();</pre><p>Then, I create a tool that is able to run Groovy code, thanks to the GroovyShell evaluator:</p><pre>class GroovyInterpreter {<br>  @Tool(&quot;Execute a Groovy script and return the result of its execution.&quot;)<br>  public Map&lt;String, String&gt; executeGroovyScript(<br>    @P(&quot;The groovy script source code to execute&quot;) String groovyScript) {<br>    String script = groovyScript.replace(&quot;\\n&quot;, &quot;\n&quot;);<br>    System.err.format(&quot;%n--&gt; Executing the following Groovy script:%n%s%n&quot;, script);<br>    try {<br>      Object result = new GroovyShell().evaluate(script);<br>      return Map.of(&quot;result&quot;, result == null ? &quot;null&quot; : result.toString());<br>    } catch (Throwable e) {<br>      return Map.of(&quot;error&quot;, e.getMessage());<br>    }<br>  }<br>}</pre><p>Notice the @Tool annotation that describes what this tool can do. And the@P annotation which explains what the parameter is about.</p><p>I noticed that sometimes the raw script that Gemini suggested contained some \n strings, instead of the plain newline characters, so I&#39;m replacing them with newlines instead.</p><p>I return a map containing either a result (as a string), or an error message if one was encountered.</p><p>Now it’s time to create our assistant contract, in the form of an interface, but with a very carefully crafted system instruction:</p><pre>interface GroovyAssistant {<br>  @SystemMessage(&quot;&quot;&quot;<br>    You are a problem solver equipped with the capability of \<br>    executing Groovy scripts.<br>    When you need to or you&#39;re asked to evaluate some math \<br>    function, some algorithm, or some code, use the \<br>    `executeGroovyScript` function, passing a Groovy script \<br>    that implements the function, the algorithm, or the code \<br>    that needs to be run.<br>    In the Groovy script, return a value. Don&#39;t print the result \<br>    to the console.<br>    Don&#39;t use semicolons in your Groovy scripts, it&#39;s not necessary.<br>    When reporting the result of the execution of a script, \<br>    be sure to show the content of that script.<br>    Call the `executeGroovyScript` function only once, \<br>    don&#39;t call it in a loop.<br>    &quot;&quot;&quot;)<br>  String chat(String msg);<br>}</pre><p>This complex system instruction above tells the model what its role is, and that it should call the provided Groovy script execution function whenever it encounters the need to calculate some function, or execute some logic.</p><p>I also instruct it to return values instead of printing results.</p><p>Funnily, Gemini is a pretty decent Groovy programmer, but it insists on always adding semi-colons like in Java, so for a more <em>idiomatic</em> code style, I suggest it to get rid of them!</p><p>The final step is now to create our LangChain4j AI service with the following code:</p><pre>var assistant = AiServices.builder(GroovyAssistant.class)<br>    .chatLanguageModel(model)<br>    .chatMemory(MessageWindowChatMemory.withMaxMessages(20))<br>    .tools(new GroovyInterpreter())<br>    .build();</pre><p>I combine the Gemini chat model, with a memory to keep track of users’ requests, and the Groovy interpreter tool I’ve just created.</p><p>Now let’s see if Gemini is able to create and calculate a fibonacci function:</p><pre>System.out.println(<br>  assistant.chat(<br>    &quot;Write a `fibonacci` function, and calculate `fibonacci(18)`&quot;));</pre><p>And the output is as follows:</p><pre>def fibonacci(n) {<br>  if (n &lt;= 1) {<br>    return n<br>  } else {<br>    return fibonacci(n - 1) + fibonacci(n - 2)<br>  }<br>}<br>fibonacci(18)<br><br>The result of executing the script is: 2584.</pre><h3>Discussion</h3><p>It took me a bit of time to find the right system instruction to get Groovy scripts that complied to my requirements. However, I noticed sometimes some internal errors returned by the model, which I haven’t fully understood (and particularly why those happen at all)</p><p>On some occasions, I also noticed that LangChain4j keeps sending the same script for execution, in a loop. Same thing: I still have to investigate why this rare behavior happens.</p><p>So this solution is a fun experiment, but I’d call it just that, an experiment, as it’s not as rock-solid as I want it to be. But if I manage to make it more bullet-proof, maybe I could contribute it back as a dedicated execution engine for LangChain4j!</p><h3>Full source code</h3><p>Here’s the full content of my experiment:</p><pre>import dev.langchain4j.agent.tool.P;<br>import dev.langchain4j.agent.tool.Tool;<br>import dev.langchain4j.memory.chat.MessageWindowChatMemory;<br>import dev.langchain4j.model.vertexai.VertexAiGeminiChatModel;<br>import dev.langchain4j.service.AiServices;<br>import dev.langchain4j.service.SystemMessage;<br>import groovy.lang.GroovyShell;<br>import java.util.Map;<br><br>public class GroovyCodeInterpreterAssistant {<br>  public static void main(String[] args) {<br>    var model = VertexAiGeminiChatModel.builder()<br>      .project(&quot;MY_GCP_PROJECT_ID&quot;)<br>      .location(&quot;us-central1&quot;)<br>      .modelName(&quot;gemini-1.5-flash-001&quot;)<br>      .maxRetries(1)<br>      .build();<br><br>    class GroovyInterpreter {<br>      @Tool(&quot;Execute a Groovy script and return the result of its execution.&quot;)<br>      public Map&lt;String, String&gt; executeGroovyScript(<br>          @P(&quot;The groovy script source code to execute&quot;)<br>          String groovyScript) {<br>        System.err.format(&quot;%n--&gt; Raw Groovy script:%n%s%n&quot;, groovyScript);<br>        String script = groovyScript.replace(&quot;\\n&quot;, &quot;\n&quot;);<br>        System.err.format(&quot;%n--&gt; Executing:%n%s%n&quot;, script);<br>        try {<br>          Object result = new GroovyShell().evaluate(script);<br>          return Map.of(&quot;result&quot;, result == null ? &quot;null&quot; : result.toString());<br>        } catch (Throwable e) {<br>          return Map.of(&quot;error&quot;, e.getMessage());<br>        }<br>      }<br>    }<br><br>    interface GroovyAssistant {<br>      @SystemMessage(&quot;&quot;&quot;<br>        You are a problem solver equipped with the capability of \<br>        executing Groovy scripts.<br>        When you need to or you&#39;re asked to evaluate some math \<br>        function, some algorithm, or some code, use the \<br>        `executeGroovyScript` function, passing a Groovy script \<br>        that implements the function, the algorithm, or the code \<br>        that needs to be run.<br>        In the Groovy script, return a value. Don&#39;t print the result \<br>        to the console.<br>        Don&#39;t use semicolons in your Groovy scripts, it&#39;s not necessary.<br>        When reporting the result of the execution of a script, \<br>        be sure to show the content of that script.<br>        Call the `executeGroovyScript` function only once, \<br>        don&#39;t call it in a loop.<br>        &quot;&quot;&quot;)<br>      String chat(String msg);<br>    }<br><br>    var assistant = AiServices.builder(GroovyAssistant.class)<br>      .chatLanguageModel(model)<br>      .chatMemory(MessageWindowChatMemory.withMaxMessages(20))<br>      .tools(new GroovyInterpreter())<br>      .build();<br><br>    System.out.println(<br>      assistant.chat(<br>        &quot;Write a `fibonacci` function, and calculate `fibonacci(18)`&quot;));<br>  }<br>}</pre><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/06/03/lets-make-gemini-groovy/"><em>https://glaforge.dev</em></a><em> on June 3, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7481ac6add04" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/lets-make-gemini-groovy-7481ac6add04">Let’s make Gemini Groovy!</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Grounding Gemini with Web Search results in LangChain4j]]></title>
            <link>https://medium.com/google-cloud/grounding-gemini-with-web-search-results-in-langchain4j-2f949bd69492?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/2f949bd69492</guid>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[langchain4j]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[java]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Tue, 28 May 2024 00:00:35 GMT</pubDate>
            <atom:updated>2024-05-29T01:16:37.987Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ztbyFKIogBmzIV8Q.jpg" /></figure><p>The latest <a href="https://github.com/langchain4j/langchain4j/releases/tag/0.31.0">release of LangChain4j</a> (version 0.31) added the capability of <em>grounding</em> large language models with results from web searches. There’s an integration with <a href="https://developers.google.com/custom-search/v1/overview">Google Custom Search Engine</a>, and also <a href="https://tavily.com/">Tavily</a>.</p><p>The fact of <em>grounding</em> an LLM’s response with the results from a search engine allows the LLM to find relevant information about the query from web searches, which will likely include up-to-date information that the model won’t have seen during its training, past its cut-off date when the training ended.</p><blockquote><strong><em>Remark:</em></strong><em> Gemini has a built-in </em><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview#ground-public"><em>Google Web Search grounding</em></a><em> capability, however, LangChain4j’s Gemini integration doesn’t yet surface this feature. I’m currently working on a pull request to support this.</em></blockquote><h3>Asking questions to your website</h3><p>An interesting use case for LLM web search grounding is for example if you want to search a particular website. I was interested in asking questions related to articles that I have posted on my personal website and blog. Let’s see, step by step, how you can implement this.</p><h3>Creating a custom search engine</h3><p>First of all, as I decided to use Google Custom Search, I created a new custom search engine. I won’t detail the steps involved in this process, as it’s explained in the <a href="https://developers.google.com/custom-search/docs/tutorial/creatingcse">documentation</a>. I created a custom search searching only the content on my website: <a href="https://glaforge.dev">glaforge.dev</a>. But you can potentially search the whole internet if you wish, or just your company website, etc.</p><p>Google Custom Search gave me an API key, as well as a Custom Search ID (csi) for my newly created custom search engine. You can test the custom search engine with that ID with this URL: <a href="https://programmablesearchengine.google.com/controlpanel/overview?cx=YOUR_CSI_HERE">https://programmablesearchengine.google.com/controlpanel/overview?cx=YOUR_CSI_HERE</a>. It gives you a Google Search-like interface where you can enter your queries. There’s also a widget that you can integrate in your website if you wish.</p><h3>Implementation</h3><p>First of all, I configure the chat model I want to use. I’m using the latest and fastest Gemini model: <a href="https://deepmind.google/technologies/gemini/flash/">Gemini 1.5 Flash</a>. I’ve saved my Google Cloud project ID and location in environment variables.</p><pre>VertexAiGeminiChatModel model = VertexAiGeminiChatModel.builder()<br>    .project(System.getenv(&quot;PROJECT_ID&quot;))<br>    .location(System.getenv(&quot;LOCATION&quot;))<br>    .modelName(&quot;gemini-1.5-flash-001&quot;)<br>    .build();</pre><p>Next, I configure my web search engine. Here, I’m using Google Search, but it could be Tavily as well. I also saved my API key and the ID of my custom web search in environment variables:</p><pre>WebSearchEngine webSearchEngine = GoogleCustomWebSearchEngine.builder()<br>    .apiKey(System.getenv(&quot;GOOGLE_CUSTOM_SEARCH_API_KEY&quot;))<br>    .csi(System.getenv(&quot;GOOGLE_CUSTOM_SEARCH_CSI&quot;))<br>//    .logRequests(true)<br>//    .logResponses(true)<br>    .build();</pre><p>Note that you can log the requests and responses, for debugging purpose.</p><p>Next, I define a <em>content retriever</em>, this is a way to let LangChain4j know that <em>content</em> can be <em>retrieved</em> from a particular tool or location:</p><pre>ContentRetriever contentRetriever = WebSearchContentRetriever.builder()<br>    .webSearchEngine(webSearchEngine)<br>    .maxResults(3)<br>    .build();</pre><p>Now, I define the contract I want to use to interact with my Gemini model, by creating my own custom search interface:</p><pre>interface SearchWebsite {<br>    String search(String query);<br>}</pre><p>This interface will be implemented by LangChain4j’s AiServices system that binds several components together: the chat language model (here, Gemini), and the web search content retriever I created above:</p><pre>SearchWebsite website = AiServices.builder(SearchWebsite.class)<br>    .chatLanguageModel(model)<br>    .contentRetriever(contentRetriever)<br>    .build();</pre><p>Then I can ask my question to the LLM, which will find the relevant information in my blog:</p><pre>String response = website.search(<br>    &quot;How can I call the Gemma model from LangChain4j?&quot;);<br><br>System.out.println(&quot;response = &quot; + response);</pre><p>If I comment out the line contentRetriever(contentRetriever), Gemini does a best effort at answering my question, but since there&#39;s nothing in its training data (before its cut-off date) about how to call the <a href="https://blog.google/technology/developers/gemma-open-models/">Gemma</a> model from LangChain4j, it is not able to provide a useful answer.</p><p>But with the web search content retriever, Gemini is able to find the right material to ground its answer, as the custom search returns my article on <a href="https://glaforge.dev/posts/2024/04/04/calling-gemma-with-ollama-and-testcontainers/">calling Gemma with Ollama, Testcontainers, and LangChain4j</a>:</p><blockquote>Based on the provided information, you can call the Gemma model from LangChain4j using the following approach:</blockquote><blockquote>1. **Use Ollama:** The articles highlight Ollama as a tool for interacting with Gemma. You would need to set up Ollama and ensure it has access to the Gemma model.<br>2. **Integrate TestContainers:** TestContainers helps you manage containerized environments for testing. You can use it to run Ollama within a container alongside LangChain4j.<br>3. **Utilize LangChain4j:** LangChain4j provides the framework for interacting with large language models. You would define your prompt, send it to Ollama (which runs Gemma), and receive the response back through LangChain4j.</blockquote><blockquote>**Example Steps:**</blockquote><blockquote>1. **Set up Ollama:** Install Ollama and configure it to use the Gemma model.<br>2. **Create a Dockerfile:** Use a Dockerfile to define an image that includes Ollama and any dependencies.<br>3. **Run Ollama in a container using TestContainers:** Start the container using TestContainers and ensure it is accessible from your LangChain4j code.<br>4. **Implement LangChain4j calls:** Use LangChain4j to construct your prompt and send it to Ollama (which will pass it to Gemma).<br>5. **Receive and process the response:** Receive the generated response from Gemma and process it as needed in your Java application.</blockquote><blockquote>**Note:** These steps provide a general approach. You will need to refer to the documentation for Ollama, TestContainers, and LangChain4j for specific implementation details.</blockquote><blockquote>This method leverages Ollama as an intermediary to access Gemma. If you have access to Google’s Gemini model directly, you might be able to integrate it with LangChain4j without the Ollama step, depending on the specific API or SDK offered by Google.</blockquote><p>The LLM found that I have to use <a href="https://ollama.com/">Ollama</a> and <a href="https://testcontainers.com/">TestContainers</a>, as explained in my article. This information wasn’t part of my query, so it proves that it really found the info in the article.</p><h3>Discussion</h3><p>The LLM based its answer on the <em>excerpts</em> contained in the search results, not the whole content of the article, so some aspects of this answer are not totally correct: For instance, you don’t have to <em>install</em> Ollama or create your own <em>Dockerfile</em>.</p><p>To make the response perfect, I believe we would have to combine web search results with Retrieval Augmented Generation, or pass the whole context of the article to the model, so that it could provide a more thorough and factual answer.</p><p>For different queries that lead to shorter answers, the answer would probably be more to the point.</p><p>Another approach is to annotate our String search(String query) method with a @SystemInstruction() with instructions that encourage the LLM to provide a shorter answer. But it&#39;s difficult to find the right balance between too long and too short, and of course without any sort of hallucinations!</p><p>For example, you can try with the following system instruction:</p><p>I got the following response:</p><blockquote>The provided information mentions using Gemma with Ollama, TestContainers, and LangChain4j. You can use Ollama, a local LLM server, and TestContainers, which provides lightweight, disposable containers, to set up a testing environment. Then, with LangChain4j, a Java library for interacting with LLMs, you can call Gemma through the Ollama server.</blockquote><p>Which is shorter and more factual, without being too short either!</p><h3>What’s next?</h3><p>In an upcoming article, I’ll show you how to use Gemini’s built-in Google Search grounding, but first, I have to finish my pull request for the LangChain4j project!</p><p>Or I can explore how to reply more precisely to queries that lead to complex answers like the above, maybe combinging a RAG approach to get the full context of the article found by the web search.</p><p>Also, the Tavily API seems to be able to return the raw content of the article, so maybe it can help giving the LLM the full context of the article to base its answers on it. So that may be worth comparing those two web search integrations too.</p><p>Stay tuned!</p><h3>Full sample code</h3><p>For reference, here is the full sample (with the system instruction approach):</p><pre>import dev.langchain4j.model.vertexai.VertexAiGeminiChatModel;<br>import dev.langchain4j.rag.content.retriever.ContentRetriever;<br>import dev.langchain4j.rag.content.retriever.WebSearchContentRetriever;<br>import dev.langchain4j.service.AiServices;<br>import dev.langchain4j.service.SystemMessage;<br>import dev.langchain4j.web.search.WebSearchEngine;<br>import dev.langchain4j.web.search.google.customsearch.GoogleCustomWebSearchEngine;<br><br>public class GroundingWithSearch {<br>  public static void main(String[] args) {<br>    VertexAiGeminiChatModel model = VertexAiGeminiChatModel.builder()<br>      .project(System.getenv(&quot;PROJECT_ID&quot;))<br>      .location(System.getenv(&quot;LOCATION&quot;))<br>      .modelName(&quot;gemini-1.5-flash-001&quot;)<br>      .build();<br><br>    WebSearchEngine webSearchEngine = GoogleCustomWebSearchEngine.builder()<br>      .apiKey(System.getenv(&quot;GOOGLE_CUSTOM_SEARCH_API_KEY&quot;))<br>      .csi(System.getenv(&quot;GOOGLE_CUSTOM_SEARCH_CSI&quot;))<br>//    .logRequests(true)<br>//    .logResponses(true)<br>      .build();<br><br>    ContentRetriever contentRetriever = WebSearchContentRetriever.builder()<br>      .webSearchEngine(webSearchEngine)<br>      .maxResults(3)<br>      .build();<br><br>    interface SearchWebsite {<br>      @SystemMessage(&quot;&quot;&quot;<br>        Provide a paragraph-long answer, not a long step by step explanation.<br>        Reply with &quot;I don&#39;t know the answer&quot; if the provided information isn&#39;t relevant.<br>        &quot;&quot;&quot;)<br>      String search(String query);<br>    }<br><br>    SearchWebsite website = AiServices.builder(SearchWebsite.class)<br>      .chatLanguageModel(model)<br>      .contentRetriever(contentRetriever)<br>      .build();<br><br>    String response = website.search(<br>      &quot;How can I call the Gemma model from LangChain4j?&quot;);<br><br>    System.out.println(&quot;response = &quot; + response);<br>  }<br>}</pre><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/05/28/grounding-gemini-with-web-search-in-langchain4j/"><em>https://glaforge.dev</em></a><em> on May 28, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2f949bd69492" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/grounding-gemini-with-web-search-results-in-langchain4j-2f949bd69492">Grounding Gemini with Web Search results in LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Calling Gemma with Ollama, TestContainers, and LangChain4j]]></title>
            <link>https://medium.com/google-cloud/calling-gemma-with-ollama-testcontainers-and-langchain4j-fbfe220ca715?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/fbfe220ca715</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[ollama]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[testcontainer]]></category>
            <category><![CDATA[langchain4j]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Wed, 03 Apr 2024 00:00:55 GMT</pubDate>
            <atom:updated>2024-04-05T04:50:25.157Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MaT34TRoitiU1RMM.jpg" /></figure><p>Lately, for my Generative AI powered Java apps, I’ve used the <a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> multimodal large language model from Google. But there’s also <a href="https://blog.google/technology/developers/gemma-open-models/">Gemma</a>, its little sister model.</p><p>Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Gemma is available in two sizes: 2B and 7B. Its weights are freely available, and its small size means you can run it on your own, even on your laptop. So I was curious to give it a run with <a href="https://docs.langchain4j.dev/">LangChain4j</a>.</p><h3>How to run Gemma</h3><p>There are many ways to run Gemma: in the cloud, via <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">Vertex AI</a> with a click of a button, or <a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm">GKE</a> with some GPUs, but you can also run it locally with <a href="https://github.com/tjake/Jlama">Jlama</a> or <a href="https://github.com/google/gemma.cpp">Gemma.cpp</a>.</p><p>Another good option is to run Gemma with <a href="https://ollama.com/">Ollama</a>, a tool that you install on your machine, and which lets you run small models, like Llama 2, Mistral, and <a href="https://ollama.com/library">many others</a>. They quickly added support for <a href="https://ollama.com/library/gemma">Gemma</a> as well.</p><p>Once installed locally, you can run:</p><pre>ollama run gemma:2b<br>ollama run gemma:7b</pre><p>Cherry on the cake, the <a href="https://glaforge.dev/posts/2024/04/04/calling-gemma-with-ollama-and-testcontainers/">LangChain4j</a> library provides an <a href="https://docs.langchain4j.dev/integrations/language-models/ollama">Ollama module</a>, so you can plug Ollama supported models in your Java applications easily.</p><h3>Containerization</h3><p>After a great discussion with my colleague <a href="https://twitter.com/ddobrin">Dan Dobrin</a> who had worked with Ollama and TestContainers (<a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/blob/main/sessions/next24/books-genai-vertex-langchain4j/src/test/java/services/OllamaContainerTest.java">#1</a> and<a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/blob/main/sessions/next24/books-genai-vertex-langchain4j/src/test/java/services/OllamaChatModelTest.java#L37">#2</a>) in his <a href="https://github.com/GoogleCloudPlatform/serverless-production-readiness-java-gcp/tree/main">serverless production readiness workshop</a>, I decided to try the approach below.</p><p>Which brings us to the last piece of the puzzle: Instead of having to install and run Ollama on my computer, I decided to use Ollama within a container, handled by <a href="https://testcontainers.com/">TestContainers</a>.</p><p>TestContainers is not only useful for testing, but you can also use it for driving containers. There’s even a specific <a href="https://java.testcontainers.org/modules/ollama/">OllamaContainer</a> you can take advantage of!</p><p>So here’s the whole picture:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lHxJaKf0ALEEnoJS.png" /></figure><h3>Time to implement this approach!</h3><p>You’ll find the code in the Github <a href="https://github.com/glaforge/gemini-workshop-for-java-developers/blob/main/app/src/main/java/gemini/workshop/CallGemma.java">repository</a> accompanying my recent <a href="https://codelabs.developers.google.com/codelabs/gemini-java-developers">Gemini workshop</a></p><p>Let’s start with the easy part, interacting with an Ollama supported model with LangChain4j:</p><pre>OllamaContainer ollama = createGemmaOllamaContainer();<br>ollama.start();<br><br>ChatLanguageModel model = OllamaChatModel.builder()<br>    .baseUrl(String.format(&quot;http://%s:%d&quot;, ollama.getHost(), ollama.getFirstMappedPort()))<br>    .modelName(&quot;gemma:2b&quot;)<br>    .build();<br><br>String response = model.generate(&quot;Why is the sky blue?&quot;);<br><br>System.out.println(response);</pre><ul><li>You run an Ollama test container.</li><li>You create an Ollama chat model, by pointing at the address and port of the container.</li><li>You specify the model you want to use.</li><li>Then, you just need to call model.generate(yourPrompt) as usual.</li></ul><p>Easy? Now let’s have a look at the trickier part, my local method that creates the Ollama container:</p><pre>// check if the custom Gemma Ollama image exists already<br>List&lt;Image&gt; listImagesCmd = DockerClientFactory.lazyClient()<br>    .listImagesCmd()<br>    .withImageNameFilter(TC_OLLAMA_GEMMA_2_B)<br>    .exec();<br><br>if (listImagesCmd.isEmpty()) {<br>    System.out.println(&quot;Creating a new Ollama container with Gemma 2B image...&quot;);<br>    OllamaContainer ollama = new OllamaContainer(&quot;ollama/ollama:0.1.26&quot;);<br>    ollama.start();<br>    ollama.execInContainer(&quot;ollama&quot;, &quot;pull&quot;, &quot;gemma:2b&quot;);<br>    ollama.commitToImage(TC_OLLAMA_GEMMA_2_B);<br>    return ollama;<br>} else {<br>    System.out.println(&quot;Using existing Ollama container with Gemma 2B image...&quot;);<br>    // Substitute the default Ollama image with our Gemma variant<br>    return new OllamaContainer(<br>        DockerImageName.parse(TC_OLLAMA_GEMMA_2_B)<br>            .asCompatibleSubstituteFor(&quot;ollama/ollama&quot;));<br>}</pre><p>You need to create a derived Ollama container that pulls in the Gemma model. Either this image was already created beforehand, or if it doesn’t exist yet, you create it.</p><p>Use the Docker Java client to check if the custom Gemma image exists. If it doesn’t exist, notice how TestContainers let you create an image derived from the base Ollama image, pull the Gemma model, and then commit that image to your local Docker registry.</p><p>Otherwise, if the image already exists (ie. you created it in a previous run of the application), you’re just going to tell TestContainers that you want to substitute the default Ollama image with your Gemma-powered variant.</p><h3>And voila!</h3><p>You can <strong>call Gemma locally on your laptop, in your Java apps, using LangChain4j</strong>, without having to install and run Ollama locally (but of course, you need to have a Docker daemon running).</p><p>Big thanks to <a href="https://twitter.com/ddobrin">Dan Dobrin</a> for the approach, and to <a href="https://twitter.com/bsideup">Sergei</a>, <a href="https://twitter.com/EdduMelendez">Eddú</a> and <a href="https://twitter.com/shelajev">Oleg</a> from TestContainers for the help and useful pointers.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/04/04/calling-gemma-with-ollama-and-testcontainers/"><em>https://glaforge.dev</em></a><em> on April 3, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fbfe220ca715" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/calling-gemma-with-ollama-testcontainers-and-langchain4j-fbfe220ca715">Calling Gemma with Ollama, TestContainers, and LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gemini codelab for Java developers using LangChain4j]]></title>
            <link>https://medium.com/google-cloud/gemini-codelab-for-java-developers-using-langchain4j-769fbd419756?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/769fbd419756</guid>
            <category><![CDATA[langchain4j]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[java]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Wed, 27 Mar 2024 00:00:46 GMT</pubDate>
            <atom:updated>2024-03-29T03:27:13.378Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*f6qD8NYosB2mVZD2.jpg" /></figure><p>No need to be a Python developer to do Generative AI! If you’re a Java developer, you can take advantage of <a href="https://docs.langchain4j.dev/">LangChain4j</a> to implement some advanced LLM integrations in your Java applications. And if you’re interested in using <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini</a>, one of the best models available, I invite you to have a look at the following “codelab” that I worked on:</p><p><a href="https://codelabs.developers.google.com/codelabs/gemini-java-developers">Codelab — Gemini for Java Developers using LangChain4j</a></p><p>In this workshop, you’ll find various examples covering the following use cases, in <em>crescendo</em> approach:</p><ul><li>Making your fist call to Gemini (streaming &amp; non-streaming)</li><li>Maintaining a conversation</li><li>Taking advantage of multimodality by analysing images with your prompts</li><li>Extracting structured information from unstructured text</li><li>Using prompt templates</li><li>Doing text classification with few-shot prompting</li><li>Implementing Retrieval Augmented Generation to chat with your documentation</li><li>How to do Function Calling to expand the LLM to interact with external APIs and services</li></ul><p>You’ll find all the <a href="https://github.com/glaforge/gemini-workshop-for-java-developers">code samples on Github</a>.</p><p>If you’re attending Devoxx France, be sure to attend the <a href="https://www.devoxx.fr/en/schedule/talk/?id=40285">Hands-on-Lab workshop</a> with my colleagues <a href="https://twitter.com/meteatamel">Mete Atamel</a> and <a href="https://twitter.com/val_deleplace">Valentin Deleplace</a> who will guide you through this codelab.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/03/27/gemini-codelab-for-java-developers/"><em>https://glaforge.dev</em></a><em> on March 27, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=769fbd419756" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gemini-codelab-for-java-developers-using-langchain4j-769fbd419756">Gemini codelab for Java developers using LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualize PaLM-based LLM tokens]]></title>
            <link>https://medium.com/google-cloud/visualize-palm-based-llm-tokens-8760b3122c0f?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/8760b3122c0f</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 05 Feb 2024 00:00:45 GMT</pubDate>
            <atom:updated>2024-02-05T15:46:50.859Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*iUeKnOWdjYkVi3BY.jpg" /></figure><p>As I was working on tweaking the Vertex AI text embedding model in <a href="https://github.com/langchain4j">LangChain4j</a>, I wanted to better understand how the textembedding-gecko<a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">model</a> tokenizes the text, in particular when we implement the <a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation</a> approach.</p><p>The various PaLM-based models offer a computeTokens endpoint, which returns a list of tokens (encoded in Base 64) and their respective IDs.</p><blockquote><strong><em>Note:</em></strong><em> At the time of this writing, there’s no equivalent endpoint for Gemini models.</em></blockquote><p>So I decided to create a <a href="https://tokens-lpj6s2duga-ew.a.run.app/">small application</a> that lets users:</p><ul><li>input some text,</li><li>select a model,</li><li>calculate the number of tokens,</li><li>and visualize them with some nice pastel colors.</li></ul><p>The available PaLM-based models are:</p><ul><li>textembedding-gecko</li><li>textembedding-gecko-multilingual</li><li>text-bison</li><li>text-unicorn</li><li>chat-bison</li><li>code-gecko</li><li>code-bison</li><li>codechat-bison</li></ul><p>You can <a href="https://tokens-lpj6s2duga-ew.a.run.app/">try the application</a> online.</p><p>And also have a look at the <a href="https://github.com/glaforge/llm-text-tokenization">source code</a> on Github. It’s a <a href="https://micronaut.io/">Micronaut</a> application. I serve the static assets as explained in my recent <a href="https://glaforge.dev/posts/2024/01/21/serving-static-assets-with-micronaut/">article</a>. I deployed the application on <a href="https://cloud.run/">Google Cloud Run</a>, the easiest way to deploy a container, and let it auto-scale for you. I did a source based deployment, as explained at the bottom <a href="https://glaforge.dev/posts/2022/10/24/build-deploy-java-17-apps-on-cloud-run-with-cloud-native-buildpacks-on-temurin/">here</a>.</p><p>And <em>voilà</em> I can visualize my LLM tokens!</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/02/05/visualize-palm-based-llm-tokens/"><em>https://glaforge.dev</em></a><em> on February 5, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8760b3122c0f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/visualize-palm-based-llm-tokens-8760b3122c0f">Visualize PaLM-based LLM tokens</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Image generation with Imagen and LangChain4j]]></title>
            <link>https://medium.com/google-cloud/image-generation-with-imagen-and-langchain4j-61ca08ae6aac?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/61ca08ae6aac</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[imagen]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[langchain]]></category>
            <category><![CDATA[generative-ai-use-cases]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Thu, 01 Feb 2024 00:00:35 GMT</pubDate>
            <atom:updated>2024-02-02T03:38:58.802Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0iPZQvd0ZVDuJjeC.jpg" /></figure><p>This week <a href="https://github.com/langchain4j">LangChain4j</a>, the LLM orchestration framework for Java developers, released version <a href="https://github.com/langchain4j/langchain4j/releases/tag/0.26.1">0.26.1</a>, which contains my first significant contribution to the open source project: <strong>support for the Imagen image generation model</strong>.</p><p><strong>Imagen</strong> is a text-to-image diffusion model that was <a href="https://imagen.research.google/">announced</a> last year. And it recently upgraded to <a href="https://deepmind.google/technologies/imagen-2/">Imagen v2</a>, with even higher quality graphics generation. As I was curious to integrate it in some of my generative AI projects, I thought that would be a great first <a href="https://github.com/langchain4j/langchain4j/pull/456">contribution</a> to LangChain4j.</p><blockquote><strong><em>Caution:</em></strong><em> At the time of this writing, image generation is still only for allow-listed accounts.</em></blockquote><blockquote><em>Furthermore, to run the snippets covered below, you should have an account on Google Cloud Platform, created a project, configured a billing account, enabled the Vertex AI API, and authenticated with the gcloud SDK and the command: </em><em>gcloud auth application-default login.</em></blockquote><p>Now let’s dive in how to use Imagen v1 and v2 with LangChain4j in Java!</p><h3>Generate your first images</h3><p>In the following examples, I’m using the following constants, to point at my project details, the endpoint, the region, etc:</p><pre>private static final String ENDPOINT = &quot;us-central1-aiplatform.googleapis.com:443&quot;;<br>private static final String LOCATION = &quot;us-central1&quot;;<br>private static final String PROJECT = &quot;YOUR_PROJECT_ID&quot;;<br>private static final String PUBLISHER = &quot;google&quot;;</pre><p>First, we’re going to create an instance of the model:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>    .endpoint(ENDPOINT)<br>    .location(LOCATION)<br>    .project(PROJECT)<br>    .publisher(PUBLISHER)<br>    .modelName(&quot;imagegeneration@005&quot;)<br>    .maxRetries(2)<br>    .withPersisting()<br>    .build();</pre><p>There are 2 models you can use:</p><ul><li>imagegeneration@005 corresponds to Imagen 2</li><li>imagegeneration@002 is the previous version (Imagen 1)</li></ul><p>In this article, we’ll use both models. Why? Because currently Imagen 2 doesn’t support image editing, so we’ll have to use Imagen 1 for that purpose.</p><p>The configuration above uses withPersisting() to save the generated images in a temporary folder on your system. If you don&#39;t persist the image files, the content of the image is avaiable as Base 64 encoded bytes in the Images objects returned. You can also specify persistTo(somePath) to specify a particular directory where you want the generated files to be saved.</p><p>Let’s create our first image:</p><pre>Response&lt;Image&gt; imageResponse = imagenModel.generate(<br>    &quot;watercolor of a colorful parrot drinking a cup of coffee&quot;);</pre><p>The Response object wraps the created Image. You can get the Image by calling imageResponse.getContent(). And you can retrieve the URL of the image (if saved locally) with imageResponse.getContent().url(). The Base 64 encoded bytes can be retrieved with imageResponse.getContent().base64Data()</p><p>Some other tweaks to the model configuration:</p><ul><li>Specify the <strong>language</strong> of the prompt: language(&quot;ja&quot;) (if the language is not officially supported, it&#39;s usually translated back to English anyway).</li><li>Define a <strong>negative prompt</strong> with things you don’t want to see in the picture: negativePrompt(&quot;black feathers&quot;).</li><li>Use a particular <strong>seed</strong> to always generate the same image with the same seed: seed(1234L).</li></ul><p>So if you want to generate a picture of a pizza with a prompt in Japanese, but you don’t want to have pepperoni and pineapple, you could configure your model and generate as follows:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@005&quot;)<br>        .language(&quot;ja&quot;)<br>        .negativePrompt(&quot;pepperoni, pineapple&quot;)<br>        .maxRetries(2)<br>        .withPersisting()<br>        .build();<br><br>Response&lt;Image&gt; imageResponse = imagenModel.generate(&quot;ピザ&quot;); // pizza</pre><h3>Image editing with Imagen 1</h3><p>With Imagen 1, you can <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/edit-images?hl=en">edit</a> existing images:</p><ul><li><strong>mask-based editing:</strong> you can specify a mask, a black &amp; white image where the white parts are the corresponding parts of the original image that should be edited,</li><li><strong>mask free editing:</strong> where you just give a prompt and let the model figure out what should be edited on its own or following the prompt.</li></ul><p>When generating and editing with Imagen 1, you can also configure the model to use a particular style (with Imagen 2, you just specify it in the prompt) with sampleImageStyle(VertexAiImageModel.ImageStyle.photograph):</p><p>- photograph<br>- digital_art<br>- landscape<br>- sketch<br>- watercolor<br>- cyberpunk<br>- pop_art</p><p>When editing an image, you may wish to decide how strong or not the modification should be, with .guidanceScale(100). Usually, between 0 and 20 or so, it&#39;s lightly edited, between 20 and 100 it&#39;s getting more impactful edits, and 100 and above it&#39;s the maximum edition level.</p><p>Let’s say I generated an image of a lush forrest (I’ll use that as my original image):</p><pre>VertexAiImageModel model = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .seed(19707L)<br>        .sampleImageStyle(VertexAiImageModel.ImageStyle.photograph)<br>        .guidanceScale(100)<br>        .maxRetries(4)<br>        .withPersisting()<br>        .build();<br><br>Response&lt;Image&gt; forestResp = model.generate(&quot;lush forest&quot;);</pre><p>Now I want to edit my forrest to add a small red tree in the bottom of the image. I’m loading a black and white mask image with a white square at the bottom. And I pass the original image, the mask image, and the modification prompt, to the new edit() method:</p><pre>URI maskFileUri = getClass().getClassLoader().getResource(&quot;mask.png&quot;).toURI();<br><br>Response&lt;Image&gt; compositeResp = model.edit(<br>        forestResp.content(),              // original image to edit<br>        fromPath(Paths.get(maskFileUri)),  // the mask image<br>        &quot;red trees&quot;                        // the new prompt<br>);</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AfQ24hvdH9hoTLsT.jpg" /></figure><p>Another kind of editing you can do is to upscale an existing image. As far as I know, it’s only supported for Imagen v1 for now, so we’ll continue with that model.</p><p>In this example, we’ll generate an image of 1024x1024 pixels, and we’ll scale it to 4096x4096:</p><pre>VertexAiImageModel imagenModel = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .sampleImageSize(1024)<br>        .withPersisting()<br>        .persistTo(defaultTempDirPath)<br>        .maxRetries(3)<br>        .build();<br><br>Response&lt;Image&gt; imageResponse =<br>        imagenModel.generate(&quot;A black bird looking itself in an antique mirror&quot;);<br><br>VertexAiImageModel imagenModelForUpscaling = VertexAiImageModel.builder()<br>        .endpoint(ENDPOINT)<br>        .location(LOCATION)<br>        .project(PROJECT)<br>        .publisher(PUBLISHER)<br>        .modelName(&quot;imagegeneration@002&quot;)<br>        .sampleImageSize(4096)<br>        .withPersisting()<br>        .persistTo(defaultTempDirPath)<br>        .maxRetries(3)<br>        .build();<br><br>Response&lt;Image&gt; upscaledImageResponse =<br>        imagenModelForUpscaling.edit(imageResponse.content(), &quot;&quot;);</pre><p>And now you have a much bigger image!</p><h3>Conclusion</h3><p>That’s about it for image generation and editing with <strong>Imagen</strong> in <strong>LangChain4j</strong> today! Be sure to use LangChain4j <strong>v0.26.1</strong> which contains that new integration. And I’m looking forward to seeing the pictures you generate with it!</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2024/02/01/image-generation-with-imagen-and-langchain4j/"><em>https://glaforge.dev</em></a><em> on February 1, 2024.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=61ca08ae6aac" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/image-generation-with-imagen-and-langchain4j-61ca08ae6aac">Image generation with Imagen and LangChain4j</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gemini Function Calling]]></title>
            <link>https://medium.com/google-cloud/gemini-function-calling-1585c044d28d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/1585c044d28d</guid>
            <category><![CDATA[java]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 22 Dec 2023 00:00:37 GMT</pubDate>
            <atom:updated>2023-12-28T07:03:58.457Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yW4nG0dXoxO1tsdaOYzWuA.png" /></figure><p>A promising feature of the Gemini large language model released recently by <a href="https://deepmind.google/">Google DeepMind</a>, is the support for <a href="https://ai.google.dev/docs/function_calling">function calls</a>. It’s a way to supllement the model, by letting it know an external functions or APIs can be called. So you’re not limited by the knowledge cut-off of the model: instead, in the flow of the conversation with the model, you can pass a list of functions the model will know are available to get the information it needs, to complete the generation of its answer.</p><p>For example, if you want to ask the model about the weather, it doesn’t have the realtime information about the weather forecast. But we can tell it that there’s a function that can be called, to get the forecast for a given location. Internally, the model will acknowledge it doesn’t know the answer about the weather, but it will request that you call an external function that you describe, using a specific set of parameters which correspond to the user’s request.</p><p>Just days ago, I wrote about how to <a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/">get started with Gemini in Java</a>. In that article, we explored how to use the hand-written Java SDK that is available to interact with Gemini from Java. However, the Java SDK doesn’t yet expose all the features of the model: in particular, function calling is missing. But not all hope is lost! Because under the hood, the SDK relies on the generated protobuf classes library, which exposes everything!</p><blockquote><em>Soon, Gemini will be supported by </em><a href="https://github.com/langchain4j/langchain4j"><em>LangChain4j</em></a><em>, and the Java SDK will also provide an easier way to take care of function calling. But in this article, I wanted to explore the use of the internal protobuf classes, to see how to best implement its support in the SDK.</em></blockquote><p>Let’s go step by step!</p><p>Instead of using the GenerativeModel API from the SDK, we&#39;ll go straight with the PredictionServiceClient:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>  PredictionServiceClient client = vertexAI.getPredictionServiceClient();<br>  ...<br>}</pre><p>We need to prepare a function declaration to describe the kind of functions that the LLM can ask us to call, and we’ll wrap it in a Tool:</p><pre>FunctionDeclaration functionDeclaration = FunctionDeclaration.newBuilder()<br>    .setName(&quot;getCurrentWeather&quot;)<br>    .setDescription(&quot;Get the current weather in a given location&quot;)<br>    .setParameters(<br>        Schema.newBuilder()<br>            .setType(Type.OBJECT)<br>            .putProperties(&quot;location&quot;, Schema.newBuilder()<br>                .setType(Type.STRING)<br>                .setDescription(&quot;location&quot;)<br>                .build()<br>            )<br>            .addRequired(&quot;location&quot;)<br>            .build()<br>    )<br>    .build();<br><br>Tool tool = Tool.newBuilder()<br>    .addFunctionDeclarations(functionDeclaration)<br>    .build();</pre><p>Functions are described using classes that represent a subset of the OpenAPI 3 specification.</p><blockquote><em>This is important to provide descriptions for the functions and its parameters, as the LLM will use that information to figure out which function to call, and which parameters should be passed.</em></blockquote><p>Next, let’s prepare a question asking about the weather in Paris, and configuring the text generation request with that prompt and the tool defined above:</p><pre>String resourceName = String.format(<br>    &quot;projects/%s/locations/%s/publishers/google/models/%s&quot;,<br>    vertexAI.getProjectId(), vertexAI.getLocation(), modelName);<br><br>Content questionContent =<br>    ContentMaker.fromString(&quot;What&#39;s the weather in Paris?&quot;);<br><br>GenerateContentRequest questionContentRequest =<br>    GenerateContentRequest.newBuilder()<br>        .setEndpoint(resourceName)<br>        .setModel(resourceName)<br>        .addTools(tool)<br>        .addContents(questionContent)<br>        .build();<br><br>ResponseStream&lt;GenerateContentResponse&gt; responseStream =<br>    new ResponseStream&lt;&gt;(new ResponseStreamIteratorWithHistory&lt;&gt;(<br>        client<br>            .streamGenerateContentCallable()<br>            .call(questionContentRequest)<br>            .iterator())<br>);<br><br>GenerateContentResponse generateContentResponse =<br>    responseStream.stream().findFirst().get();<br>Content callResponseContent =<br>    generateContentResponse.getCandidates(0).getContent();</pre><p>If you print the callResponseContent variable, you&#39;ll see that it contains a function call request, suggesting that you should call the predefined function with the parameter of Paris:</p><pre>role: &quot;model&quot;<br>parts {<br>  function_call {<br>    name: &quot;getCurrentWeather&quot;<br>    args {<br>      fields {<br>        key: &quot;location&quot;<br>        value {<br>          string_value: &quot;Paris&quot;<br>        }<br>      }<br>    }<br>  }<br>}</pre><p>At that point, as the developer, it’s your turn to work a little, and make the call to that function yourself! Let’s pretend I called an external Web Service that gives weather information, and that it returns some JSON payload that would look like so:</p><pre>{<br>  &quot;weather&quot;: &quot;sunny&quot;,<br>  &quot;location&quot;: &quot;Paris&quot;<br>}</pre><p>We need now to create a function response structure to pass that information back to the LLM:</p><pre>Content contentFnResp = Content.newBuilder()<br>    .addParts(Part.newBuilder()<br>        .setFunctionResponse(<br>            FunctionResponse.newBuilder()<br>                .setResponse(<br>                    Struct.newBuilder()<br>                        .putFields(&quot;weather&quot;,<br>                            Value.newBuilder().setStringValue(&quot;sunny&quot;).build())<br>                        .putFields(&quot;location&quot;,<br>                            Value.newBuilder().setStringValue(&quot;Paris&quot;).build())<br>                        .build()<br>                )<br>                .build()<br>        )<br>        .build())<br>    .build();</pre><p>Then, since LLMs are actually stateless beasts, we need to give it the whole context of the conversation again, passing the query, the function call response the model suggested us to make, as well as the response we got from the external weather service:</p><pre>GenerateContentRequest generateContentRequest = GenerateContentRequest.newBuilder()<br>    .setEndpoint(resourceName)<br>    .setModel(resourceName)<br>    .addContents(questionContent)<br>    .addContents(callResponseContent)<br>    .addContents(contentFnResp)<br>    .addTools(tool)<br>    .build();</pre><p>And to finish, we’ll invoke the client one last time with that whole dialog and information, and print a response out:</p><pre>responseStream = new ResponseStream&lt;&gt;(new ResponseStreamIteratorWithHistory&lt;&gt;(<br>    client<br>        .streamGenerateContentCallable()<br>        .call(generateContentRequest)<br>        .iterator())<br>);<br><br>for (GenerateContentResponse resp : responseStream) {<br>    System.out.println(ResponseHandler.getText(resp));<br>}</pre><p>And happily, Gemini will reply to us that:</p><pre>The weather in Paris is sunny.</pre><p>What a lovely way to start the holiday season with a nice and sunny weather!</p><p>I wish you all happy year end festivities, and I look forward to seeing you next year. Hopefully next month, I’ll be able to show you some cool new SDK features or the LangChain4j integration! Thanks for reading.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/22/gemini-function-calling/"><em>https://glaforge.dev</em></a><em> on December 22, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1585c044d28d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/gemini-function-calling-1585c044d28d">Gemini Function Calling</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualize and Inspect Workflows Executions]]></title>
            <link>https://medium.com/google-cloud/visualize-and-inspect-workflows-executions-00aafe188d3d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/00aafe188d3d</guid>
            <category><![CDATA[gcp-workflows]]></category>
            <category><![CDATA[workflow]]></category>
            <category><![CDATA[debugging]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Fri, 22 Dec 2023 00:00:27 GMT</pubDate>
            <atom:updated>2023-12-22T14:23:56.467Z</atom:updated>
            <content:encoded><![CDATA[<p>When using a service like Google Cloud <a href="https://cloud.google.com/workflows/">Workflows</a>, in particular as your workflows get bigger, it can be difficult to understand what’s going on under the hood. With multiple branches, step jumps, iterations, and also parallel branches and iterations, if your workflow fails during an execution, until now, you had to check the execution status, or go deep through the logs to find more details about the failed step.</p><p>I have good news for you! Workflows recently added some deeper introspection capability: you can now <a href="https://cloud.google.com/workflows/docs/debug-steps">view the history of execution steps</a>. From the Google Cloud console, you can see the lists of steps, and see the logical flow between them. The usual workflow visualisation will also highlight in green the successful steps, and in red the failed one. Of course, it is also possible to make a curl call to get the JSON of the <a href="https://cloud.google.com/workflows/docs/debug-steps#list-entries">list of executed steps</a>.</p><p>Let’s have a look!</p><p>In the console, when you click on an execution, in the summary tab, you&#39;ll see not only the failed step, but also the nice workflow graph colored green and red:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xomRWaQkCtyo6N5S.png" /></figure><p>That way, you know which path the execution followed, in a visual manner. But you can also see the actual list of steps executed, with more details, by clicking on the steps tab:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/888/0*rgzpqpjUpJ3YmsD8.png" /></figure><p>From this table, the filter will let you further refine particular type of steps you’d like to investigate, or visualise the steps of a subworkflow only, etc.</p><p>This is a nice improvement to the developer experience, and for your ops team, to better understand what happens during your workflow executions! Feel free to read more about this new capabability in the documentation about <a href="https://cloud.google.com/workflows/docs/debug-steps">viewing the history of execution steps</a>.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/22/visualize-and-inspect-workflows-executions/"><em>https://glaforge.dev</em></a><em> on December 22, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=00aafe188d3d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/visualize-and-inspect-workflows-executions-00aafe188d3d">Visualize and Inspect Workflows Executions</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hands on Codelabs to dabble with Large Language Models in Java]]></title>
            <link>https://medium.com/google-cloud/hands-on-codelabs-to-dabble-with-large-language-models-in-java-ee7bc330f5fe?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/ee7bc330f5fe</guid>
            <category><![CDATA[llm]]></category>
            <category><![CDATA[generative-ai]]></category>
            <category><![CDATA[java]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[langchain]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Mon, 18 Dec 2023 00:00:36 GMT</pubDate>
            <atom:updated>2023-12-19T06:49:15.192Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QAqyUui7Mp-t2o8U.png" /></figure><p>Hot on the heels of the <a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/">release of Gemini</a>, I’d like to share a couple of resources I created to get your hands on large language models, using <a href="https://github.com/langchain4j/">LangChain4J</a>, and the <a href="https://ai.google/discover/palm2/">PaLM 2</a> model. Later on, I’ll also share with you articles and codelabs that take advantage of Gemini, of course.</p><p>The PaLM 2 model supports 2 modes:</p><ul><li>text generation,</li><li>and chat.</li></ul><p>In the 2 codelabs, you’ll need to have created an account on Google Cloud, and created a project. The codelabs will guide you through the steps to setup the environment, and show you how to use the Google Cloud built-in shell and code editor, to develop in the cloud.</p><p>You should be a Java developer, as the examples are in Java, use the <a href="https://github.com/langchain4j/">LangChain4J</a> project, and Maven for building the code.</p><h3>Generative AI text generation in Java with PaLM and LangChain4J</h3><p>In the first <a href="https://codelabs.developers.google.com/codelabs/genai-text-gen-java-palm-langchain4j?hl=en#0">codelab</a> you can explore:</p><ul><li>how to make your first call to PaLM for simple question/answer scenarios</li><li>how to extract structured data out of unstructured text</li><li>how to use prompts and prompt templates</li><li>how to classify text, with an example on sentiment analysis</li></ul><h3>Generative AI powered chat with users and docs in Java with PaLM and LangChain4J</h3><p>In the second <a href="https://codelabs.developers.google.com/codelabs/genai-chat-java-palm-langchain4j?hl=en#0">codelab</a> you’ll use the chat model to learn:</p><ul><li>how to create your first chat with the PaLM model</li><li>how to give your chatbot a personality, with an example with a chess player</li><li>how to extract structured data out of unstructured text using LangChain4J’s AiServices and its annotations</li><li>how to implement Retrieval Augmented Generation (RAG) to answer questions about your own documentation</li></ul><h3>Going further with Generative AI</h3><p>If you’re interested in going further with Generative AI, and learn more, feel free to <a href="https://goo.gle/generativeai">join the Google Cloud Innovators program</a>.</p><p>Google Cloud Innovators is <strong>free</strong> and includes:</p><ul><li>live discussions, AMAs, and roadmap sessions to learn the latest directly from Googlers,</li><li>the latest Google Cloud news right in your inbox,</li><li>digital badge and video conference background,</li><li>500 credits of labs and learning on Skills Boost.</li></ul><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/18/get-hands-on-codelabs-to-dabble-with-llms/"><em>https://glaforge.dev</em></a><em> on December 18, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ee7bc330f5fe" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/hands-on-codelabs-to-dabble-with-large-language-models-in-java-ee7bc330f5fe">Hands on Codelabs to dabble with Large Language Models in Java</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Get Started with Gemini in Java]]></title>
            <link>https://medium.com/google-cloud/get-started-with-gemini-in-java-923f2069ea4d?source=rss-431147437aeb------2</link>
            <guid isPermaLink="false">https://medium.com/p/923f2069ea4d</guid>
            <category><![CDATA[java]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Guillaume Laforge]]></dc:creator>
            <pubDate>Wed, 13 Dec 2023 00:00:25 GMT</pubDate>
            <atom:updated>2023-12-18T02:13:02.158Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Logo of the Gemini large language model launched by Google" src="https://cdn-images-1.medium.com/max/1024/0*4ohfuLfaP-ZAbo5_" /></figure><p>Google announced today the availability of <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-support-on-vertex-ai">Gemini</a>, its latest and more powerful Large Language Model. Gemini is <strong>multimodal</strong>, which means it’s able to consume not only text, but also images or videos.</p><p>I had the pleasure of working on the Java samples and help with the Java SDK, with wonderful engineer colleagues, and I’d like to share some examples of <strong>what you can do with Gemini, using Java</strong>!</p><p>First of all, you’ll need to have an account on Google Cloud and created a project. The Vertex AI API should be enabled, to be able to access the Generative AI services, and in particular the Gemini large language model. Be sure to check out the <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal?hl=en">instructions</a>.</p><h3>Preparing your project build</h3><p>To get started with some coding, you’ll need to create a Gradle or a Maven build file that requires the Google Cloud libraries BOM, and the google-cloud-vertexai library. Here&#39;s an example with Maven:</p><pre>...<br>&lt;dependencyManagement&gt;<br>    &lt;dependencies&gt;<br>        &lt;dependency&gt;<br>            &lt;artifactId&gt;libraries-bom&lt;/artifactId&gt;<br>            &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;<br>            &lt;scope&gt;import&lt;/scope&gt;<br>            &lt;type&gt;pom&lt;/type&gt;<br>            &lt;version&gt;26.29.0&lt;/version&gt;<br>        &lt;/dependency&gt;<br>    &lt;/dependencies&gt;<br>&lt;/dependencyManagement&gt;<br><br>&lt;dependencies&gt;<br>    &lt;dependency&gt;<br>        &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;<br>        &lt;artifactId&gt;google-cloud-vertexai&lt;/artifactId&gt;<br>    &lt;/dependency&gt;<br>    ...<br>&lt;/dependencies&gt;<br>...</pre><h3>Your first queries</h3><p>Now let’s have a look at our first multimodal example, mixing text prompts and images:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>    byte[] imageBytes = Base64.getDecoder().decode(dataImageBase64);<br><br>    GenerativeModel model = new GenerativeModel(&quot;gemini-pro-vision&quot;, vertexAI);<br>    GenerateContentResponse response = model.generateContent(<br>        ContentMaker.fromMultiModalData(<br>            &quot;What is this image about?&quot;,<br>            PartMaker.fromMimeTypeAndData(&quot;image/jpg&quot;, imageBytes)<br>        ));<br><br>    System.out.println(ResponseHandler.getText(response));<br>}</pre><p>You instantiate VertexAI with your Google Cloud project ID, and the region location of your choice. To pass images to Gemini, you should either pass the bytes directly, or you can pass a URI of an image stored in a cloud storage bucket (like gs://my-bucket/my-img.jpg). You create an instance of the model. Here, I&#39;m using gemini-pro-vision. But later on, a gemini-ultra-vision model will also be available. Let&#39;s ask the model to generate content with the generateContent() method, by passing both a text prompt, and also an image. The ContentMaker and PartMaker classes are helpers to further simplify the creation of more advanced prompts that mix different modalities. But you could also just pass a simple string as argument of the generateContent() method. The ResponseHandler utility will retrieve all the text of the answer of the model.</p><p>Instead of getting the whole output once all the text is generated, you can also adopt a streaming approach:</p><pre>model.generateContentStream(&quot;Why is the sky blue?&quot;)<br>    .stream()<br>    .forEach(System.out::print);</pre><p>You can also iterate over the stream with a for loop:</p><pre>ResponseStream&lt;GenerateContentResponse&gt; responseStream =<br>    model.generateContentStream(&quot;Why is the sky blue?&quot;);<br><br>for (GenerateContentResponse responsePart: responseStream) {<br>    System.out.print(ResponseHandler.getText(responsePart));<br>}</pre><h3>Let’s chat!</h3><p>Gemini is a multimodal model, and it’s actually both a text generation model, but also a chat model. So you can chat with Gemini, and ask a series of questions in context. There’s a handy ChatSession utility class which simplifies the handling of the conversation:</p><pre>try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>    GenerateContentResponse response;<br><br>    GenerativeModel model = new GenerativeModel(modelName, vertexAI);<br>    ChatSession chatSession = new ChatSession(model);<br><br>    response = chatSession.sendMessage(&quot;Hello.&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br><br>    response = chatSession.sendMessage(&quot;What are all the colors in a rainbow?&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br><br>    response = chatSession.sendMessage(&quot;Why does it appear when it rains?&quot;);<br>    System.out.println(ResponseHandler.getText(response));<br>}</pre><p>This is convenient to use ChatSession as it takes care of keeping track of past questions from the user, and answers from the assistant.</p><h3>Going further</h3><p>This is just a few examples of the capabilities of Gemini. Be sure to check out some of the <a href="https://github.com/GoogleCloudPlatform/java-docs-samples/tree/main/vertexai/snippets/src/main/java/vertexai/gemini">samples that are available on Github</a>. Read <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal?hl=en">more about Gemini and Generative AI</a> in the Google Cloud documentation.</p><p><em>Originally published at </em><a href="https://glaforge.dev/posts/2023/12/13/get-started-with-gemini-in-java/"><em>https://glaforge.dev</em></a><em> on December 13, 2023.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=923f2069ea4d" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/get-started-with-gemini-in-java-923f2069ea4d">Get Started with Gemini in Java</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>