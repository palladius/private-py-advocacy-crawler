<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Tahir Fayyaz on Medium]]></title>
        <description><![CDATA[Stories by Tahir Fayyaz on Medium]]></description>
        <link>https://medium.com/@tfayyaz?source=rss-61dbd674077e------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/2*X3JLC3UgCaufCvH72wlTOg.png</url>
            <title>Stories by Tahir Fayyaz on Medium</title>
            <link>https://medium.com/@tfayyaz?source=rss-61dbd674077e------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 15:32:17 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@tfayyaz/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Apache Spark BigQuery Connector — Optimization tips & example Jupyter Notebooks]]></title>
            <link>https://medium.com/google-cloud/apache-spark-bigquery-connector-optimization-tips-example-jupyter-notebooks-f17fd8476309?source=rss-61dbd674077e------2</link>
            <guid isPermaLink="false">https://medium.com/p/f17fd8476309</guid>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[apache-spark]]></category>
            <category><![CDATA[jupyter-notebook]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Tahir Fayyaz]]></dc:creator>
            <pubDate>Thu, 21 May 2020 12:13:03 GMT</pubDate>
            <atom:updated>2020-05-21T15:22:06.042Z</atom:updated>
            <content:encoded><![CDATA[<h3>Apache Spark BigQuery Connector — Optimization tips &amp; example Jupyter Notebooks</h3><h4>Learn how to use the BigQuery Storage API with Apache Spark on Cloud Dataproc</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/952/1*TClXUkxA6gLadoUFcDn0jw.png" /><figcaption>BigQuery storage to BigQuery compute</figcaption></figure><p>Google <a href="https://cloud.google.com/bigquery">BigQuery</a> is Google Cloud’s fully managed data warehouse and just turned 10 years old (<a href="https://www.youtube.com/watch?v=O4_q2fQ1sJw">Happy Birthday BigQuery!!!</a>). One of its key features is that it separates compute and storage and recently this led to the development of the <a href="https://cloud.google.com/bigquery/docs/reference/storage">BigQuery Storage API</a> which allows you to read data at scale from other platforms, like Apache Spark, where the data will be processed without the need to first export the data to Google Cloud Storage as an intermediate step.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jntSkef5VwRD6fRrci7k6w.png" /><figcaption>BigQuery storage API connecting to Apache Spark, Apache Beam, Presto, TensorFlow and Pandas</figcaption></figure><p>Some examples of this integration with other platforms are Apache Spark (which will be be the focus of this post), <a href="https://prestosql.io/docs/current/connector/bigquery.html">Presto</a>, <a href="https://beam.apache.org/documentation/io/built-in/google-bigquery/#storage-api">Apache Beam</a>, <a href="https://www.tensorflow.org/io/tutorials/bigquery">Tensorflow</a>, and <a href="https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas">Pandas</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rR5yYEKCVSjfymjMdpnRmg.png" /><figcaption>Apache Spark can read multiple streams of data from the BigQuery Storage API in parallel</figcaption></figure><p>The BigQuery Storage API allows you reads data in parallel which makes it a perfect fit for a parallel processing platform like Apache Spark.</p><p>Using the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector">Apache Spark BigQuery connector</a>, which is built on top of the BigQuery Storage API and BigQuery API, you can now treat BigQuery as another source to read and write data from Apache Spark.</p><h3>Apache and Spark and JupyterLab</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KnyIHUmxrCvrk-3p5URYlA.png" /><figcaption>How Cloud Dataproc, Apache Spark, Apache Spark BigQuery Connector and Jupyter notebooks connect</figcaption></figure><p>Jupyter notebooks are a great way to get started with learning how to use the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector">Apache Spark BigQuery connector</a>.</p><p>You can read this post on using <a href="https://medium.com/google-cloud/apache-spark-and-jupyter-notebooks-made-easy-with-dataproc-component-gateway-fa91d48d6a5a">Apache Spark with Jupyter Notebooks on Cloud Dataproc</a> to get set-up and then read on for tips on how to use the connector, how to optimize your jobs and view the example Jupyter notebooks now available on GitHub.</p><h3>Set-up the Apache Spark BigQuery Storage connector</h3><p>Once you have your notebook running you just need to include the Apache Spark BigQuery Storage connector package when you create a Spark session.</p><pre>from pyspark.sql import SparkSession<br>spark = SparkSession.builder \<br>  .appName(&#39;Optimize BigQuery Storage&#39;) \<br>  .config(&#39;spark.jars.packages&#39;, &#39;com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta&#39;) \<br>  .getOrCreate()</pre><p>Based on what version of Scala you are running will need to change the artifact name to one of the following:</p><ul><li><strong>Scala 2.11: </strong>com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta</li><li><strong>Scala 2.12:</strong> com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.15.1-beta</li></ul><p>You can view the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/CHANGES.md">release notes of the connector</a> to check what is the latest version of the package.</p><h4>Reading BigQuery Data</h4><p>Followed by this you simply need to set the read format as &quot;bigquery&quot; and you are ready to load the BigQuery table into your Spark job. These examples will make use of the <a href="https://medium.com/@marcacohen/processing-10tb-of-wikipedia-page-views-part-2-48352059fd44">Wikipedia pageviews public dataset</a> created by <a href="https://medium.com/@marcacohen">Marc Cohen</a>.</p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .load()</pre><p>However as the Wikipedia pageviews table is very large at 2TB you should filter the data you actually need for your Apache Spark job.</p><p>The data will not actually be read now as .load() does not trigger a Spark job. The data will be read when an action is called which is the .show() method we use later.</p><h3>Filtering data to optimize Apache Spark jobs</h3><p>To optimize the performance of your Apache Spark jobs when using the Apache Spark BigQuery Storage connector here are some steps to show you how to only read the data required for the job.</p><h4>BigQuery partition filtering</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/774/1*Ng-p7jZlNy3doGMVPKmd4A.png" /><figcaption>The API rebalances records between readers until they all complete</figcaption></figure><p><a href="https://cloud.google.com/bigquery/docs/partitioned-tables">BigQuery tables can be partitioned</a> by date or integers in a similar fashion to Hive, Parquet and ORC. The wikipedia public dataset uses date partitions and so you can set the filter option to only read 7 days of data instead of all 365 days.</p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><h4>BigQuery columnar storage filtering</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/801/1*5TQFdyHEyuX2zhun01-VSg.png" /><figcaption>Only the columns required are read by Apache Spark</figcaption></figure><p>BigQuery uses columnar storage similar to Apache Parquet and ORC. Therefore you can read just the columns you need for your Spark job by selecting certain columns with the filter option.</p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><pre>df_wiki_en = df_wiki_pageviews \<br>  .select(&quot;title&quot;, &quot;wiki&quot;, &quot;views&quot;)</pre><pre>df_wiki_en.printSchema()</pre><h4>BigQuery rows filtering</h4><p>The BigQuery Storage API supports predicate push-down of filters which means that if you set a filter in the where statement later on in your Apache Spark job it will attempt to push the filter to BigQuery. Therefore the two jobs below would yield the same result</p><p><strong>Using filter and where</strong></p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><pre>df_wiki_en = df_wiki_pageviews \<br>  .select(&quot;title&quot;, &quot;wiki&quot;, &quot;views&quot;) \<br>  .where(&quot;views &gt; 10 AND wiki in (&#39;en&#39;, &#39;en.m&#39;)&quot;)</pre><pre>df_wiki_en.show()</pre><p><strong>Using only filter</strong></p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39; AND views &gt; 10 AND wiki in (&#39;en&#39;, &#39;en.m&#39;)&quot;) \<br>  .load()</pre><pre>df_wiki_en = df_wiki_pageviews \<br>  .select(&quot;title&quot;, &quot;wiki&quot;, &quot;views&quot;)</pre><pre>df_wiki_en.show()</pre><h4>View job performance in the Spark UI</h4><p>When you run your job using data from BigQuery you will want to look at the Spark job performance. Take this aggregation job for example:</p><pre>import pyspark.sql.functions as F</pre><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><pre>df_wiki_en = df_wiki_pageviews \<br>  .select(&quot;title&quot;, &quot;wiki&quot;, &quot;views&quot;) \<br>  .where(&quot;views &gt; 10 AND wiki in (&#39;en&#39;, &#39;en.m&#39;)&quot;)</pre><pre>df_wiki_en_totals = df_wiki_en \<br>.groupBy(&quot;title&quot;) \<br>.agg(F.sum(&#39;views&#39;).alias(&#39;total_views&#39;))</pre><pre>df_wiki_en_totals.orderBy(&#39;total_views&#39;, ascending=False).show()</pre><p>Using the <a href="https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways">Component Gateway</a> feature you can easily access the Spark UI to see how the job performed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5H-X2L2SRpeEowqhfOYtQA.png" /></figure><p>This stage of the job here shows that there were 188 tasks created to read data in parallel from BigQuery Storage.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2uM3GNQIhDt7h6kK_iHW0A.png" /></figure><h4>Setting Max Parallelism</h4><p>As we can see, some of these tasks are empty as shown in the summary metrics for the 188 completed tasks.<strong> </strong>We can reduce the number of tasks reading from the BigQuery Storage by setting the maxParallelism <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector#properties">property in the read API</a> to match the cluster size and settings<strong>. </strong>For most use cases the default setting of attempting to read one partition per 400MB should be adequate.</p><p>If you have a cluster with a total of 8 executor cores you can consider setting the property as maxParallelism property as:</p><ul><li>8 total executor cores * 7 partition days = 56</li></ul><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;maxParallelism&quot;, 56) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><p>You should test this with your data size, BigQuery format (partitioned vs non-partitioned), filter options and cluster configuration.</p><p>Currently the maximum parallelism allowed by the BigQuery storage API is 1000. If you set the maxParallelism property value to greater than 1000 you will still only have 1000 tasks reading from 1000 multiple streams within a session.</p><h4>Cache data in memory</h4><p>There might be scenarios where you want the data in memory instead of reading from BigQuery Storage every time to improve performance .</p><p>The job above to find the total page views per title will read the data from BigQuery and push the filter to BigQuery. The aggregation will then be computed in Apache Spark.</p><p>You can modify the job to include a cache of the filtered data from the table and further filter the data on the wiki column which will be applied in memory by Apache Spark.</p><pre>import pyspark.sql.functions as F</pre><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2019&quot;</pre><pre>df_wiki_pageviews = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .option(&quot;filter&quot;, &quot;datehour &gt;= &#39;2019-01-01&#39; AND datehour &lt; &#39;2019-01-08&#39;&quot;) \<br>  .load()</pre><pre>df_wiki_all = df_wiki_pageviews \<br>  .select(&quot;title&quot;, &quot;wiki&quot;, &quot;views&quot;) \<br>  .where(&quot;views &gt; 10&quot;)</pre><pre>df_wiki_all.cache()</pre><pre>df_wiki_en = df_wiki_all \<br>  .where(&quot;wiki in (&#39;en&#39;, &#39;en.m&#39;)&quot;)</pre><pre>df_wiki_en_totals = df_wiki_en \<br>.groupBy(&quot;title&quot;) \<br>.agg(F.sum(&#39;views&#39;).alias(&#39;total_views&#39;))</pre><pre>df_wiki_en_totals.orderBy(&#39;total_views&#39;, ascending=False).show()</pre><p>If you view the job details you will see that the table was cached as part of the job.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Q5QRdn54-OC1GPdphLZI-A.png" /></figure><p>You can then filter for another wiki language using the cache instead of reading data from BigQuery storage again.</p><pre>df_wiki_de = df_wiki_all \<br>  .where(&quot;wiki in (&#39;de&#39;, &#39;de.m&#39;)&quot;)</pre><pre>df_wiki_de_totals = df_wiki_de \<br>.groupBy(&quot;title&quot;) \<br>.agg(F.sum(&#39;views&#39;).alias(&#39;total_views&#39;))</pre><pre>df_wiki_de_totals.orderBy(&#39;total_views&#39;, ascending=False).show()</pre><p>The difference in this next job is that the table is now being read from memory and therefore runs much faster.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wzmaejs-uRCQ0Rhg_sbubA.png" /></figure><p>You can remove the cache by running</p><pre>df_wiki_all.unpersist()</pre><h3>Examples Jupyter Notebooks</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FlRqq9yDOt5kOaPbURFGAg.png" /><figcaption>BigQuery Storage &amp; Spark DataFrames — Python Jupyter notebook</figcaption></figure><p>Example <a href="https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks">Jupyter notebooks now available on the official Google Cloud Dataproc Github repo</a> on how the Apache Spark BigQuery Storage connector works with Spark DataFrames, Spark SQL and Spark MLlib to read and write data.</p><blockquote><strong>Note: These notebooks are designed to work with the Python 3 kernel (not PySpark kernel) as this allows you create your Spark session and include the Apache Spark BigQuery connector</strong></blockquote><p>These notebooks make use of spark.sql.repl.eagerEval to output the results of DataFrames in each step without the need to use df.show() and also improves the formatting of the output.</p><ul><li><a href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.1.%20BigQuery%20Storage%20%26%20Spark%20DataFrames%20-%20Python.ipynb">BigQuery Storage &amp; Spark DataFrames Notebook</a></li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bb3802b11526fa248c46acd33c283c18/href">https://medium.com/media/bb3802b11526fa248c46acd33c283c18/href</a></iframe><ul><li><a href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.2.%20BigQuery%20Storage%20%26%20Spark%20SQL%20-%20Python.ipynb">BigQuery Storage &amp; Spark SQL</a></li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e6c2c74e580f3da56786ebad789d49c9/href">https://medium.com/media/e6c2c74e580f3da56786ebad789d49c9/href</a></iframe><ul><li><a href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.3.%20BigQuery%20Storage%20%26%20Spark%20MLlib%20-%20Python.ipynb">BigQuery Storage &amp; Spark MLlib</a></li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/11f551467735cf71c0fe8fc894c732e2/href">https://medium.com/media/11f551467735cf71c0fe8fc894c732e2/href</a></iframe><h3>What’s next</h3><ul><li>Get in touch here on <a href="https://medium.com/@tfayyaz">Medium (@tfayyaz)</a> or on <a href="http://twitter.com/tfayyaz">Twitter (tfayyaz)</a> if you have ideas for other Apache Spark notebook examples you would like to see.</li><li>Ask any questions in the comments or on Stackoverflow under the <a href="https://stackoverflow.com/questions/tagged/google-cloud-dataproc">google-cloud-dataproc</a> tag.</li><li>Leave any feedback or issues for the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector">Apache Spark BigQuery connector</a> on GitHub</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f17fd8476309" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/apache-spark-bigquery-connector-optimization-tips-example-jupyter-notebooks-f17fd8476309">Apache Spark BigQuery Connector — Optimization tips &amp; example Jupyter Notebooks</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Apache Spark and Jupyter Notebooks made easy with Dataproc component gateway]]></title>
            <link>https://medium.com/google-cloud/apache-spark-and-jupyter-notebooks-made-easy-with-dataproc-component-gateway-fa91d48d6a5a?source=rss-61dbd674077e------2</link>
            <guid isPermaLink="false">https://medium.com/p/fa91d48d6a5a</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[apache-spark]]></category>
            <category><![CDATA[jupyter-notebook]]></category>
            <category><![CDATA[bigquery]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Tahir Fayyaz]]></dc:creator>
            <pubDate>Thu, 12 Mar 2020 12:48:09 GMT</pubDate>
            <atom:updated>2020-04-16T14:56:30.973Z</atom:updated>
            <content:encoded><![CDATA[<h4>Use the new Dataproc optional components and component gateway features to easily set-up and use Jupyter Notebooks</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ITz23f880aFggnAT9sqoLw.png" /><figcaption>Apache Spark and Jupyter Notebooks architecture on Google Cloud</figcaption></figure><p>As a long time user and fan of Jupyter Notebooks I am always looking for the best ways to set-up and use notebooks especially in the cloud. I believe Jupyter Notebooks are the perfect tool for learning, prototyping, and in some cases production of your data projects as they allow you to interactively run your code and immediately see your results. They are a great tool for collaboration thanks to a background coming from being used and shared in scientific communities.</p><p>You might have used Jupyter notebooks on your desktop in the past with Python but struggled with handling very large datasets. However with many kernels now available you can make use of Apache Spark for distributed processing of large-scale data in Jupyter but also continue to use your Python libraries in the same notebook.</p><p>However getting an Apache Spark cluster set-up with Jupyter Notebooks can be complicated and so in Part 1 of this new “<strong>Apache Spark and Jupyter Notebooks on Cloud Dataproc” </strong>series of posts I will show you how easy it is to get started thanks to new features like <a href="https://cloud.google.com/dataproc/docs/concepts/components/overview">optional components</a> and <a href="https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways">component gateways</a>.</p><h3>Create a Dataproc cluster with Spark and Jupyter</h3><p>You can create a Cloud Dataproc cluster using the Google Cloud Console, gcloud CLI or <a href="https://cloud.google.com/dataproc/docs/reference/libraries">Dataproc client libraries</a>.</p><p>We will be using the gcloud CLI from the <a href="https://console.cloud.google.com/?cloudshell=true">Cloud Shell</a> where gcloud is already installed (If you’re new to Google Cloud view the <a href="https://codelabs.developers.google.com/codelabs/cloud-shell/index.html?index=..%2F..index#2">Getting Started with Cloud Shell &amp; gcloud codelab</a>).</p><p>You can also use the gcloud CLI locally by installing the <a href="https://cloud.google.com/sdk/gcloud">Cloud SDK</a>.</p><p>To get started once in the cloud shell or your terminal window set your project ID where you will create your Dataproc cluster</p><pre>gcloud config set project &lt;project-id&gt;</pre><h4><strong>Enable product APIs and IAM roles</strong></h4><p>Run this command to enable all the APIs required in the <strong>Apache Spark and Jupyter Notebooks on Cloud Dataproc </strong>series of posts.</p><pre>gcloud services enable dataproc.googleapis.com \<br>  compute.googleapis.com \<br>  storage-component.googleapis.com \<br>  bigquery.googleapis.com \<br>  bigquerystorage.googleapis.com</pre><p>If you are not the admin or do not have the correct permissions to enable APIs ask the admin for your GCP organization or project to enable the APIs above.</p><p>They will also need give you the correct <a href="https://cloud.google.com/dataproc/docs/concepts/iam/iam#roles"><strong>Dataproc IAM roles</strong></a> and Google <a href="https://cloud.google.com/storage/docs/access-control/iam-roles">Cloud Storage IAM roles</a> to create and use your Dataproc cluster.</p><h4><strong>Create a GCS bucket to be used by your Dataproc Cluster</strong></h4><p>Create a Google Cloud Storage bucket in the region closest to your data and give it a unique name. This will be used for the Dataproc cluster.</p><pre>REGION=us-central1<br>BUCKET_NAME=&lt;your-bucket-name&gt;</pre><pre>gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}</pre><p>You should see the following output</p><blockquote>Creating gs://&lt;your-bucket-name&gt;/...</blockquote><p><strong>Create your Dataproc Cluster with Jupyter &amp; Component Gateway</strong></p><p>Set the env variables for your cluster</p><pre>REGION=us-central1 <br>ZONE=us-central1-a <br>CLUSTER_NAME=spark-jupyter-&lt;your-name&gt; <br>BUCKET_NAME=&lt;your-bucket-name&gt;</pre><p>Then run this gcloud command to create your cluster with all the necessary components to work with Jupyter on your cluster.</p><pre>gcloud beta dataproc clusters create ${CLUSTER_NAME} \<br>  --region=${REGION} \<br>  --zone=${ZONE} \<br>  --image-version=1.5 \<br>  --master-machine-type=n1-standard-4 \<br>  --worker-machine-type=n1-standard-4 \<br>  --bucket=${BUCKET_NAME} \<br>  --optional-components=ANACONDA,JUPYTER \<br>  --enable-component-gateway \<br>  --metadata &#39;PIP_PACKAGES=google-cloud-bigquery google-cloud-storage&#39; \<br>  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh</pre><p>You should see the following output while your cluster is being created</p><blockquote>Waiting on operation [projects/spark-jupyter-notebooks/regions/us-central1/operations/random-letters-numbers-abcd123456].<br>Waiting for cluster creation operation…</blockquote><p>It should take 2 to 3 minutes to create your cluster and once it is ready you will be able to access your cluster from the <a href="https://console.cloud.google.com/dataproc/clusters">Dataproc Cloud console UI</a>.</p><p>You should the following output once the cluster is created:</p><blockquote>Created [https://dataproc.googleapis.com/v1beta2/projects/project-id/regions/us-central1/clusters/spark-jupyter-your-name] Cluster placed in zone [us-central1-a].</blockquote><h4>Flags used in gcloud dataproc create command</h4><p>Here is a breakdown of the flags used in the gcloud dataproc create command</p><pre>--region=${REGION} <br>--zone=${ZONE} </pre><p>Specifies the region and zone of where the cluster will be created. You can see the list of <a href="https://cloud.google.com/compute/docs/regions-zones#available">available regions here.</a> Zone is optional unless if you are using n2 machine types when you must specify a zone.</p><pre>--image-version=1.4</pre><p>The image version to use in your cluster. You can see the list of <a href="https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions">available versions here</a>.</p><pre>--bucket=${BUCKET_NAME}</pre><p>Specify the Google Cloud Storage bucket you created earlier to use for the cluster. If you do not supply a GCS bucket it will be created for you.</p><p>This is also where your notebooks will be saved even if you delete your cluster as the GCS bucket is not deleted.</p><pre>--master-machine-type=n1-standard-4<br>--worker-machine-type=n1-standard-4</pre><p>The machine types to use for your Dataproc cluster. You can see a list of available <a href="https://cloud.google.com/compute/docs/machine-types">machine types here</a>.</p><p><strong><em>Note: Look out for future post in the series on recommendations for what machine types to use and how to enable auto-scaling</em></strong></p><pre>--optional-components=ANACONDA,JUPYTER</pre><p>Setting these values for <a href="https://cloud.google.com/dataproc/docs/concepts/components/overview">optional components</a> will install all the necessary libraries for Jupyter and Anaconda (which is required for Jupyter notebooks) on your cluster.</p><pre>--enable-component-gateway</pre><p>Enabling <a href="https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways">component gateway</a> creates an App Engine link using Apache Knox and Inverting Proxy which gives easy, secure and authenticated access to the Jupyter and JupyterLab web interfaces meaning you no longer need to create SSH tunnels.</p><p>It will also create links for other tools on the cluster including the Yarn Resource manager and Spark History Server which are useful for seeing the performance of your jobs and cluster usage patterns.</p><pre>--metadata &#39;PIP_PACKAGES=google-cloud-bigquery google-cloud-storage&#39; <br>--initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh</pre><p>Installs the latest versions of the <a href="https://googleapis.dev/python/bigquery/latest/index.html">Google Cloud BigQuery python library</a> and the <a href="https://googleapis.dev/python/storage/latest/client.html">Google Cloud Storage python library</a>. These will be used to perform various tasks when working with BigQuery and GCS in your notebooks.</p><h3>Accessing Jupyter or JupyterLab web interfaces</h3><p>Once the cluster is ready you can find the Component Gateway links to the Jupyter and JupyterLab web interfaces in the Google Cloud console for Dataproc by clicking on the cluster you created and going to the <strong>Web Interfaces</strong> tab.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6eNJc6DjMbp6JPEvuHb5Lg.png" /></figure><p>Alternatively you can get the links by running this gcloud command.</p><pre>REGION=us-central1<br>CLUSTER_NAME=spark-jupyter-&lt;your-name&gt;</pre><pre>gcloud beta dataproc clusters describe ${CLUSTER_NAME} \<br>  --region=${REGION}</pre><p>Which will show an output with the links in the following format.</p><pre>clusterName: spark-jupyter-&lt;your-name&gt;<br>clusterUuid: XXXX-1111-2222-3333-XXXXXX<br>config:<br>  configBucket: bucket-name<br>  endpointConfig:<br>    enableHttpPortAccess: true<br>    httpPorts:<br>      Jupyter: <a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/jupyter/">https://</a><a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/hdfs/dfshealth.html">random-characters</a><a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/jupyter/">-dot-us-east1.dataproc.googleusercontent.com/jupyter/</a><br>      JupyterLab: <a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/jupyter/lab/">https://</a><a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/hdfs/dfshealth.html">random-characters</a><a href="https://gsbrebdjijhgjlmsdqkc2byoua-dot-us-east1.dataproc.googleusercontent.com/jupyter/lab/">-dot-us-east1.dataproc.googleusercontent.com/jupyter/lab/</a><br>...</pre><p>You will notice that you have access to Jupyter which is the classic notebook interface or <a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html">JupyterLab</a> which is is described as the next-generation UI for Project Jupyter.</p><p>There are a lot of great new UI features in JupyterLab and so if you are new to using notebooks or looking for the latest improvements it is recommended to go with using JupyterLab as it will eventually replace the classic Jupyter interface according to the official docs.</p><h3>Python 3, PySpark, R, and Scala kernels</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VewHWLFn1H4HsuKzX-iCEQ.png" /></figure><p>Based on the image version you selected when creating your Dataproc cluster you will have different kernels available:</p><ul><li><strong>Image version 1.3:</strong> Python 2 and PySpark</li><li><strong>Image version 1.4:</strong> Python 3, PySpark (Python), R, and Spylon (Scala)</li><li><strong>Image version Preview (1.5):</strong> Python 3, PySpark (Python), R, and Spylon (Scala)</li></ul><p>You should use image version 1.4 or above so that you can make use of the Python 3 kernel to run PySpark code or the Spylon kernel to run Scala code.</p><h3>Creating your first PySpark Jupyter Notebook</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bDl1Zag6z39HJUmWvhDUfQ.png" /></figure><p>From the launcher tab click on the Python 3 notebook icon to create a notebook with a Python 3 kernel (not the PySpark kernel) which allows you to configure the SparkSession in the notebook and include the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector">spark-bigquery-connector</a> required to use the <a href="https://cloud.google.com/bigquery/docs/reference/storage">BigQuery Storage API</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v2gjZBDWZ7Hvx0ENZCx7VA.png" /></figure><p>Once your notebook opens in the first cell check the Scala version of your cluster so you can include the correct version of the spark-bigquery-connector jar.</p><p><strong>Input [1]:</strong></p><pre>!scala -version</pre><p><strong>Output [1]:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dOpjH7XG--f_a2BS1tYGgA.png" /></figure><p>Create a Spark session and include the spark-bigquery-connector jar</p><p><strong>Input [2]:</strong></p><pre><strong>from</strong> <strong>pyspark.sql</strong> <strong>import</strong> SparkSession<br>spark = SparkSession.builder \<br>  .appName(&#39;Jupyter BigQuery Storage&#39;)\<br>  .config(&#39;spark.jars&#39;, &#39;gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar&#39;) \<br>  .getOrCreate()</pre><p>Create a Spark DataFrame by reading in data from a public BigQuery dataset. This makes use of the <a href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector">spark-bigquery-connector</a> and BigQuery Storage API to load the data into the Spark cluster.</p><p>If your Scala version is 2.11 use the following jar</p><pre>gs://spark-lib/bigquery/spark-bigquery-latest.jar</pre><p>If your Scala version is 2.12 use the following jar</p><pre>gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar</pre><p>We will create a Spark DataFrame and load data from the <a href="https://medium.com/@marcacohen/processing-10tb-of-wikipedia-page-views-part-1-b984d8ebe146">BigQuery public dataset for Wikipedia pageviews</a>. You will notice we are not running a query on the data as we are using the bigquery-storage-connector to load the data into Spark where the processing of the data will happen.</p><p><strong>Input [3]:</strong></p><pre>table = &quot;bigquery-public-data.wikipedia.pageviews_2020&quot;</pre><pre>df = spark.read \<br>  .format(&quot;bigquery&quot;) \<br>  .option(&quot;table&quot;, table) \<br>  .load()</pre><pre>df.printSchema()</pre><p><strong>Output [3]:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Yjv0b9CR1V2Beu-TK9d0ZA.png" /></figure><p>Create a new aggregated Spark DataFrame and print the schema</p><p><strong>Input [4]:</strong></p><pre>df_agg = df \<br>  .select(&#39;wiki&#39;, &#39;views&#39;) \<br>  .where(&quot;datehour = &#39;2020-03-03&#39;&quot;) \<br>  .groupBy(&#39;wiki&#39;) \<br>  .sum(&#39;views&#39;)</pre><pre>df_agg.printSchema()</pre><p><strong>Output [4]:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-39EJWmJY1Xh_jFG8EJKyQ.png" /></figure><p>Run the aggregation using the .show() function on the DataFrame which will start the Spark job to process the data and then show the output of the Spark DataFrame limited to the first 20 rows.</p><p><strong>Input [5]:</strong></p><pre>df_agg.show()</pre><p><strong>Output [5]:</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OL1_dLe3K64fjsNrQ6TzAA.png" /></figure><p>You should now have your first Jupyter notebook up and running on your Dataproc cluster. Give your notebook a name and it will be auto-saved to the GCS bucket used when creating the cluster. You can check this using this gsutil command.</p><pre>BUCKET_NAME=&lt;your-bucket-name&gt;</pre><pre>gsutil ls gs://${BUCKET_NAME}/notebooks/jupyter</pre><h3>Example notebooks for more use cases</h3><p>The next posts in this series will feature Jupyter notebooks with common Apache Spark patterns for loading data, saving data, and plotting your data with various Google Cloud Platform products and open-source tools:</p><ul><li>Spark and BigQuery Storage API</li><li>Spark and Google Cloud Storage</li><li>Spark and Apache Iceberg / DeltaLake</li><li>Plotting Spark DataFrames using Pandas</li></ul><p>You can also access the upcoming <a href="http://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks">examples notebooks on the Cloud Dataproc GitHub repo</a></p><h3>Giving the cluster’s service account access to data</h3><p>In the example above we are accessing a public dataset but for your use case you will most likely be accessing your companies data with restricted access. The Jupyter notebook and Dataproc cluster will attempt to access data in Google Cloud Platform services using the service account of the underlying Google Computer Engine (GCE) VMs and not your own Google credentials.</p><p>You can find the service account of your cluster by running this command to describe the master VM in GCE which will have the same name of your Dataproc cluster followed by <strong>-m</strong></p><pre>ZONE=us-central1-a <br>CLUSTER_NAME=spark-jupyter-&lt;your-name&gt;</pre><pre>gcloud compute instances describe ${CLUSTER_NAME}-m \<br>  --zone=${ZONE}</pre><p>This will give a long list of attributes including the service account and scopes as shown in this example output.</p><pre>serviceAccounts:- <br>  email: &lt;random-number&gt;-compute@developer.gserviceaccount.com  <br>  scopes:  - <a href="https://www.googleapis.com/auth/bigquery">https://www.googleapis.com/auth/bigquery</a>  - <a href="https://www.googleapis.com/auth/bigtable.admin.table">https://www.googleapis.com/auth/bigtable.admin.table</a>  - <a href="https://www.googleapis.com/auth/bigtable.data">https://www.googleapis.com/auth/bigtable.data</a>  - <a href="https://www.googleapis.com/auth/cloud.useraccounts.readonly">https://www.googleapis.com/auth/cloud.useraccounts.readonly</a>  - <a href="https://www.googleapis.com/auth/devstorage.full_control">https://www.googleapis.com/auth/devstorage.full_control</a>  - <a href="https://www.googleapis.com/auth/devstorage.read_write">https://www.googleapis.com/auth/devstorage.read_write</a>  - <a href="https://www.googleapis.com/auth/logging.write">https://www.googleapis.com/auth/logging.write</a></pre><p>Alternatively you can view the service account in the Google Cloud Console by going to the <strong>VM Instances</strong> tab in your Dataproc cluster and clicking on the master VM instance.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bB_ttptnMvqJQ_7bE2Jc1A.png" /></figure><p>Once in the VM page scroll to the bottom and you will see the service account for the VM. This is the same service account for all VM instances in your cluster.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vfz-qlUQEe1hFHHz6nTBdQ.png" /></figure><p>You should then give the service account the correct <a href="https://cloud.google.com/bigquery/docs/access-control#bigquery"><strong>BigQuery IAM roles</strong></a><strong> and </strong><a href="https://cloud.google.com/storage/docs/access-control/iam-roles"><strong>GCS IAM roles</strong></a> to access the BigQuery datasets or GCS buckets you need.</p><p>For more details on providing the correct access read this solution to <a href="https://cloud.google.com/solutions/help-secure-the-pipeline-from-your-data-lake-to-your-data-warehouse">Help secure the pipeline from your data lake to your data warehouse.</a></p><h3>Deleting your Dataproc cluster</h3><p>Once you have have finished all of your work within the Jupyter Notebook and all Spark jobs have finished processing it is recommended to delete the Dataproc cluster which can be done via the Cloud Console or using this gcloud command:</p><pre>REGION=us-central1 <br>CLUSTER_NAME=spark-jupyter-&lt;your-name&gt; </pre><pre>gcloud beta dataproc clusters delete ${CLUSTER_NAME} \<br>  --region=${REGION}</pre><p>As mentioned before you can always delete and recreate your cluster and all your notebooks will still be saved in the Google Cloud Storage bucket which is not deleted when you delete your Dataproc cluster.</p><h3>What’s Next</h3><ul><li>Look out for next post series which will cover using the bigquery-storage-connector in a Jupyter Notebook in more depth.</li><li>Follow me here on <a href="https://medium.com/@tfayyaz">Medium (@tfayyaz)</a> and on <a href="http://twitter.com/tfayyaz">Twitter (tfayyaz)</a> to hear more about the latest updates about Dataproc and share feedback.</li><li>Ask any questions in the comments or on Stackoverflow under the <a href="https://stackoverflow.com/questions/tagged/google-cloud-dataproc">google-cloud-dataproc</a> tag.</li><li>Have fun working with Spark and Jupyter Notebooks on Dataproc.</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fa91d48d6a5a" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/apache-spark-and-jupyter-notebooks-made-easy-with-dataproc-component-gateway-fa91d48d6a5a">Apache Spark and Jupyter Notebooks made easy with Dataproc component gateway</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>