<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Laurent Picard on Medium]]></title>
        <description><![CDATA[Stories by Laurent Picard on Medium]]></description>
        <link>https://medium.com/@PicardParis?source=rss-6be63961431c------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*OMNmXavx5hd7qUUZxNYLhA.jpeg</url>
            <title>Stories by Laurent Picard on Medium</title>
            <link>https://medium.com/@PicardParis?source=rss-6be63961431c------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 03 Jul 2024 16:03:39 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@PicardParis/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI]]></title>
            <link>https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/4b0fc2a14851</guid>
            <category><![CDATA[gemma]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[llm]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Wed, 21 Feb 2024 18:04:07 GMT</pubDate>
            <atom:updated>2024-02-26T14:27:56.268Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*UboIDVQkrG2Qy0mnW21vPw.png" /></figure><h3>Gemma just opened ;)</h3><p><a href="https://blog.google/technology/developers/gemma-open-models"><strong>Gemma</strong></a> is a family of open, lightweight, and easy-to-use models developed by Google Deepmind. The Gemma models are built from the same research and technology used to create Gemini.</p><p>This means that we (ML developers &amp; practitioners) now have additional versatile large language models (LLMs) in our toolbox!</p><p>If you’d rather read code, you can jump straight to this Python notebook:<br>→ <a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb">Finetune Gemma using KerasNLP and deploy to Vertex AI</a></p><h3>Availability</h3><p>Gemma is available today in Google Cloud and the machine learning (ML) ecosystem. You’ll probably find an ML platform you’re already familiar with:</p><ul><li>Gemma joins 130+ models in <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335">Vertex AI Model Garden</a></li><li>Gemma joins the <a href="https://www.kaggle.com/models/google/gemma">Kaggle Models</a></li><li>Gemma joins the <a href="http://huggingface.co/google">Hugging Face Models</a></li><li>Gemma joins <a href="https://ai.google.dev/gemma">Google AI for Developers</a></li></ul><p>Here are the <a href="https://www.kaggle.com/models/google/gemma/license/consent">Gemma Terms of Use</a>.</p><h3>Gemma models</h3><p>The Gemma family launches in two sizes, Gemma <strong>2B</strong> and <strong>7B</strong>, targeting two typical platform types:</p><pre>| Model    | Parameters  | Platforms                           |<br>| -------- | ----------- | ----------------------------------- |<br>| Gemma 2B | 2.5 billion | Mobile devices and laptops          |<br>| Gemma 7B | 8.5 billion | Desktop computers and small servers |</pre><p>These models are text-to-text English LLMs:</p><ul><li><strong>Input</strong>: a text string, such as a question, a prompt, or a document.</li><li><strong>Output</strong>: generated English text, such as an answer or a summary.</li></ul><p>They were trained on a diverse and massive dataset of <strong>8 trillion tokens</strong>, including web documents, source code, and mathematical text.</p><p>Each size is available in untuned and tuned versions:</p><ul><li><strong>Pretrained (untuned)</strong>: Models were trained on core training data, not using any specific tasks or instructions.</li><li><strong>Instruction-tuned</strong>: Models were additionally trained on human language interactions.</li></ul><p>That gives us four variants. As an example, here are the corresponding model IDs when using Keras:</p><pre>- `gemma_2b_en`<br>- `gemma_instruct_2b_en`<br>- `gemma_7b_en`<br>- `gemma_instruct_7b_en`</pre><h3>Use cases</h3><p>Google now offers two families of LLMs: Gemini and Gemma. Gemma is a complement to Gemini, is based on technologies developed for Gemini, and addresses different use cases.</p><p>Examples of Gemini benefits:</p><ul><li><strong>Enterprise</strong> applications</li><li><strong>Multilingual</strong> tasks</li><li>Optimal <strong>qualitative results</strong> and <strong>complex tasks</strong></li><li>State-of-the-art <strong>multimodality</strong> (text, image, video)</li><li><strong>Groundbreaking features</strong> (e.g. <a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-on-vertex-ai-expands">Gemini 1.5 Pro 1M token context window</a>)</li></ul><p>Examples of Gemma benefits:</p><ul><li><strong>Learning</strong>, <strong>research</strong>, or <strong>prototyping</strong> based on lightweight models</li><li>Focused tasks such as <strong>text generation</strong>, <strong>summarization</strong>, and <strong>Q&amp;A</strong></li><li>Framework or cross-device <strong>interoperability</strong></li><li><strong>Offline</strong> or <strong>real-time</strong> text-only processings</li></ul><p>The Gemma model variants are useful in the following use cases:</p><ul><li><strong>Pretrained (untuned)</strong>: Consider these models as a lightweight foundation and perform a custom finetuning to optimize them to your own needs.</li><li><strong>Instruction-tuned</strong>: You can use these models for conversational applications, such as a chatbot, or to customize them even further.</li></ul><h3>Interoperability</h3><p>As Gemma models are open, they are virtually interoperable with all ML platforms and frameworks.</p><p>For the launch, Gemma is supported by the following ML ecosystem players:</p><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/gemma-model-available-in-vertex-ai-and-via-gke">Google Cloud</a></li><li><a href="https://www.kaggle.com/models/google/gemma">Kaggle</a></li><li><a href="https://developers.googleblog.com/2024/02/gemma-models-in-keras.html">Keras</a>, which means that Gemma runs on <strong>JAX</strong>, <strong>PyTorch</strong>, and <strong>TensorFlow</strong></li><li><a href="https://huggingface.co/blog/gemma">Hugging Face</a></li><li><a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-revs-up-inference-for-google-gemma/">Nvidia</a></li></ul><p>And, of course, Gemma can <strong>run on GPUs and TPUs</strong>.</p><h3>Requirements</h3><p>Gemma models are lightweight but, in the LLM world, this still means gigabytes.</p><p>In my tests, when running inferences in half precision (bfloat16), here are the minimum storage and GPU memory that were required:</p><pre>| Model    | Total parameters | Assets size | Min. GPU RAM to run |<br>| -------- | ---------------: | ----------: | ------------------: |<br>| Gemma 2B |    2,506,172,416 |     4.67 GB |              8.3 GB |<br>| Gemma 7B |    8,537,680,896 |    15.90 GB |             20.7 GB |</pre><p>To experiment with Gemma and Vertex AI, I used <strong>Colab Enterprise</strong> with a g2-standard-8 runtime, which comes with an <strong>NVIDIA L4</strong> GPU and 24 GB of GPU RAM. This is a cost-effective configuration to save time and avoid running out-of-memory when prototyping in a notebook.</p><h3>Finetuning Gemma</h3><p>Depending on your preferred frameworks, you’ll find different ways to customize Gemma. Keras is one of them and provides everything you need.</p><p>KerasNLP lets you load a Gemma model in a single line:</p><pre>import keras<br>import keras_nlp<br><br>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_instruct_2b_en&quot;)</pre><p>Then, you can directly generate text:</p><pre>inputs = [<br>    &quot;What&#39;s the most famous painting by Monet?&quot;,<br>    &quot;Who engineered the Statue of Liberty?&quot;,<br>    &#39;Who were &quot;The Lumières&quot;?&#39;,<br>]<br><br>for input in inputs:<br>    response = gemma_lm.generate(input, max_length=25)<br>    output = response[len(input) :]<br>    print(f&quot;{input!r}\n{output!r}\n&quot;)</pre><p>With the instruction-tuned version, Gemma gives you expected answers, as would a good LLM-based chatbot:</p><pre># With &quot;gemma_instruct_2b_en&quot;<br><br>&quot;What&#39;s the most famous painting by Monet?&quot;<br>&#39;\n\nThe most famous painting by Monet is &quot;Water Lilies,&quot; which&#39;<br><br>&#39;Who engineered the Statue of Liberty?&#39;<br>&#39;\n\nThe Statue of Liberty was built by French sculptor Frédéric Auguste Bartholdi between 1&#39;<br><br>&#39;Who were &quot;The Lumières&quot;?&#39;<br>&#39;\n\nThe Lumières were a group of French scientists and engineers who played a significant role&#39;</pre><p>Now, try the untuned version:</p><pre>gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_2b_en&quot;)</pre><p>You’ll get different results, which is expected as this version was not trained on any specific task and is designed to be finetuned to your own needs:</p><pre># With &quot;gemma_2b_en&quot;<br><br>&quot;What&#39;s the most famous painting by Monet?&quot;<br>&quot;\n\nWhat&#39;s the most famous painting by Van Gogh?\n\nWhat&quot;<br><br>&#39;Who engineered the Statue of Liberty?&#39;<br>&#39;\n\nA. George Washington\nB. Napoleon Bonaparte\nC. Robert Fulton\nD&#39;<br><br>&#39;Who were &quot;The Lumières&quot;?&#39;<br>&#39; What did they invent?\n\nIn the following sentence, underline the correct modifier from the&#39;</pre><p>If you’d like, for example, to build a Q&amp;A application, prompt engineering may fix some of these issues but, to be grounded on facts and return consistent results, untuned models need to be finetuned with training data.</p><p>The Gemma models have billions of trainable parameters. The next step consists of using the <a href="https://arxiv.org/abs/2106.09685">Low Rank Adaptation</a> (LoRA) to greatly reduce the number of trainable parameters:</p><pre># Number of trainable parameters before enabling LoRA: 2.5B<br><br># Enable LoRA for the model and set the LoRA rank to 4<br>gemma_lm.backbone.enable_lora(rank=4)<br><br># Number of trainable parameters after enabling LoRA: 1.4M (1,800x less)</pre><p>A training can now be performed with reasonable time and GPU memory requirements.</p><p>For prototyping on a small training dataset, you can launch a local finetuning:</p><pre>training_data: list[str] = [...]<br><br># Reduce the input sequence length to limit memory usage<br>gemma_lm.preprocessor.sequence_length = 128<br><br># Use AdamW (a common optimizer for transformer models)<br>optimizer = keras.optimizers.AdamW(<br>    learning_rate=5e-5,<br>    weight_decay=0.01,<br>)<br><br># Exclude layernorm and bias terms from decay<br>optimizer.exclude_from_weight_decay(var_names=[&quot;bias&quot;, &quot;scale&quot;])<br><br>gemma_lm.compile(<br>    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br>    optimizer=optimizer,<br>    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],<br>)<br>gemma_lm.fit(training_data, epochs=1, batch_size=1)</pre><p>After a couple of minutes of training (on 1,000 examples) and using a structured prompt, the finetuned model can now answer questions based on your training data:</p><pre># With &quot;gemma_2b_en&quot; before finetuning<br>&#39;Who were &quot;The Lumières&quot;?&#39;<br>&#39; What did they invent?\n\nIn the following sentence, underline the correct modifier from the&#39;<br><br># With &quot;gemma_2b_en&quot; after finetuning<br>&quot;&quot;&quot;<br>Instruction:<br>Who were &quot;The Lumières&quot;?<br><br>Response:<br>&quot;&quot;&quot;<br>&quot;The Lumières were the inventors of the first motion picture camera. They were&quot;</pre><p>The prototyping stage is over. Before you proceed to finetuning models in production with large datasets, check out new specific tools to build a responsible Generative AI application.</p><h3>Responsible AI</h3><p>With great power comes great responsibility, right?</p><p>The <a href="https://ai.google.dev/responsible">Responsible Generative AI Toolkit</a> will help you build safer AI applications with Gemma. You’ll find expert guidance on safety strategies for the different aspects of your solution.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*W_SYa_3eYK4SxPbyLVrQvg.png" /></figure><p>Also, do not miss the <a href="https://pair-code.github.io/lit">Learning Interpretability Tool</a> (LIT) for visualizing and understanding the behavior of your Gemma models. Here’s the tool in action, investigating Gemma’s behavior:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/999/1*2qd_RFenWKZrm0t543Rosw.gif" /></figure><h3>Deploying Gemma</h3><p>We’re in <a href="https://cloud.google.com/vertex-ai/docs/start/introduction-mlops"><strong>MLOps</strong></a> territory here and you’ll find different LLM-optimized serving frameworks running on GPUs or TPUs.</p><p>Here are popular serving frameworks:</p><ul><li><a href="https://github.com/vllm-project/vllm">vLLM</a> (GPU)</li><li><a href="https://github.com/huggingface/text-generation-inference/blob/main/README.md">TGI: Text Generation Interface</a> (GPU)</li><li><a href="https://github.com/google/saxml">Saxml</a> (TPU or GPU)</li><li><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a> (NVIDIA Triton GPU)</li></ul><p>These frameworks come with prebuilt container images that you can easily deploy to Vertex AI.</p><p>Here is a simple notebook to get you started:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb">Finetune Gemma using KerasNLP and deploy to Vertex AI</a></li></ul><p>For production finetuning, you can use <a href="https://cloud.google.com/vertex-ai/docs/training/create-custom-job">Vertex AI custom training jobs</a>. Here are detailed notebooks:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb">Gemma Finetuning</a> (served by vLLM or HexLLM)</li><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_gemma_peft_finetuning_hf.ipynb">Gemma Finetuning</a> (served by TGI)</li></ul><p>Those notebooks focus on deployment and serving:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_vertex.ipynb">Gemma Deployment</a> (served by vLLM or HexLLM)</li><li><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_gke.ipynb">Gemma Deployment to GKE using TGI on GPU</a></li></ul><p>For more details and updates, check out the documentation:</p><ul><li><a href="https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma">Vertex AI</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra">GKE</a></li></ul><h3>All the best to Gemini and Gemma!</h3><p>After years of consolidation in machine learning hardware and software, it’s exciting to see the pace at which the overall ML landscape now evolves and in particular with LLMs. LLMs are technologies still in their infancy, so we can expect to see more breakthroughs in the near future.</p><p>I very much look forward to seeing what the ML community is going to build with Gemma!</p><p><strong>All the best to the Gemini and Gemma families!</strong></p><p><em>Follow me on </em><a href="https://twitter.com/PicardParis"><em>Twitter</em></a><em> or </em><a href="https://linkedin.com/in/PicardParis"><em>LinkedIn</em></a><em> for more cloud explorations</em>…</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4b0fc2a14851" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851">Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Moderating text with the Natural Language API]]></title>
            <link>https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/5d379727da2c</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[moderation]]></category>
            <category><![CDATA[nlp]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Fri, 16 Jun 2023 16:49:45 GMT</pubDate>
            <atom:updated>2023-09-12T09:40:16.984Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*k_L9ToCSSwqKeROq17wA5A.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/fr/@daninasplash">Da Nina</a> on <a href="https://unsplash.com/photos/MBqwXZTfkdA">Unsplash</a></figcaption></figure><blockquote>2023–09–12: text moderation got Generally Available (GA) over the summer + added link to Sep. blog post</blockquote><p>The <a href="https://cloud.google.com/natural-language/docs/">Natural Language API</a> lets you extract information from unstructured text using Google machine learning and provides a solution to the following problems:</p><ul><li>Sentiment analysis</li><li>Entity analysis</li><li>Entity sentiment analysis</li><li>Syntax analysis</li><li>Content classification</li><li><strong>Text moderation</strong></li></ul><h3>🔍 Moderation categories</h3><p>Text moderation lets you detect sensitive or harmful content. The first moderation category that comes to mind is “toxicity”, but there can be many more topics of interest. A <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model">PaLM 2</a>-based model powers the predictions and scores 16 categories:</p><pre>| ---------- | --------------------- | ----------------- | -------------- |<br>| Toxic      | Insult                | Public Safety     | War &amp; Conflict |<br>| Derogatory | Profanity             | Health            | Finance        |<br>| Violent    | Death, Harm &amp; Tragedy | Religion &amp; Belief | Politics       |<br>| Sexual     | Firearms &amp; Weapons    | Illicit Drugs     | Legal          |</pre><h3>⚡ Moderating text</h3><p>Like always, you can call the API through the REST/RPC interfaces or with idiomatic client libraries.</p><p>Here is an example using the Python client library (<a href="https://cloud.google.com/python/docs/reference/language">google-cloud-language</a>) and the moderate_text method:</p><pre>from google.cloud import language<br><br>def moderate_text(text: str) -&gt; language.ModerateTextResponse:<br>    client = language.LanguageServiceClient()<br>    document = language.Document(<br>        content=text,<br>        type_=language.Document.Type.PLAIN_TEXT,<br>    )<br>    return client.moderate_text(document=document)<br><br>text = (<br>    &quot;I have to read Ulysses by James Joyce.\n&quot;<br>    &quot;I&#39;m a little over halfway through and I hate it.\n&quot;<br>    &quot;What a pile of garbage!&quot;<br>)<br>response = moderate_text(text)</pre><blockquote><em>🚀 It’s fast! The model latency is very low, allowing real-time analyses.</em></blockquote><p>The response contains confidence scores for each moderation category. Let’s sort them out:</p><pre>import pandas as pd<br><br>def confidence(category: language.ClassificationCategory) -&gt; float:<br>    return category.confidence<br><br>columns = [&quot;category&quot;, &quot;confidence&quot;]<br>categories = sorted(<br>    response.moderation_categories,<br>    key=confidence,<br>    reverse=True,<br>)<br>data = ((category.name, category.confidence) for category in categories)<br>df = pd.DataFrame(columns=columns, data=data)<br><br>print(f&quot;Text analyzed:\n{text}\n&quot;)<br>print(f&quot;Moderation categories:\n{df}&quot;)</pre><p>You may typically ignore scores below 50% and calibrate your solution by defining upper limits (or buckets) for the confidence scores. In this example, depending on your thresholds, you may flag the text as disrespectful (toxic) and insulting:</p><pre>Text analyzed:<br>I have to read Ulysses by James Joyce.<br>I&#39;m a little over halfway through and I hate it.<br>What a pile of garbage!<br><br>Moderation categories:<br>                 category  confidence<br>0                   Toxic    0.680873<br>1                  Insult    0.609475<br>2               Profanity    0.482516<br>3                 Violent    0.333333<br>4                Politics    0.237705<br>5   Death, Harm &amp; Tragedy    0.189759<br>6                 Finance    0.176955<br>7       Religion &amp; Belief    0.151079<br>8                   Legal    0.100946<br>9                  Health    0.096305<br>10          Illicit Drugs    0.083333<br>11     Firearms &amp; Weapons    0.076923<br>12             Derogatory    0.073953<br>13         War &amp; Conflict    0.052632<br>14          Public Safety    0.051813<br>15                 Sexual    0.028222</pre><h3>🖖 More</h3><ul><li>To try it, run this Colab notebook: <a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb">Using the Natural Language API</a></li><li>See the <a href="https://cloud.google.com/natural-language/docs/languages#text_moderation">supported languages</a></li><li>Read more about <a href="https://cloud.google.com/natural-language/docs/moderating-text">text moderation</a></li><li>See the latest blog post on <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-text-moderation">Improving Trust in AI and Online Communities</a></li><li>Follow me on <a href="https://twitter.com/PicardParis">Twitter</a> or <a href="https://linkedin.com/in/PicardParis">LinkedIn</a> for more cloud explorations</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d379727da2c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c">Moderating text with the Natural Language API</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Better Way to Use Google Cloud from Colab]]></title>
            <link>https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/bb93f88b5021</guid>
            <category><![CDATA[jupyter-notebook]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[google-colab]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[announcements]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 13 Jun 2023 18:23:54 GMT</pubDate>
            <atom:updated>2023-06-13T18:23:54.271Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SsBZTw8ePeH59hdH" /><figcaption>Photo by <a href="https://unsplash.com/@pablogamedev?utm_source=medium&amp;utm_medium=referral">Pablo Arroyo</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>One of the best things about Colab is low friction. You open a notebook and Colab knows who you are based on your current Google account — there’s no need to separately login to Colab (unless you’re not already logged into Google).</p><p><strong>Problem</strong>: Google Cloud provides an amazingly powerful array of services, which work great in a Colab notebook, but you need to separately authenticate yourself to Google Cloud. This is not difficult but, till now, many developers resorted to using a service account, which adds a bit of complexity and, even worse, can lead to accidentally checking service account credentials into a repo, which is a serious security concern.</p><p><strong>Solution</strong>: We simplified the flow for you so that you can now use your Google Cloud project from a Colab notebook in a more straightforward way and hopefully without thinking about creating a service account.</p><p><strong>How does it work?</strong> In order to understand what’s new, let’s do a before-after comparison. Here is what can be found in some notebooks using a service account and downloading a private key:</p><pre># BEFORE<br>import os<br>from google.colab import auth<br>auth.authenticate_user()<br><br>PROJECT_ID = &quot;YOUR_PROJECT_ID&quot;<br>SA_NAME = &quot;YOUR_SERVICE_ACCOUNT_NAME&quot;<br>SA_EMAIL = f&quot;{SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com&quot;<br>!gcloud config set project $PROJECT_ID<br>!gcloud iam service-accounts create $SA_NAME<br>!gcloud iam service-accounts keys create ./key.json --iam-account $SA_EMAIL<br>os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;./key.json&quot;</pre><p>This is not necessary and there are two potential problems here:</p><ul><li>A service account is created. If you’re not the owner of the project, you may not have the permissions for this.</li><li>Service account credentials are manually downloaded at source code level. The private key can accidentally be pushed and made public.</li></ul><p>And here’s the simpler, service-account-less method:</p><pre># AFTER<br>from google.colab import auth<br>PROJECT_ID = &quot;YOUR_PROJECT_ID&quot;<br>auth.authenticate_user(project_id=PROJECT_ID)</pre><ul><li>You are authenticated for gcloud <a href="https://cloud.google.com/sdk/gcloud/reference">CLI commands</a> (like before).</li><li>You are also authenticated when using Google Cloud <a href="https://cloud.google.com/python/docs/reference">Python Client Libraries</a>.</li><li>Your default project is set.</li></ul><p><strong>The new version is shorter, simpler, and less error-prone.</strong></p><blockquote><em>If you’re new to Colab, would like to get more details or example notebooks, check out “</em><a href="https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=friends_link&amp;sk=1045f8cdd0b4b494fc2e1ef51edf1b6d"><em>Using Google Cloud from Colab</em></a><em>”.</em></blockquote><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bb93f88b5021" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021">A Better Way to Use Google Cloud from Colab</a> was originally published in <a href="https://medium.com/google-colab">Google Colab</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Google Cloud from Colab]]></title>
            <link>https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/75691f4a731</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[google-colab]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 13 Jun 2023 16:31:33 GMT</pubDate>
            <atom:updated>2023-06-16T16:28:51.151Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Screencast showing different steps of a notebook running in Colab" src="https://cdn-images-1.medium.com/max/1024/1*nLjcqvMkvZ5TRHF4pZjbHg.gif" /></figure><p>Google Colab is an amazing tool for Pythonistas. It can be used for a variety of tasks, from quickly experimenting with Python code to sharing extensive data processing notebooks with the world. Colab runs on Google Cloud, which gives you a boost when accessing cloud services because your code is running inside Google’s high performance network, and also offers a simple way to use Google Cloud services, letting you run powerful cloud workloads from your browser.</p><h3>📦️ Install dependencies</h3><p>Colab comes with hundreds of preinstalled packages, including pandas, numpy, matplotlib, Flask, Pillow, tensorflow, and pytorch. You can install additional required dependencies as needed.</p><p>For example, to analyze text with the power of machine learning using the <a href="https://cloud.google.com/natural-language/docs">Natural Language API</a>, install the google-cloud-language client library:</p><pre>!pip install google-cloud-language&gt;=2.9.1</pre><h3>🔑 Authenticate</h3><p>To authenticate with your Google Cloud account within the Colab notebook, use the authenticate_user method. A new parameter lets you specify your project ID:</p><pre>from google.colab import auth<br><br>PROJECT_ID = &quot;&quot;  # @param {type:&quot;string&quot;}<br><br>auth.authenticate_user(project_id=PROJECT_ID)</pre><p>After this step:</p><ul><li>👍 You are authenticated for gcloud <a href="https://cloud.google.com/sdk/gcloud/reference">CLI commands</a>.</li><li>👍 You are also authenticated when using Google Cloud <a href="https://cloud.google.com/python/docs/reference">Python client libraries</a>. Note that <strong>you generally won’t need to create a service account</strong>.</li><li>👍 Your default project is set.</li><li>👍 Your notebook can also access your Google Drive, letting you ingest or generate your own files.</li></ul><h3>🔓 Enable APIs</h3><p>Ensure required APIs are enabled. In our example, that’s the service language.googleapis.com:</p><pre>!gcloud services enable language.googleapis.com</pre><h3>🤯 Use Google Cloud services</h3><p>That’s it! You can now directly use the service by calling its Python client library:</p><pre>from google.cloud import language<br><br>def analyze_text_sentiment(text: str) -&gt; language.AnalyzeSentimentResponse:<br>    client = language.LanguageServiceClient()<br>    document = language.Document(<br>        content=text,<br>        type_=language.Document.Type.PLAIN_TEXT,<br>    )<br>    return client.analyze_sentiment(document=document)<br><br># Input<br>text = &quot;Python is a very readable language, ...&quot;  # @param {type:&quot;string&quot;}<br><br># Send a request to the API<br>response = analyze_text_sentiment(text)<br><br># Use the results<br>print(response)</pre><h3>📊 Benefit from notebook advanced features</h3><p>Colab is hosting a Jupyter notebook. I personally depict Colab as the “<em>Google Drive of notebooks</em>”. As notebooks have additional superpowers, this lets you nicely process and visualize your data, such as tables, images, or charts.</p><p>In our example, the function show_text_sentiment gathers results in a <em>pandas</em> DataFrame, which renders as a table:</p><figure><img alt="Screencast showing successive text sentiment analyses with results displayed in a table" src="https://cdn-images-1.medium.com/max/1024/1*5WIc70V9irgXcGgSb_ayBw.gif" /></figure><h3>✨ Check it out</h3><p>In these examples, click the “<em>Open in Colab</em>” link:</p><ul><li><a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20Google%20Cloud%20from%20Colab.ipynb">Using Google Cloud from Colab</a> (short)</li><li><a href="https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb">Using the Natural Language API with Python</a> (detailed)</li></ul><blockquote><em>💡 You can directly create a new notebook by opening the </em><a href="https://colab.new"><em>colab.new</em></a><em> URL.</em></blockquote><h3>👋 Have fun!</h3><ul><li>Read the <a href="https://medium.com/google-colab">Google Colab</a> blog to learn more about Colab.</li><li>Follow me (<a href="https://twitter.com/PicardParis">Twitter</a> / <a href="https://linkedin.com/in/PicardParis">LinkedIn</a>) for more cloud explorations ;)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=75691f4a731" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731">Using Google Cloud from Colab</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From pixels to information with Document AI]]></title>
            <link>https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/3476431c3010</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Wed, 15 Mar 2023 17:17:47 GMT</pubDate>
            <atom:updated>2023-03-15T17:17:47.561Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jj8xPtmBQlwTZ6K_qoiAEQ.png" /></figure><p>We’re seeing successively difficult problems getting solved thanks to machine learning (ML) models. For example, <a href="https://cloud.google.com/natural-language">Natural Language AI</a> and <a href="https://cloud.google.com/vision">Vision AI</a> extract insights from text and images, with human-like results. They solve problems central to the way we communicate:</p><ul><li>✅ Natural language processing (NLP)</li><li>✅ Optical character recognition (OCR)</li><li>✅ Handwriting recognition (HWR)</li></ul><p>What’s next? Well, it’s already here, with <a href="https://cloud.google.com/document-ai">Document AI</a>, and keeps growing:</p><ul><li>✅ Document understanding (DU)</li></ul><p>Document AI builds on these foundations to let you extract information from documents, in many forms:</p><ul><li>Text</li><li>Structure</li><li>Semantic entities</li><li>Normalized values</li><li>Enrichments</li><li>Qualitative signals</li></ul><p>In this article, you’ll see the following:</p><ul><li>An overview of Document AI</li><li>Practical and visual examples</li><li>A document processing demo (source code included)</li></ul><h3>Processing documents</h3><h4>Processor types</h4><p>There are as many document types as you can imagine so, to meet all needs, document processors are at the heart of Document AI. They can be of the following types:</p><ul><li><strong>General processors</strong> handle common tasks such as detecting document tables or parsing forms.</li><li><strong>Specialized processors</strong> analyze specific documents such as invoices, receipts, pay slips, bank statements, identity documents, official forms, etc.</li><li><strong>Custom processors</strong> meet other specific needs, letting you automatically train private ML models based on your own documents, and perform tasks such as custom document classification or custom entity detection.</li></ul><h4>Processor locations</h4><p>When you create a processor, you specify its location. This helps control where the documents will be processed. Here are the current multi-region locations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*vf34kofWBNhim4b5uSHicA.png" /></figure><p>In addition, some processors are available in single-region locations; this lets you address local regulation requirements. If you regularly process documents in real-time or large batches of documents, this can also help get responses with even lower latencies. As an example, here are the current locations for the “Document OCR” general processor:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*08MKu92DdUiLOc4nIxP8hw.png" /></figure><p>Note: API endpoints are named according to the convention {location}-documentai.googleapis.com.</p><p>For more information, check out <a href="https://cloud.google.com/document-ai/docs/regions">Regional and multi-regional support</a>.</p><h4>Processor versions</h4><p>Processors can evolve over time, to offer more precise results, new features, or fixes. For example, here are the current versions available for the “Expense Parser” specialized processor:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*5pGJPYaig4xPbKnc1K1t4g.png" /></figure><p>You may typically do the following:</p><ul><li>Use Stable version 1.3 in production.</li><li>Use Release Candidate version 1.4 to benefit from new features or test a future version.</li></ul><p>To stay updated, follow the <a href="https://cloud.google.com/document-ai/docs/release-notes">Release notes</a>.</p><h4>Interfaces</h4><p>Document AI is available to developers and practitioners through the usual interfaces:</p><ul><li><a href="https://cloud.google.com/document-ai/docs/reference/rest">REST API</a> (universal JSON-based interface)</li><li><a href="https://cloud.google.com/document-ai/docs/reference/rpc">RPC API</a> (low-latency gRPC interface, used by all Google services)</li><li><a href="https://cloud.google.com/document-ai/docs/quickstart-client-libraries#install_the_client_library">Client Libraries</a> (gRPC wrappers, to develop in your preferred programming language)</li><li><a href="https://console.cloud.google.com/ai/document-ai">Cloud Console</a> (web admin user interface)</li></ul><h4>Requests</h4><p>There are two ways you can process documents:</p><ul><li><strong>Synchronously</strong> with online requests, to analyze a single document and directly use the results</li><li><strong>Asynchronously</strong> with batch requests, to launch a batch processing operation on multiple or larger documents</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*wj-Vap3gqQxdVYnEgJyH7w.png" /></figure><h3>Extracting information from pixels</h3><p>Let’s start by analyzing this simple screenshot with the “Document OCR” general processor, and check how pixels become structured data.</p><p><strong>Input image</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*BVNXioR-3X9EYqgv.png" /></figure><p><strong>Document AI response</strong></p><ul><li>The response contains a Document instance.</li><li>The entire text of the input, detected in natural reading order, is serialized under Document.text.</li><li>The structured data is returned on a per page basis (a single page here).</li><li>The “Document OCR” processor returns a structural skeleton common to all processors.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*YZll14-jI-lSUB-htfbE_w.png" /></figure><p>The examples in this article show snake-case field names (like mime_type), using the convention used by gRPC and programming languages like Python. For camel-case environments (like REST/JSON), there is a direct mapping between the two conventions:</p><ul><li>mime_type ↔ mimeType</li><li>page_number ↔ pageNumber</li><li>…</li></ul><h3>Text levels</h3><p>For each page, four levels of text detection are returned:</p><ul><li>blocks</li><li>paragraphs</li><li>lines</li><li>tokens</li></ul><p>In these lists, each item exposes a layout including the item&#39;s position relative to the page in bounding_poly.normalized_vertices.</p><p>This lets us, for example, highlight the 22 detected tokens:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*A2GXdeserG9If_3v.gif" /></figure><p>Here is the last token:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*_vPlheSSbY2hqADWexImCQ.png" /></figure><p>Note: Float values are presented truncated for the sake of readability.</p><h3>Language support</h3><p>Documents are often written using one single language, but sometimes use multiple languages. You can retrieve the detected languages at different text levels.</p><p>In our previous example, two blocks are detected ( pages[0].blocks[]). Let&#39;s highlight them:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*qcMudBjq4iYZOeSo.gif" /></figure><p>The left block is a mix of German, French, and English, while the right block is English only. Here is how the three languages are reported at the page level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*F9luc3xz2V-HQAK90DBUaw.png" /></figure><p>Note: At this level, the language confidence ratios roughly correspond to the proportion of text detected in each language.</p><p>Now, let’s highlight the five detected lines ( pages[0].lines[]):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*9MQdRYljnE7pyxH6.gif" /></figure><p>Each language is also reported at the line level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*J3JpAXRPz8YvRMCNTP18uA.png" /></figure><p>If needed, you can get language info at the token level too. “Question” is the same word in French and in English, and is adequately returned as an English token in this context:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*pnqEAxbUmMTutIJAZJ2Nmg.png" /></figure><p>In the screenshot, did you notice something peculiar in the left block?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/0*FhNl5BWWBb1DRzDj.gif" /></figure><p>Well, punctuation rules can be different between languages. French uses a typographical space for double punctuation marks (“double” as “written in two parts”, such as in &quot;!&quot;, &quot;?&quot;, &quot;&quot;&quot;,...). Punctuation is an important part of languages that can get &quot;lost in translation&quot;. Here, the space is preserved in the transcription of &quot;Bienvenue !&quot;. Nice touch Document AI or, should I say, touché!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*GoB_u-QIjw5v6kwdvnkKnw.png" /></figure><p>For more information, see <a href="https://cloud.google.com/document-ai/docs/languages">Language support</a>.</p><h3>Handwriting detection</h3><p>Now — a much harder problem — let’s check how handwriting is handled.</p><p>This example is a mix of printed and handwritten text, where I wrote both a question and an answer. Here are the detected tokens:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aNu-Z50vqkp0cBTy.gif" /></figure><p>I am pleasantly surprised to see my own handwriting transcribed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*2fu6sl8TJ4KNRWpxStmfUg.png" /></figure><p>I asked my family (used to writing French) to do the same. Each unique handwriting sample also gets correctly detected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*O_yCkRBxOKNxBMVA.gif" /></figure><p>… and transcribed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*K9Gi_ZyVQ7eAuPjgJFM6Mw.png" /></figure><p>This can look magical but that’s one of the goals of ML models: return results as close as possible to human responses.</p><h3>Confidence scores</h3><p>We make mistakes, and so can ML models. To better appreciate the structured data you get, results include confidence scores:</p><ul><li>Confidence scores do not represent accuracy.</li><li>They represent how confident the model is with the extracted results.</li><li>They let you — and your users — ponder the model’s extractions.</li></ul><p>Let’s overlay confidence scores on top of the previous example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qKTXjDXeOnu77iIP.gif" /></figure><p>After grouping them in buckets, confidence scores appear to be generally high, with a few outliers:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/809/0*Qje3vHm13P04tlQ_.png" /></figure><p>The lowest confidence score here is 57%. It corresponds to a handwritten word (token) that is both short (less context given for confidence) and not particularly legible indeed:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*r80MhJ4EcgSFet63xuMD-w.png" /></figure><p>For best results, keep in mind these general rules of thumb:</p><ul><li>ML results are best guesses, which can be correct or incorrect.</li><li>Results with lower confidence scores are more likely to be incorrect guesses.</li><li>If we can’t smoothly read a part of a document, or need to think twice about it, it’s probably also harder for the ML model.</li></ul><p>Although all text is correctly transcribed in the presented examples, this won’t always be the case depending on the input document. To build safer solutions — especially with critical business applications — you may consider the following:</p><ul><li>Be clear with your users that results are auto-generated.</li><li>Communicate the scope and limitations of your solution.</li><li>Improve the user experience with interpretable information or filterable results.</li><li>Design your processes to trigger human intervention on lower confidence scores.</li><li>Monitor your solution to detect changes over time (drift of stable metrics, decline in user satisfaction, etc.).</li></ul><p>To learn more about AI principles and best practices, check out <a href="https://ai.google/responsibilities/responsible-ai-practices">Responsible AI practices</a>.</p><h3>Rotation, skew, distortion</h3><p>How many times did you scan a document upside down by mistake? Well, this shouldn’t be a concern anymore. Text detection is very robust to rotation, skew, and other distortions.</p><p>In this example, the webcam input is not only upside down but also skewed, blurry, and with text in unusual orientations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*X9bZmB1QIADOJWp1.jpg" /></figure><p>Before further analysis, Document AI considers the best reading orientation, at the page level, and preprocesses (deskews) each page if needed. This gives you results that can be used and visualized in a more natural way. Once processed by Document AI, the preceding example gets easier to read, without straining your neck:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OClQBbzZjDwd4182.gif" /></figure><p>In the results, each page has an image field by default. This represents the image - deskewed if needed - used by Document AI to extract information. All the page results and coordinates are relative to this image. When a page has been deskewed, a transforms element is present and contains the list of transformation matrices applied to the image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*3bn1qDt-9cXmmbyM71On5Q.png" /></figure><p>Notes:</p><ul><li>Page images can be in different formats. For instance, if your input is a JPEG image, the response will include either the same JPEG or a deskewed PNG (as in the earlier example).</li><li>Deskewed images have larger dimensions (deskewing adds blank outer areas).</li><li>If you don’t need visual results in your solution, you can specify a field_mask in your request to receive lighter responses, with only your fields of interest.</li></ul><h3>Orientation</h3><p>Documents don’t always have all of their text in a single orientation, in which case deskewing alone is not enough. In this example, the sentence is broken out in four different orientations. Each part gets properly recognized and processed in its natural orientation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hrnNBdXqI-FejECn.gif" /></figure><p>Orientations are reported in the layout field:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*0RhaOXSb8oXT1s_o99f4Sg.png" /></figure><p>Note: Orientations are returned at each OCR level ( blocks, paragraphs, lines, and tokens).</p><h3>Noise</h3><p>When documents come from our analog world, you can expect … the unexpected. As ML models are trained from real-world samples — containing real-life noise — a very interesting outcome is that Document AI is also significantly robust to noise.</p><p>In this example with crumpled paper, the text starts to be difficult to read but still gets correctly transcribed by the OCR model:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*P-SQC50hw62rcppH.gif" /></figure><p>Documents can also be dirty or stained. With the same sample, this keeps working after adding some layers of noise:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kH1AQmX_vGemiApv.gif" /></figure><p>In both cases, the exact same text is correctly detected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*Mr9h8OrJ7fplwDx1ipDCww.png" /></figure><p>You’ve seen most core features. They are supported by the “Document OCR” general processor as well as the other processors, which leverage these features to focus on more specific document types and provide additional information. Let’s check the next-level processor: the “Form Parser” processor.</p><h3>Form fields</h3><p>The “Form Parser” processor lets you detect form fields. A form field is the combination of a field name and a field value, also called a key-value pair.</p><p>In this example, printed and handwritten text is detected as seen before:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*94XK4nbuOgLsFRUF.gif" /></figure><p>In addition, the form parser returns a list of form_fields:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*2jI2xxd09LPJbisXTzvqmQ.png" /></figure><p>Here is how the detected key-value pairs are returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*HSv255xZseQziHbCW72h5w.png" /></figure><p>And here are their detected bounding boxes:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*tCOInmMLqZwdOwqW.gif" /></figure><p>Note: Form fields can follow flexible layouts. In this example, keys and values are in a left-right order. You’ll see a right-left example next. Those are just simple arbitrary examples. It also works with vertical or free layouts where keys and values are logically (visually) related.</p><h3>Checkboxes</h3><p>The form parser also detects checkboxes. A checkbox is actually a particular form field value.</p><p>This example is a French exam with affirmations that should be checked when exact. To test this, I used checkboxes of different kinds, printed or handmade. All form fields are detected, with the affirmations as field names and the corresponding checkboxes as field values:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UJ_fx2dF22Ce1TW4.gif" /></figure><p>When a checkbox is detected, the form field contains an additional value_type field, which value is either unfilled_checkbox or filled_checkbox:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*UwE4Xvo8Pdy-H_qA7M6DVw.png" /></figure><p>Being able to analyze forms can lead to huge time savings, by consolidating — or even autoprocessing — content for you. The preceding checkbox detection example was actually an evolution of a prior experiment to autocorrect my wife’s pile of exam copies. The proof of concept got better using checkboxes, but was already conclusive enough with True/False handwritten answers. Here is how it can autocorrect and autograde:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BEzESKpTcyWrdorH.gif" /></figure><h3>Tables</h3><p>The form parser can also detect another important structural element: tables.</p><p>In this example, words are presented in a tabular layout without any borders. The form parser finds a table very close to the (hidden) layout. Here are the detected cells:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fpueWsZxtFjy6MeE.gif" /></figure><p>In this other example, some cells are filled with text while others are blank. There are enough signals for the form parser to detect a tabular structure:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yDxJ4Zy33HcXOyj8.gif" /></figure><p>When tables are detected, the form parser returns a list of tables with their rows and cells. Here is how the table is returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*DwoO44-zi8jc3X6IWBa4-w.png" /></figure><p>And here is the first cell:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*yXZF0PBB6x5De_Fe4UEqxw.png" /></figure><h3>Specialized processors</h3><p>Specialized processors focus on domain-specific documents and extract <strong>entities</strong>. They cover many different document types that can currently be classified in the following families:</p><ul><li><strong>Procurement</strong> — receipts, invoices, utility bills, purchase orders,…</li><li><strong>Lending</strong> — bank statements, pay slips, official forms,…</li><li><strong>Identity</strong> — national IDs, driver licenses, passports,…</li><li><strong>Contract</strong> — legal agreements</li></ul><p>For example, procurement processors typically detect the total_amount and currency entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*FLNM6pGN0oRrzv_hzeEddg.png" /></figure><p>For more information, check out <a href="https://cloud.google.com/document-ai/docs/fields">Fields detected</a>.</p><h3>Expenses</h3><p>The “Expense Parser” lets you process receipts of various types. Let’s analyze this actual (French) receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*9KGKV8_agwvJY3A7.gif" /></figure><p>A few remarks:</p><ul><li>All expected entities are extracted.</li><li>Ideally (to be picky), I’d like to get the tax rate too. If there’s customer demand for it, this may be a supported entity in a future version.</li><li>Due to the receipt’s very thin paper, the text on the back side is visible by transparency (another type of noise).</li><li>The supplier name is wrong (it’s actually mirrored text from the back side) but with a very low confidence (4%). You’d typically handle it with special care (it’s shown differently here) or ignore it.</li><li>The actual supplier info is hidden (the top part of the receipt is folded) on purpose. You’ll see another example with supplier data a bit later.</li><li>Receipts are often printed on single (sometimes on multiple) pages. The expense parser supports analyzing expense documents of up to 10 pages.</li></ul><p>Procurement documents often list parts of the data in tabular layouts. Here, they’re returned as many line_item/* entities. When the entities are detected as part of a hierarchical structure, the results are nested in the properties field of a parent entity, providing an additional level of information. Here&#39;s an excerpt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*_OdZdElj3-qYJHDOTMQw2A.png" /></figure><p>For more information, see the <a href="https://cloud.google.com/document-ai/docs/processors-list#processor_expense-parser">Expense Parser</a> details.</p><h3>Entity normalization</h3><p>Getting results is generally not enough. Results often need to be handled in a post-processing stage, which can be both time consuming and a source of errors. To address this, specialized processors also return normalized values when possible. This lets you directly use standard values consolidated from the context of the whole document.</p><p>Let’s check it with this other receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kusJyoZehiKDL4M6.jpg" /></figure><p>First, the receipt currency is returned with its standard code under normalized_value:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*PPG-t22mNm8N_uAgfIFHww.png" /></figure><p>Then, the receipt is dated 11/12/2022. But is it Nov. 12 or Dec. 11? Document AI uses the context of the document (a French receipt) and provides a normalized value that removes all ambiguity:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*7Mth-IX8WGmLXKbvLR1HIg.png" /></figure><p>Likewise, the receipt contains a purchase time, written in a non-standard way. The result also includes a canonical value that avoids any interpretation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*Jyrs9cI4URn0upimWwtgVg.png" /></figure><p>Normalized values simplify the post-processing stage:</p><ul><li>They provide standard values that are straightforward to use (e.g. enable direct storage in a data warehouse).</li><li>They prevent bugs (esp. the recurring developer mistakes we make when converting data).</li><li>They remove ambiguity and avoid incorrect interpretations by using the context of the whole document.</li></ul><p>For more information, check out the <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#NormalizedValue">NormalizedValue</a> structure.</p><h3>Entity enrichment</h3><p>Did you notice there was more information in the receipt?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CxkHjnhkR9wrOUK_.jpg" /></figure><ul><li><em>“Maison Jeanne d’Arc, Place de Gaulle”</em> mentions a Joan of Arc’s House and a place name. Nonetheless, there is no address, zip code, city, or even country.</li><li>This receipt comes most likely from a museum in France, but Joan of Arc lived in a few places (starting with her birthplace in the Domrémy village).</li><li>So, which location does this correspond to?</li><li>A manual search should give hints to investigate, but can we keep this automated?</li></ul><p>Extracting the information behind the data requires extra knowledge or human investigation. An automated solution would generally ignore this partial data, but Document AI handles this in a unique way. To understand the world’s information, Google has been consistently analyzing the web for over 20 years. The result is a gigantic up-to-date knowledge base called the Knowledge Graph. Document AI leverages this knowledge graph to normalize and enrich entities.</p><p>First, the supplier is correctly detected and normalized with its usual name:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*h6gfiTACcy8mLQD0XqVyGQ.png" /></figure><p>Then, the supplier city is also returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*HjNW6mcFw-CoKWlhvnD8WA.png" /></figure><p>Note: Joan of Arc spent some time in Orléans in 1429, as she led the liberation of the besieged city at the age of 17 (but that’s another story).</p><p>And finally, the complete and canonical supplier address is also part of the results, closing our case here:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*1paAmdvSYvQDQKYDgFRn6Q.png" /></figure><p>Enriched entities bring significant value:</p><ul><li>They are canonical results, which avoids information conflicts and discrepancies.</li><li>They can add information, which prevents ignoring useful data.</li><li>They can complete or fix partially correct data.</li><li>In a nutshell, they provide information that is reliable, consistent, and comparable.</li></ul><p>Here is a recap of the expected — as well as the non-obvious — entities detected in the receipt:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*STXit5_dgb8JVQ_5.jpg" /></figure><p>Note: The slightest characteristic element can be sufficient to detect an entity. I’ve been regularly surprised to get entities I wasn’t expecting, to eventually realize I had missed clues in the document. For example, I recently wondered how the expense processor was able to identify a specific store. The receipt only specified a zip code and the retail chain has several stores in my neighborhood. Well, a public phone number (hidden in the footer) was enough to uniquely identify the store in question and provide its full address.</p><p>To learn more about the Knowledge Graph and possible enrichments, check out <a href="https://cloud.google.com/document-ai/docs/ekg-enrichment">Enrichment and normalization</a>.</p><h3>Invoices</h3><p>Invoices are the most elaborate type of procurement documents, often spreading over multiple pages.</p><p>Here’s an example showing entities extracted by the “Invoice Parser”:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*31ffw3l1v8SEB03k.gif" /></figure><p>A few remarks:</p><ul><li>The relevant information is extracted (even the supplier tax ID is detected on the 90°-rotated left side).</li><li>As seen earlier with the expense parser, the invoice parser extracts many line_item/* entities but also supplier_* and receiver_* info.</li><li>This is a typical invoice (issued by the historical French energy provider) with many numbers on the first pages.</li><li>The original source is a PDF (digital) that was anonymized, printed, stained, and scanned as a PDF (raster).</li></ul><p>For more information, see the <a href="https://cloud.google.com/document-ai/docs/processors-list#processor_invoice-processor">Invoice Parser</a> details.</p><h3>Barcodes</h3><p>Barcode detection is enabled for some processors. In this example, the invoice parser detects the barcode (a manifest number):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*goBikGmCIaE_7O5Y.gif" /></figure><p>Note: I didn’t have an invoice with barcodes at hand, so I used a (slightly anonymized) packing list.</p><p>Barcodes are returned, at the page level, like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*acC_Pwcy2q4ml9rmoZtf4Q.png" /></figure><p>For more information, check out the <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#detectedbarcode">DetectedBarcode</a> structure.</p><h3>Identity documents</h3><p>Identity processors let you extract identity entities. In this US passport specimen (credit: <a href="https://twitter.com/travelgov/status/1537072860346318849">Bureau of Consular Affairs</a>), the expected fields can be automatically extracted:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BOEVjAwzrxgeNjNP.gif" /></figure><p>For more details about identity processors, you can read my previous article <a href="https://cloud.google.com/blog/topics/developers-practitioners/automate-identity-document-processing-document-ai">Automate identity document processing</a>.</p><h3>Document signals</h3><p>Some processors return document signals — information relative to the document itself.</p><p>For example, the “Document OCR” processor returns quality scores for the document, estimating the defects that might impact the accuracy of the results.</p><p>The preceding crumpled paper example gets a high quality score of 95%, with glare as a potential defect:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1W1bFYhuI8VejI1G.jpg" /></figure><p>The same example, at a 4x lower resolution, gets a lower quality score of 53%, with blurriness detected as the main potential issue:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*BKBsDLI8Nms2cm4A.jpg" /></figure><p>Some additional remarks:</p><ul><li>A low score lets you flag documents that may need a manual review (or a new better capture).</li><li>Surprisingly, the exact same text is correctly extracted in both cases (I checked twice) but that might not always be the case.</li><li>The quality scores are returned both as entities (with a quality_score parent entity) and in the image_quality_scores field at the page level (see <a href="https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#imagequalityscores">ImageQualityScores</a>).</li><li>This was tested with release candidate pretrained-ocr-v1.1-2022-09-12.</li></ul><p>Here’s how the image quality scores are returned at the page level:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*3ayVT7oPZ1It2EAJBQWwkg.png" /></figure><p>Likewise, the “Identity Proofing” processor gives you signals about the validity of ID documents.</p><p>First, this can help detect whether documents look like IDs. This preceding example is a random document analyzed as NOT_AN_ID:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*K07HHYahhA-5YlDH.jpg" /></figure><p>Here are the corresponding entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*CCXyY2lYJzn6wJlPQ-4ogA.png" /></figure><p>The preceding passport example does get detected as an ID but also triggers useful fraud signals:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*EGtPKZlJIdYIi0xO.gif" /></figure><p>The results include evidences that #1 it’s a specimen, #2 which can be found online:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*1SQiOGsvSBJ4a4W4YP6evA.png" /></figure><p>As new use cases appear, it’s likely that some processors will extract new document signals. Follow the <a href="https://cloud.google.com/document-ai/docs/release-notes">Release notes</a> to stay updated.</p><h3>Pre-trained processors</h3><p>Document AI is already huge and keeps evolving. Here is a screencast showing the current processor gallery and how to create a processor from the Cloud Console:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SaQV-rhnbBI8mBqB.gif" /></figure><h3>Custom processors with Workbench</h3><p>If you have your own business-specific documents, you may wish to extract custom entities not covered by the existing processors. Document AI Workbench can help you solve this by creating your own custom processor, trained with your own documents, in two ways:</p><ul><li>With <strong>uptraining</strong>, you can <strong>extend an existing processor</strong>.</li><li>You can also create a <strong>new processor from scratch</strong>.</li></ul><p>For more information, watch this great introduction video made by my teammate Holt:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FBkE69dM0G-w%3Flist%3DPLIivdWyY5sqIR88BxIK-3w14Vm-jTH1id&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DBkE69dM0G-w&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FBkE69dM0G-w%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/de47c3c4fffcd14ff4cbc0f2a569fc8e/href">https://medium.com/media/de47c3c4fffcd14ff4cbc0f2a569fc8e/href</a></iframe><h3>Performance</h3><p>To capture your documents, you’ll probably use either:</p><ul><li>A scanner</li><li>A camera</li></ul><p>With scanners, you’ll need to choose a resolution in dots per inch (dpi). To optimize the performance (especially the accuracy and consistency of the results), you can keep in mind the following:</p><ul><li>300 dpi is often a nice spot</li><li>200 dpi is generally a hard minimum</li></ul><p>Here are the resolutions needed to capture a sheet of paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gmmWWHQNbc2fK_IX.png" /></figure><p>Note: Dimensions are presented in the horizontal orientation (image sensors generally have a landscape native resolution).</p><p>With cameras, the captured dots per inch depend on the camera resolution but also on the way you zoom on the document. Here’s an example with an A4 sheet of paper:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6bmm-VT6DzFHx_vO.png" /></figure><p>This translates into different dpi ranges. Here are indicative values:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A40rNOXGuWPL2YSO.png" /></figure><p>To give you an idea, here is how a PNG image size evolves when capturing the same document at different resolutions. The total number of pixels is a surface. Though the PNG compression slightly limits the growth, the size increases almost quadratically:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7iDRnwpnuJz3cUez.png" /></figure><p>A few general observations:</p><ul><li>Document captures are usually in the 100–600 dpi range, depending on the use case.</li><li>100–200 dpi may be enough for some specific end user solutions, for example to process small receipts captured from mobile phone cameras or laptop webcams.</li><li>Scans of 300–600 dpi may provide better results but will generate larger files and make your processing pipeline slower.</li><li>Scans above 600 dpi are less likely. They are useful for very small documents (such as slides or negative films) or documents that need to be upscaled (e.g. to print a poster).</li><li>If your use case involves documents with small text, barcodes, or very noisy (e.g. blurry) inputs, you’ll typically need to compensate with a higher capture resolution.</li><li>Capture devices have noise intrinsic to the technology they use. They generally have options that will produce different results. Also, image sensors are more noisy in low-light environments.</li><li>Every conversion step in your architecture is likely to introduce side effects and a potential loss of information.</li><li>Lossy image formats (such as JPEG) can nicely reduce image sizes. On the other hand, too high compression levels (a setting) can generate artifacts with a negative impact on the detection performance (causing a strong degradation of image quality and consequently of result accuracy).</li><li>Lossless formats (such as PNG) imply larger file sizes but may bring the benefit of using lower resolutions. They may also be more future-proof (e.g. for archival) by preserving the possibility of later conversions to other formats or to smaller resolutions without losing information.</li><li>Storage costs are rather low, so it may also be interesting to consider higher acceptable scanning resolutions, to optimize both the performance and the overall cost of your solution.</li><li>In a nutshell, choosing capture devices, resolutions, and formats depends on your use case and is a tradeoff between accuracy, speed, and cost.</li></ul><p>To finetune your solution and get more accurate, faster, or consistent results, you may consider the following:</p><ul><li>Check the performance of your solution with worst-case documents at different resolutions.</li><li>Have a clear understanding of the different formats used in your processing pipeline.</li><li>Test your solution with actual documents (as close as possible to what you’ll have in production). Also, as the ML models are trained on real documents, you may get better results when testing real documents.</li><li>Keep a lossless format upstream when possible; documents can be converted to a lossy format downstream.</li><li>Evaluate whether capturing at a higher resolution and then downscaling produces better results.</li><li>When using lossy formats, finetune the compression level. There is usually a threshold at which compression artifacts become negligible while maintaining a good compression ratio.</li><li>When using cameras, control your lighting environment and avoid low-light conditions.</li><li>Make your users aware that blurry captures may decrease the performance.</li></ul><p>For example:</p><ul><li>The examples in this article can all be analyzed using cameras with resolutions ranging from 100 to 220 dpi. However, results won’t always be consistent with low resolutions.</li><li>For practical reasons, I mostly use laptop, mobile phone, or dedicated cameras. There are excellent ones (called document cameras) that can sit on your desk and provide solid captures with a fast auto-focus.</li><li>For an enterprise-grade solution, I’d probably stick to 300 dpi scans. Multifunction printers generally have scanners that provide excellent PNG, PDF, or TIFF captures.</li><li>The barcode (contained in the preceding packing list sample) starts to be detected at 150 dpi with a scanner, and at 200 dpi with a webcam. After being photocopied (i.e. scanned + printed), it’s still detected at 150 dpi with a scanner but needs to be zoomed in with the webcam.</li><li>While capturing documents from a browser and a webcam, I checked the HTML canvas default format: it’s PNG (great!) but in RGBA (with a transparency channel, unnecessary for document images). This made the data larger without any benefit. The optimization was a parameter away (canvas.getContext(&#39;2d&#39;) → canvas.getContext(&#39;2d&#39;, { alpha: false })) and made the captures sent to the backend 9 to 18% smaller.</li></ul><h3>Sample demo</h3><p>I put myself in your shoes to see what it takes to build a document processing prototype and made the following choices:</p><ul><li>A frontend using vanilla JavaScript, responsible for managing user interactions and input documents</li><li>A backend using the Python client library, responsible for all document processings and renderings</li><li>Synchronous calls to Document AI, to process documents in real-time</li><li>No cloud storage, to avoid storing personally identifiable information (analyses are stored client-side)</li></ul><p>Here is the chosen software stack based on open-source Python projects:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WDZhCqNnLdYOPh7b.png" /></figure><p>… and one possible architecture to deploy it to production using Cloud Run:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kE8SL9syaPbfSkMm.png" /></figure><p>Here is the core function I use to process a document live in Python:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/659/1*LyMVoCHRpo-xFR1T0Vy_kg.png" /></figure><p>To make sure that “what you see is what you get”, the sample images and animations in this article have been generated by the demo:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*N3Gk5aQe91lR4owf.gif" /></figure><p>Note: To avoid making repeated and unnecessary calls to Document AI while developing the app, sample document analyses are cached (JSON serialization). This lets you check the returned structured documents and also explains why responses are immediate in this (real-time, not sped up) screencast.</p><p>It takes seconds to analyze a document. Here is an example with the user uploading a PDF scan:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qRuSmdeL3ZPt5wWq.gif" /></figure><p>You can also take camera captures from the web app:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xe3R6pX9-NOI6mkg.gif" /></figure><p>Check out the <a href="https://github.com/GoogleCloudPlatform/document-ai-samples/tree/main/web-app-pix2info-python">source code</a> and feel free to reuse it. You’ll also find instructions to deploy it as a serverless app.</p><h3>More?</h3><ul><li>Test documents? → Try <a href="https://cloud.google.com/document-ai/docs/drag-and-drop">Document AI in your browser</a>.</li><li>Video series? → Watch <a href="https://goo.gle/FutureOfDocuments">The future of documents</a>.</li><li>New to Google Cloud? → Check out the <a href="https://cloud.google.com/free/docs/free-cloud-features">$300 free trial offer</a>.</li><li>Learn and grow? → Become a <a href="https://cloud.google.com/innovators">Google Cloud Innovator</a>.</li><li>Feedback or questions? → Reach out on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>).</li></ul><p><em>Originally published on </em><a href="https://github.com/GoogleCloudPlatform/document-ai-samples/blob/main/web-app-pix2info-python/README.md"><em>GitHub</em></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3476431c3010" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010">From pixels to information with Document AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automate identity document processing with Document AI]]></title>
            <link>https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/912e1011e164</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[document-management]]></category>
            <category><![CDATA[programming]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 02 Jun 2022 12:32:10 GMT</pubDate>
            <atom:updated>2022-06-02T12:34:24.591Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FYi9mbiyo0ldGxOwtQEAAw.png" /></figure><p>How many times have you filled out forms requesting personal information? It’s probably too many times to count. When online and signed in, you can save a lot of time thanks to your browser’s autofill feature. In other cases, you often have to provide the same data manually, again and again. The first Document AI identity processors are now generally available and can help you solve this problem.</p><p>In this post, you’ll see how to…</p><ul><li>Process identity documents with Document AI</li><li>Create your own identity form autofiller</li></ul><h3>Use cases</h3><p>Here are a few situations that you’ve probably encountered:</p><ul><li><strong>Financial accounts</strong>: Companies need to validate the identity of individuals. When creating a customer account, you need to present a government-issued ID for manual validation.</li><li><strong>Transportation networks</strong>: To handle subscriptions, operators often manage fleets of custom identity-like cards. These cards are used for in-person validation, and they require an ID photo.</li><li><strong>Identity gates</strong>: When crossing a border (or even when flying domestically), you need to pass an identity check. The main gates have streamlined processes and are generally well equipped to scale with the traffic. On the contrary, smaller gates along borders can have manual processes — sometimes on the way in and the way out — which can lead to long lines and delays.</li><li><strong>Hotels</strong>: When traveling abroad and checking in, you often need to show your passport for a scan. Sometimes, you also need to fill out a longer paper form and write down the same data.</li><li><strong>Customer benefits</strong>: For benefit certificates or loyalty cards, you generally have to provide personal info, which can include a portrait photo.</li></ul><p>In these examples, the requested info — including the portrait photo — is already on your identity document. Moreover, an official authority has already validated it. Checking or retrieving the data directly from this source of truth would not only make processes faster and more effective, but also remove a lot of friction for end users.</p><h3>Identity processors</h3><h4>Processor types</h4><p>Each Document AI identity processor is a machine learning model trained to extract information from a standard ID document such as:</p><ul><li>Driver license</li><li>National ID</li><li>Passport</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Frzqp_Q52oefAtYkaOOgpA.png" /></figure><blockquote>Note: an ID can have information on both sides, so identity processors support up to two pages per document.</blockquote><h4>Availability</h4><p><strong>Generally available</strong> as of June 2022, you can use two US identity processors in production:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*cobi6drwCVZcnVtKuZXHMQ.png" /></figure><p>Currently available in <strong>Preview</strong>:</p><ul><li>The Identity Doc Fraud Detector, to check whether an ID document has been tampered with</li><li>Three French identity processors</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*TEZeOERZ2sXsWcy_8pTA4w.png" /></figure><p>Notes:</p><ul><li>More identity processors are in the pipe.</li><li>To request access to processors in Preview, please fill out the <a href="https://docs.google.com/forms/d/e/1FAIpQLSc_6s8jsHLZWWE0aSX0bdmk24XDoPiE_oq5enDApLcp1VKJ-Q/viewform">Access Request Form</a>.</li></ul><h4>Processor creation</h4><p>You can create a processor:</p><ul><li><strong>Manually</strong> from Cloud Console (web admin UI)</li><li><strong>Programmatically</strong> with the API</li></ul><p>Processors are location-based. This helps guarantee where processing will occur for each processor.</p><p>Here are the current multi-region locations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*gTsVHyU5McPy9QArRU3oiQ.png" /></figure><p>Once you’ve created a processor, you reference it with its ID (PROCESSOR_ID hereafter).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AjqSo2y2c7gpgCz3IbgUxQ.gif" /></figure><blockquote>Note: To manage processors programmatically, see the codelab <a href="https://codelabs.developers.google.com/codelabs/cloud-documentai-manage-processors-python">Managing Document AI processors with Python</a>.</blockquote><h3>Document processing</h3><p>You can process documents in two ways:</p><ul><li>Synchronously with an <strong>online request</strong>, to analyze a single document and directly use the results</li><li>Asynchronously with a <strong>batch request</strong>, to launch a batch processing operation on multiple or larger documents</li></ul><h4>Online requests</h4><p>Example of a REST online request:</p><ul><li>The method is named process.</li><li>The input document here is a PNG image (base64 encoded).</li><li>This request is processed in the European Union.</li><li>The response is returned synchronously.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*laxgKwcnPcXYA8o65Olx8g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*HVJXyLrE-xvWK_dGkAvGdA.png" /></figure><h4>Batch requests</h4><p>Example of a REST batch request:</p><ul><li>The method is named batchProcess.</li><li>The batchProcess method launches the batch processing of multiple documents.</li><li>This request is processed in the United States.</li><li>The response is returned asynchronously; output files will be stored under my-storage-bucket/output/.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*DSKr6L1XLzUKOR0PutZ4wg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Qn5Aa9y1LICaRCwevsnCXQ.png" /></figure><h4>Interfaces</h4><p>Document AI is available through the usual Google Cloud interfaces:</p><ul><li>The RPC API (low-latency gRPC)</li><li>The REST API (JSON requests and responses)</li><li>Client libraries (gRPC wrappers, currently available for Python, Node.js, and Java)</li><li>Cloud Console (web admin UI)</li></ul><blockquote>Note: With the client libraries, you can develop in your preferred programming language. You’ll see an example later in this post.</blockquote><h3>Identity fields</h3><p>A typical REST response looks like the following:</p><ul><li>The text and pages fields include the OCR data detected by the underlying ML models. This part is common to all Document AI processors.</li><li>The entities list contains the fields specifically detected by the identity processor.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*Nqsohxiz_Y8xisRR2AcFkA.png" /></figure><p>Here are the detectable identity fields:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*53J9QMAGacEIaqhQPM0m7w.png" /></figure><p>Please note that Address and MRZ Code are optional fields. For example, a US passport contains an MRZ but no address.</p><h3>Fraud detection</h3><p>Available in preview, the <strong>Identity Doc Fraud Detector</strong> helps detect tampering attempts. Typically, when an identity document does not “pass” the fraud detector, your automated process can block the attempt or trigger a human validation.</p><p>Here is an example of signals returned:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*k0ylMb9Ghac5MUO1tZCizQ.png" /></figure><h3>Sample demo</h3><p>You can process a document live with just a few lines of code.</p><p>Here is a Python example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*rCVvAfK1kp69IItzZZPVvg.png" /></figure><p>This function uses the Python client library:</p><ul><li>The input is a file (any format supported by the processor).</li><li>client is an API wrapper (configured for processing to take place in the desired location).</li><li>process_document calls the API process method, which returns results in seconds.</li><li>The output is a structured Document.</li></ul><p>You can collect the detected fields by parsing the document entities:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*LiBNpP9sCXGUDO7OkCbjtw.png" /></figure><blockquote>Note: This function builds a mapping ready to be sent to a frontend. A similar function can be used for other specialized processors.</blockquote><p>Finalize your app:</p><ul><li>Define your user experience and architecture</li><li>Implement your backend and its API</li><li>Implement your frontend with a mix of HTML + CSS + JS</li><li>Add a couple of features: file uploads, document samples, or webcam captures</li><li>That’s it; you’ve built an identity form autofiller</li></ul><p>Here is a sample web app in action:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*yGnCNNuPUTYAvEBjRW40Xw.gif" /></figure><p>Here is the processing of a French national ID, dropping images from the client:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/676/1*r3x3nkr3BJuuW3zxmUITlw.gif" /></figure><blockquote>Note: For documents with multiple pages, you can use a PDF or TIFF container. In this example, the two uploaded PNG images are merged by the backend and processed as a TIFF file.</blockquote><p>And this is the processing of a US driver license, captured with a laptop 720p webcam:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/546/1*_bnJS9-LyllaW1Qb6JxCEw.gif" /></figure><p>Notes:</p><ul><li>Did you notice that the webcam capture is skewed and the detected portrait image straight? That’s because Document AI automatically deskews the input at the page level. Documents can even be upside down.</li><li>Some fields (such as the dates) are returned with their normalized values. This can make storing and processing these values a lot easier — and less error-prone — for developers.</li></ul><p>The source code for this demo is available in our <a href="https://github.com/GoogleCloudPlatform/document-ai-samples/tree/main/community/identity-form-autofiller-python">Document AI sample repository</a>.</p><h3>More</h3><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-extends-document-ai-with-new-parsers-for-identity">Check out the official announcement</a></li><li><a href="https://cloud.google.com/document-ai/docs/drag-and-drop">Try Document AI in your browser</a></li><li><a href="https://cloud.google.com/document-ai/docs">Document AI documentation</a></li><li><a href="https://cloud.google.com/document-ai/docs/how-to">Document AI how-to guides</a></li><li><a href="https://cloud.google.com/document-ai/docs/send-request">Sending a processing request</a></li><li><a href="https://cloud.google.com/document-ai/docs/processors-list">Full processor and detail list</a></li><li><a href="https://cloud.google.com/document-ai/docs/release-notes">Release notes</a></li><li><a href="https://codelabs.developers.google.com/codelabs/docai-specialized-processors-python">Codelab — Specialized processors with Document AI</a></li><li><a href="https://github.com/GoogleCloudPlatform/document-ai-samples">Code — Document AI samples</a></li><li>For more cloud content, follow me on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>), and feel free to get in touch with any feedback or questions.</li></ul><p>Stay tuned; the family of Document AI processors keeps growing and growing.</p><p><em>Originally published on the </em><a href="https://cloud.google.com/blog/topics/developers-practitioners/automate-identity-document-processing-document-ai"><em>Google Cloud Blog</em></a><em> on June 1, 2022.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=912e1011e164" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164">Automate identity document processing with Document AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deploy a coloring page generator in minutes with Cloud Run]]></title>
            <link>https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/bff59e59d890</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[image-processing]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 07 Apr 2022 07:56:59 GMT</pubDate>
            <atom:updated>2022-04-07T10:18:20.736Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="Coloring page generated from Hokusai’s painting “The Great Wave off Kanagawa”" src="https://cdn-images-1.medium.com/max/902/1*A0eoGye0p45FcR8oimVR3g.gif" /></figure><h3>👋 Hello</h3><p>Have you ever written a script to transform an image? Did you share the script with others or did you run it on multiple computers? How many times did you need to update the script or the setup instructions? Did you end up making it a service or an online app? If your script is useful, you’ll likely want to make it available to others. Deploying processing services is a recurring need — one that comes with its own set of challenges. Serverless technologies let you solve these challenges easily and efficiently.</p><p>In this post, you’ll see how to…</p><ul><li>Create an image processing service that generates coloring pages</li><li>Make it available online using minimal resources</li></ul><p>…and do it all in less than 200 lines of Python and JavaScript!</p><h3>🛠️ Tools</h3><p>To build and deploy a coloring page generator, you’ll need a few tools:</p><ul><li>A library to process images</li><li>A web application framework</li><li>A web server</li><li>A serverless solution to make the demo available 24/7</li></ul><h3>🧱 Architecture</h3><p>Here is one possible architecture for a coloring page generator using Cloud Run:</p><figure><img alt="Architecture serving a web app with Cloud Run" src="https://cdn-images-1.medium.com/max/1024/1*SecpJoaRTIPrlSsGv_dXKg.png" /></figure><p>And here is the workflow:</p><ul><li>1. The user opens the web app: the browser requests the main page.</li><li>2. Cloud Run serves the app HTML code.</li><li>3. The browser requests the additional needed resources.</li><li>4. Cloud Run serves the CSS, JavaScript, and other resources.</li><li>A. The user selects an image and the frontend sends the image to the /api/coloring-page endpoint.</li><li>B. The backend processes the input image and returns an output image, which the user can then visualize, download, or print via the browser.</li></ul><h3>🐍 Software stack</h3><p>Of course, there are many different software stacks that you could use to implement such an architecture.</p><p>Here is a good one based on Python:</p><figure><img alt="schema" src="https://cdn-images-1.medium.com/max/1024/1*kbwf9VRqKl_9pRO9x9xG-w.png" /></figure><p>It includes:</p><ul><li><a href="https://pypi.org/project/gunicorn">Gunicorn</a>: A production-grade WSGI HTTP server</li><li><a href="https://pypi.org/project/Flask">Flask</a>: A popular web app framework</li><li><a href="https://pypi.org/project/scikit-image">scikit-image</a>: An extensive image processing library</li></ul><p>Define these app dependencies in a file named requirements.txt:</p><figure><img alt="Code snippet with highlighted code. For the text version, please see the GitHub repo: https://github.com/PicardParis/cherry-on-py/tree/main/cr_image_processing" src="https://cdn-images-1.medium.com/max/659/1*p2_rOHvlbr3kOBJhviONYA.png" /></figure><h3>🎨 Image processing</h3><p>How do you remove colors from an image? One way is by detecting the object edges and removing everything but the edges in the result image. This can be done with a <a href="https://wikipedia.org/wiki/Sobel_operator">Sobel</a> filter, a convolution filter that detects the regions in which the image intensity changes the most.</p><p>Create a Python file named main.py, define an image processing function, and within it use the Sobel filter and other functions from scikit-image:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*sC8g_4Sbe4JvkE3-IuoI2w.png" /></figure><blockquote><em>Note: The NumPy and Pillow libraries are automatically installed as dependencies of scikit-image.</em></blockquote><p>As an example, here is how the Cloud Run logo is processed at each step:</p><figure><img alt="Colored input transformed into edge-detected grayscale output" src="https://cdn-images-1.medium.com/max/1024/1*je2Em2eSHrVwCKcWqq3T3w.png" /></figure><h3>✨ Web app</h3><h4>Backend</h4><p>To expose both endpoints ( GET / and POST /api/coloring-page), add Flask routes in main.py:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*HCEIvUafupEEKeg4tXtCdA.png" /></figure><h4>Frontend</h4><p>On the browser side, write a JavaScript function that calls the /api/coloring-page endpoint and receives the processed image:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*fp8qLIYKqTl1ANE3n-6ueQ.png" /></figure><p>The base of your app is there. Now you just need to add a mix of HTML + CSS + JS to complete the desired user experience.</p><h4>Local development</h4><p>To develop and test the app on your computer, once your environment is set up, make sure you have the needed dependencies:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*gW5jSS_m_DdrJ8vrAbYDug.png" /></figure><p>Add the following block to main.py. It will only execute when you run your app manually:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*KOVCp7A63RQja-oz2TNkyQ.png" /></figure><p>Run your app:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*ab-plcqk4A_u4LzN_CS7Qg.png" /></figure><p>Flask starts a local web server:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*E04hdGqMC3AO-66BzG5fyQ.png" /></figure><blockquote><em>Note: In this mode, you’re using a development web server (one that is not suited for production). You’ll next set up the deployment to serve your app with Gunicorn, a production-grade server.</em></blockquote><p>You’re all set. Open localhost:8080 in your browser, test, refine, and iterate.</p><h3>🚀 Deployment</h3><p>Once your app is ready for prime time, you can define how it will be served with this single line in a file named Procfile:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*b-UqCiIfH1xb4O5ycx7M-Q.png" /></figure><p>At this stage, here are the files found in a typical project:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*7S3XGplsZn7aCAaHu-Jdqg.png" /></figure><p>That’s it, you can now deploy your app from the source folder:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*-rElij1wP6lT0PtoNj928g.png" /></figure><h3>⚙️ Under the hood</h3><p>The command line output details all the different steps:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*YzsvbclKXnk2KFzwC7wNBg.png" /></figure><p>Cloud Build is indirectly called to containerize your app. One of its core components is Google Cloud <a href="https://github.com/GoogleCloudPlatform/buildpacks">Buildpacks</a>, which automatically builds a production-ready container image from your source code. Here are the main steps:</p><ul><li>Cloud Build fetches the source code.</li><li>Buildpacks autodetects the app language (Python, in this case) and uses the corresponding secure base image.</li><li>Buildpacks installs the app dependencies (defined in requirements.txt for Python).</li><li>Buildpacks configures the service entrypoint (defined in Procfile for Python).</li><li>Cloud Build pushes the container image to <a href="https://cloud.google.com/artifact-registry">Artifact Registry</a>.</li><li>Cloud Run creates a new revision of the service based on this container image.</li><li>Cloud Run routes production traffic to it.</li></ul><blockquote>Notes:<br>- Buildpacks currently supports the following runtimes: Go, Java, .NET, Node.js, and Python.<br>- The base image is actively maintained by Google, scanned for security vulnerabilities, and patched against known issues. This means that, when you deploy an update, your service is based on an image that is as secure as possible.<br>- If you need to build your own container image, for example with a custom runtime, you can add your own Dockerfile and Buildpacks will use it instead.</blockquote><h3>💫 Updates</h3><p>More testing from real-life users shows some issues.</p><p>First, the app does not handle pictures taken with digital cameras in non-native orientations. You can fix this using the EXIF orientation data:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*uynIAotvDG0qoOeBOTl_QA.png" /></figure><p>In addition, the app is too sensitive to details in the input image. Textures in paintings, or noise in pictures, can generate many edges in the processed image. You can improve the processing algorithm by adding a denoising step upfront:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*cH68EUUcrEwgqbbrxC6gIg.png" /></figure><p>This additional step makes the coloring page cleaner and reduces the quantity of ink used if you print it:</p><figure><img alt="La nascita di Venere by Botticelli, with and without denoising" src="https://cdn-images-1.medium.com/max/1024/1*sUrm87Gbc6JXq-ESvdjvXw.png" /></figure><p>Redeploy, and the app is automatically updated:</p><figure><img alt="code snippet" src="https://cdn-images-1.medium.com/max/659/1*nZubLUqea4Tr6R9DAbwm5w.png" /></figure><h3>🎉 It’s alive</h3><p>The app is visible as a service in Cloud Run:</p><figure><img alt="screenshot" src="https://cdn-images-1.medium.com/max/1024/1*SDcZzkB1a5ZJFeGQT-H__w.png" /></figure><p>The service dashboard gives you an overview of app usage:</p><figure><img alt="screenshot" src="https://cdn-images-1.medium.com/max/1024/1*7kwCH5smK9AjLN9MmnNK7w.png" /></figure><p>That’s it; your image processing app is in production!</p><figure><img alt="Animated Demo" src="https://cdn-images-1.medium.com/max/1024/1*FR6K-3gmfmoPH-JlrhD7vA.gif" /></figure><h3>🤯 It’s serverless</h3><p>There are many benefits to using Cloud Run in this architecture:</p><ul><li>Your app is available 24/7.</li><li>The environment is fully managed: you can focus on your code and not worry about the infrastructure.</li><li>Your app is automatically available through HTTPS.</li><li>You can map your app to a custom domain.</li><li>Cloud Run scales the number of instances automatically and the billing includes only the resources used when your code runs.</li><li>If your app is not used, Cloud Run scales down to zero.</li><li>If your app gets more traffic (imagine it makes the news), Cloud Run scales up to the number of instances needed.</li><li>You can control performance and cost by fine-tuning many settings: CPU, memory, concurrency, minimum instances, maximum instances, and more.</li><li>Every month, the <a href="https://cloud.google.com/run/pricing">free tier</a> offers the first 50 vCPU-hours, 100 GiB-hours, and 2 million requests for no cost.</li></ul><h3>💾 Source code</h3><p>The project includes just seven files and less than 200 lines of Python + JavaScript code.</p><p>You can reuse this demo as a base to build your own image processing app:</p><ul><li>Check out the source code on <a href="https://github.com/PicardParis/cherry-on-py/tree/main/cr_image_processing">GitHub</a>.</li><li>For step-by-step instructions on deploying the app yourself in a few minutes, see <a href="https://github.com/PicardParis/cherry-on-py/blob/main/cr_image_processing/DEPLOY.md">“Deploying from scratch”</a>.</li></ul><h3>🖖 More</h3><ul><li><a href="https://coloring-page.lolo.dev/">Try the demo</a> and generate your own coloring pages.</li><li><a href="https://cloud.google.com/run/docs">Learn more</a> about Cloud Run.</li><li>For more cloud content, follow me on Twitter (<a href="https://twitter.com/PicardParis">@PicardParis</a>) or LinkedIn (<a href="https://linkedin.com/in/PicardParis">in/PicardParis</a>), and feel free to get in touch with any feedback or questions.</li></ul><h3>📜 Also in this series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking video objects</a></li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li>Processing images</li></ol><p><em>Originally published on the </em><a href="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run"><em>Google Cloud Blog</em></a><em> on April 5, 2022.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bff59e59d890" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890">Deploy a coloring page generator in minutes with Cloud Run</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Face detection and processing in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/38dc51a115d4</guid>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[technology]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Tue, 29 Sep 2020 14:19:14 GMT</pubDate>
            <atom:updated>2022-04-07T10:18:08.300Z</atom:updated>
            <content:encoded><![CDATA[<h3>⏳ 2021–10–08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>👋 Hello!</h3><p>In this article, you’ll see the following:</p><ul><li>how to detect faces in pictures,</li><li>how to automatically anonymize, crop,… a picture with faces,</li><li>how to make this a serverless online demo,</li><li>in less than 300 lines of Python code.</li></ul><p>Here is a famous face that has been automatically anonymized and cropped.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/329/1*EhOakiFNwIzh6ES549zEQw.png" /><figcaption>Do you guess who this is?</figcaption></figure><blockquote>Note: We’re talking about face detection, not face recognition. Though technically possible, face recognition can have harmful applications. Responsible companies have established AI principles and avoid exposing such potentially harmful technologies (e.g. <a href="https://ai.google/principles">Google AI Principles</a>).</blockquote><h3>🛠️ Tools</h3><p>A few tools will do:</p><ul><li>a machine learning model to analyze images,</li><li>a library to process images,</li><li>a web application framework,</li><li>a serverless solution to keep the demo available 24/7 and at minimal cost.</li></ul><h3>🧱 Architecture</h3><p>Here is an architecture using 2 Google Cloud services (<a href="https://cloud.google.com/appengine/docs">App Engine</a> + <a href="https://cloud.google.com/vision/docs">Vision API</a>):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5ms7LOeFClA5bU9BtpUMMw.png" /></figure><p>The workflow is the following:</p><ol><li>Open the demo: App Engine serves the home page.</li><li>Take a selfie: the frontend sends it to the /analyze-image endpoint.</li><li>The backend sends a request to the Vision API: the image is analyzed and the results (annotations) are returned.</li><li>The backend returns the annotations, in addition to the number of detected faces (to display the info directly in the web page).</li><li>The frontend sends image, annotations, and processing options to the /process-image endpoint.</li><li>The backend processes the image with the given options and returns the result image.</li><li>Change the options: steps 5 and 6 are repeated.</li><li>Get the image with new options.</li></ol><p>This is one of many possible architectures. The advantages of this one are the following:</p><ul><li>The web browser caches both the selfie and the annotations: no storage is involved and no private images are stored anywhere in the cloud.</li><li>The Vision API is only called once per image.</li></ul><h3>🐍 Python libraries</h3><h4>Google Cloud Vision</h4><ul><li>The client library that wraps calls to the Vision API.</li><li><a href="https://pypi.org/project/google-cloud-vision">https://pypi.org/project/google-cloud-vision</a></li></ul><h4>Pillow</h4><ul><li>A very popular imaging library, both extensive and easy to use.</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h4>Flask</h4><ul><li>One of the most popular web app frameworks.</li><li><a href="https://pypi.org/project/Flask">https://pypi.org/project/Flask</a></li></ul><h4>Dependencies</h4><p>Define the dependencies in the requirements.txt file:</p><pre><strong>google-cloud-vision</strong>==1.0.0</pre><pre><strong>Pillow</strong>==7.2.0</pre><pre><strong>Flask</strong>==1.1.2</pre><blockquote>Notes:<br>- As a best practice, also specify the dependency versions. This freezes your production environment in a known state and prevents newer versions from potentially breaking future deployments.<br>- App Engine will automatically deploy these dependencies.</blockquote><h3>🧠 Image analysis</h3><h4>Vision API</h4><p>The Vision API gives access to state-of-the-art machine learning models for image analysis. One of the multiple features is face detection. Here is a way to detect faces in an image:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/href">https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/href</a></iframe><h4>Backend endpoint</h4><p>Exposing an API endpoint with Flask consists in wrapping a function with a route. Here is a possible POST endpoint:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/href">https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/href</a></iframe><h4>Frontend request</h4><p>Here is a javascript function to call the API from the frontend:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/80380594bf727837b452b5a8d62a8242/href">https://medium.com/media/80380594bf727837b452b5a8d62a8242/href</a></iframe><h3>🎨 Image processing</h3><h4>Face bounding box and landmarks</h4><p>The Vision API provides the bounding box of the detected faces and the position of 30+ face landmarks (mouth, nose, eyes,…). Here is a way to visualize them with Pillow (PIL):</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9875815330881f6d59e489872744d4ab/href">https://medium.com/media/9875815330881f6d59e489872744d4ab/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*uMuj-PdEoeejIfrCh7zAMg.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h4>Face anonymization</h4><p>Here is way to anonymize the faces thanks to the bounding boxes:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ed328faf40629175a2bd873aeda793a/href">https://medium.com/media/8ed328faf40629175a2bd873aeda793a/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*bD7vuRmQdJ-Vffi57rYiqA.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h4>Face cropping</h4><p>Similarly, to focus on the detected faces, you can crop everything around the faces:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/href">https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/502/1*Iup-0wvY_vI10_EZ8dVG_w.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><h3>🍒 Cherry on Py 🐍</h3><p>Now, the icing on the cake (or the “cherry on the pie” as we say in French):</p><ul><li>Having independent rendering functions lets you combine multiple options at once.</li><li>Knowing the bounding box for all faces allows cropping the image to the minimal bounding box.</li><li>Using the location of the nose and the mouth, you can add a moustache to everyone.</li><li>If your functions have parameters to render a single frame, you can generate animations with a few lines of code.</li><li>Once your Flask app works locally, you can deploy and keep it available 24/7 at minimal cost.</li></ul><p>Here is what’s detected on famous photorealistic paintings:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/438/1*X4hw8gESYNENqBAtlmJB3w.png" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/310/1*A7BWyLQvJohvCMzJw7otug.png" /><figcaption>Girl with a Pearl Earring (<a href="https://commons.wikimedia.org/wiki/File:Meisje_met_de_parel.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/461/1*qyMK3DDzT68qLSF5y_shRg.png" /><figcaption>Shakespeare (<a href="https://commons.wikimedia.org/wiki/File:Sanders_portrait2.jpg">Wikimedia</a>)</figcaption></figure><p>Here are some animated versions:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*r7OUErW_bKnlWD0p8iYKhw.gif" /><figcaption>American Gothic (<a href="https://commons.wikimedia.org/wiki/File:Grant_DeVolson_Wood_-_American_Gothic.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*7yqwaRfOK-EB23NrUz9kwQ.gif" /><figcaption>Girl with a Pearl Earring (<a href="https://commons.wikimedia.org/wiki/File:Meisje_met_de_parel.jpg">Wikimedia</a>)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*T762aj2zjz9onKAN1Gld-A.gif" /><figcaption>Shakespeare (<a href="https://commons.wikimedia.org/wiki/File:Sanders_portrait2.jpg">Wikimedia</a>)</figcaption></figure><blockquote>Note: animations are a bit degraded (GIF version) as Medium does not support animated PNGs. The demo below lets you generate them in GIF, PNG, and WebP.</blockquote><p>And, of course, this works even better on real pictures:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*rDrfkj4apSdEkwI0smp3-w.png" /><figcaption>Personal pictures (aged from 2 to 44)</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*EVyxahDBrJyxwyf3N54QSQ.gif" /><figcaption>Yes, I’ve had a moustache for over 42 years, and my sister too ;)</figcaption></figure><p>And, finally, here is our famous anonymous from the beginning:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/329/1*VjKW1nSa0yMxKqlNLGR03w.gif" /><figcaption>Mona Lisa (<a href="https://commons.wikimedia.org/wiki/File:Mona_Lisa-LF-restoration-v2.jpg">Wikimedia</a>)</figcaption></figure><h3>🚀 Source code and deployment</h3><h4>Source code</h4><ul><li>The Python code for the backend takes less than 300 lines of code.</li><li>See the <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection">source on GitHub</a>.</li></ul><h4>Deployment</h4><ul><li>You can deploy this demo in 4 minutes.</li><li>See “<a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection/DEPLOY.md">Deploying from scratch</a>”.</li></ul><h3>🎉 Online demo</h3><p>Try the demo by yourself:<br>➡️ <a href="https://face-detection.lolo.dev/">https://face-detection.lolo.dev</a> ⬅️</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/333/1*sXW8u2q5bpcS_S2nb3a72w.gif" /><figcaption><a href="https://face-detection.lolo.dev/">https://face-detection.lolo.dev</a></figcaption></figure><h3>🖖 See you</h3><p><a href="https://bit.ly/feedback-face-detection">Feedback, questions</a>? I’d love to read from you! <a href="https://twitter.com/PicardParis">Follow me on Twitter</a> for more…</p><h3>⏳ Updates</h3><ul><li><strong>2021–10–08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gae_face_detection">GitHub version</a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>📜 Also in this series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking video objects</a></li><li>Face detection and processing</li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=38dc51a115d4" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4">Face detection and processing in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tracking video objects in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/18eb4227df34</guid>
            <category><![CDATA[python]]></category>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Thu, 25 Jun 2020 17:46:01 GMT</pubDate>
            <atom:updated>2022-04-07T10:17:26.867Z</atom:updated>
            <content:encoded><![CDATA[<h3>⏳ 2021–10–08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>👋 Hello!</h3><p>In this article, you’ll see the following:</p><ul><li>how to track objects present in a video,</li><li>with an automated processing pipeline,</li><li>in less than 300 lines of Python code.</li></ul><p>Here is an example of an auto-generated object summary for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/animals.mp4">animals.mp4</a>&gt;:</p><figure><img alt="Tracked object summary for animals.mp4" src="https://cdn-images-1.medium.com/max/1024/0*U73HEF_TUvYuP1Ne.jpeg" /></figure><h3>🛠️ Tools</h3><p>A few tools will do:</p><ul><li>Storage space for videos and results</li><li>A serverless solution to run the code</li><li>A machine learning model to analyze videos</li><li>A library to extract frames from videos</li><li>A library to render the objects</li></ul><h3>🧱 Architecture</h3><p>Here is a possible architecture using 3 Google Cloud services (<a href="https://cloud.google.com/storage/docs">Cloud Storage</a>, <a href="https://cloud.google.com/functions/docs">Cloud Functions</a>, and the <a href="https://cloud.google.com/video-intelligence/docs">Video Intelligence API</a>):</p><figure><img alt="Architecture" src="https://cdn-images-1.medium.com/max/1024/0*-mUux73HcpRY0Uue.png" /></figure><p>The processing pipeline follows these steps:</p><ol><li>You upload a video</li><li>The upload event automatically triggers the tracking function</li><li>The function sends a request to the Video Intelligence API</li><li>The Video Intelligence API analyzes the video and uploads the results (annotations)</li><li>The upload event triggers the rendering function</li><li>The function downloads both annotation and video files</li><li>The function renders and uploads the objects</li><li>You know which objects are present in your video!</li></ol><h3>🐍 Python libraries</h3><h4>Video Intelligence API</h4><ul><li>To analyze videos</li><li><a href="https://pypi.org/project/google-cloud-videointelligence">https://pypi.org/project/google-cloud-videointelligence</a></li></ul><h4>Cloud Storage</h4><ul><li>To manage downloads and uploads</li><li><a href="https://pypi.org/project/google-cloud-storage">https://pypi.org/project/google-cloud-storage</a></li></ul><h4>OpenCV</h4><ul><li>To extract video frames</li><li>OpenCV offers a headless version (without GUI features, ideal for a service)</li><li><a href="https://pypi.org/project/opencv-python-headless">https://pypi.org/project/opencv-python-headless</a></li></ul><h4>Pillow</h4><ul><li>To render and annotate object images</li><li>Pillow is a very popular imaging library, both extensive and easy to use</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h3>🧠 Video analysis</h3><h4>Video Intelligence API</h4><p>The Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of its multiple features is detecting and tracking objects. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the OBJECT_TRACKING feature:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/href">https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/href</a></iframe><h4>Cloud Function entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/href">https://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/href</a></iframe><blockquote>Notes:<br>• This function will be called when a video is uploaded to the bucket defined as a trigger.<br>• Using an environment variable makes the code more portable and lets you deploy the exact same code with different trigger and output buckets.</blockquote><h3>🎨 Object rendering</h3><h4>Code structure</h4><p>It’s interesting to split the code into 2 main classes:</p><ul><li>StorageHelper for managing local files and cloud storage objects</li><li>VideoProcessor for all graphical processings</li></ul><p>Here is a possible core function for the 2nd Cloud Function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/href">https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/href</a></iframe><h4>Cloud Function entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/43f8e86165e95433fed65f48610591b0/href">https://medium.com/media/43f8e86165e95433fed65f48610591b0/href</a></iframe><blockquote>Note: This function will be called when an annotation file is uploaded to the bucket defined as a trigger.</blockquote><h4>Frame rendering</h4><p>OpenCV and Pillow easily let you extract video frames and compose over them:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/href">https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/href</a></iframe><blockquote>Note: It would probably be possible to only use OpenCV but I found it more productive developing with Pillow (code is more readable and intuitive).</blockquote><h3>🔎 Results</h3><p>Here are the main objects found in the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">JaneGoodall.mp4</a>&gt;:</p><figure><img alt="Tracked object summary for JaneGoodall.mp4" src="https://cdn-images-1.medium.com/max/1024/0*v21gdT8bxvr_C_gP.jpeg" /></figure><blockquote>Notes:<br>• The machine learning model has correctly identified different wildlife species: those are “true positives”. It has also incorrectly identified our planet as “packaged goods”: this is a “false positive”. Machine learning models keep learning by being trained with new samples so, with time, their precision keeps increasing (resulting in less false positives).<br>• The current code filters out objects detected with a confidence below 70% or with less than 10 frames. Lower the thresholds to get more results.</blockquote><h3>🍒 Cherry on Py 🐍</h3><p>Now, the icing on the cake (or the “cherry on the pie” as we say in French), you can enrich the architecture to add new possibilities:</p><ul><li>Trigger the processing for videos from any bucket (including external public buckets)</li><li>Generate individual object animations (in parallel to object summaries)</li></ul><h4>Architecture (v2)</h4><figure><img alt="Architecture (v2)" src="https://cdn-images-1.medium.com/max/1024/0*kMM7XhzxfkkpdkTR.png" /></figure><ul><li>A — Video object tracking can also be triggered manually with an HTTP GET request</li><li>B — The same rendering code is deployed in 2 sibling functions, differentiated with an environment variable</li><li>C — Object summaries and animations are generated in parallel</li></ul><h4>Cloud Function HTTP entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/href">https://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/href</a></iframe><blockquote>Note: This is the same code as gcf_track_objects() with the video URI parameter specified by the caller through a GET request.</blockquote><h3>🎉 Results</h3><p>Here are some auto-generated trackings for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/animals.mp4">animals.mp4</a>&gt;:</p><ul><li>The left elephant (a big object ;) is detected:</li></ul><figure><img alt="Elephant on the left" src="https://cdn-images-1.medium.com/max/1024/0*uKmLlOt_2qr6_mBA.gif" /></figure><ul><li>The right elephant is perfectly isolated too:</li></ul><figure><img alt="Elephant on the right" src="https://cdn-images-1.medium.com/max/1024/0*fMtzSrFPU6IJ19j3.gif" /></figure><ul><li>The veterinarian is correctly identified:</li></ul><figure><img alt="Person on the left" src="https://cdn-images-1.medium.com/max/1024/0*M-XTcd6nMar2bgKL.gif" /></figure><ul><li>The animal he’s feeding too:</li></ul><figure><img alt="Animal on the right" src="https://cdn-images-1.medium.com/max/1024/0*TMz8h9fIqyLNh6y3.gif" /></figure><p>Moving objects or static objects in moving shots are tracked too, as in &lt;beyond-the-map-rio.mp4&gt;:</p><ul><li>A building in a moving shot:</li></ul><figure><img alt="Shot with buildings 1" src="https://cdn-images-1.medium.com/max/1024/0*X5Z9ORKF_o3ZSItY.gif" /></figure><ul><li>Neighbor buildings are tracked too:</li></ul><figure><img alt="Shot with buildings 2" src="https://cdn-images-1.medium.com/max/1024/0*A1T0UfvHwrokdgWz.gif" /></figure><ul><li>Persons in a moving shot:</li></ul><figure><img alt="Moving persons" src="https://cdn-images-1.medium.com/max/1024/0*YjdIZ6jHfJbhZJ1X.gif" /></figure><ul><li>A surfer crossing the shot:</li></ul><figure><img alt="Surfer" src="https://cdn-images-1.medium.com/max/1024/0*b1da7YkFdsqqf6bV.gif" /></figure><p>Here are some others for the video &lt;<a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">JaneGoodall.mp4</a>&gt;:</p><ul><li>A butterfly (easy?):</li></ul><figure><img alt="Butterfly" src="https://cdn-images-1.medium.com/max/1024/0*GuhCti_2CpVZUn1n.gif" /></figure><ul><li>An insect, in larval stage, climbing a moving twig:</li></ul><figure><img alt="Caterpillar on moving twig" src="https://cdn-images-1.medium.com/max/1024/0*20XPF-jSBu_pS5KO.gif" /></figure><ul><li>An ape in a tree far away (hard?):</li></ul><figure><img alt="Ape catching bugs in tree far away" src="https://cdn-images-1.medium.com/max/1024/0*mabEdHmI0VtkEUSJ.gif" /></figure><ul><li>A monkey jumping from the top of a tree (harder?):</li></ul><figure><img alt="Monkey jumping from tree top" src="https://cdn-images-1.medium.com/max/1024/0*uB8flZZRyxc7g8Cc.gif" /></figure><ul><li>Now, a trap! If we can be fooled, current machine learning state of the art can too:</li></ul><figure><img alt="A flower or maybe not a flower" src="https://cdn-images-1.medium.com/max/1024/0*9KwNRtwax1eV2tg8.gif" /></figure><h3>🚀 Source code and deployment</h3><h3>Source code</h3><ul><li>The Python source is less than 300 lines of code.</li><li>See the <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking">source on GitHub</a>.</li></ul><h3>Deployment</h3><ul><li>You can deploy this architecture in less than 8 minutes.</li><li>See “<a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking/DEPLOY.md">Deploying from scratch</a>”.</li></ul><h3>🖖 See you</h3><p>Do you want more, do you have questions? I’d love to read <a href="https://bit.ly/feedback-video-object-tracking">your feedback</a>. You can also <a href="https://twitter.com/PicardParis">follow me on Twitter</a>.</p><h3>⏳ Updates</h3><ul><li><strong>2021–10–08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_object_tracking">GitHub version</a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>📜 Also in this series</h3><ol><li><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=friends_link&amp;sk=f94ff51885c51dd52539848ce04654ab">Summarizing videos</a></li><li>Tracking video objects</li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=18eb4227df34" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34">Tracking video objects in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Summarizing videos in 300 lines of code]]></title>
            <link>https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=rss-6be63961431c------2</link>
            <guid isPermaLink="false">https://medium.com/p/c2f261c8035c</guid>
            <category><![CDATA[gcp-app-dev]]></category>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[programming]]></category>
            <category><![CDATA[python]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Laurent Picard]]></dc:creator>
            <pubDate>Sat, 30 May 2020 13:54:30 GMT</pubDate>
            <atom:updated>2022-04-07T10:16:05.744Z</atom:updated>
            <content:encoded><![CDATA[<h3>⏳ 2021–10–08 update</h3><ul><li>Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_video_summary"><strong>GitHub version</strong></a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>👋 Hello!</h3><p>Dear developers,</p><p>Do you like the adage <em>“a picture is worth a thousand words”</em>? I do! Let’s check if it also works for <em>“a picture is worth a thousand frames”</em>.</p><p>In this tutorial, you’ll see the following:</p><ul><li>how to understand the content of a video in a blink,</li><li>in less than 300 lines of Python (3.7) code.</li></ul><figure><img alt="Video summary example" src="https://cdn-images-1.medium.com/max/1024/0*sBJJr_f1TVd9Ocwl.jpeg" /><figcaption>A visual summary generated from a 2&#39;42&quot; video made of 35 sequences (shots). The summary is a grid where each cell is a frame representing a video shot.</figcaption></figure><h3>🔭 Objectives</h3><p>This tutorial has 2 objectives, 1 practical and 1 technical:</p><ol><li>Automatically generate visual summaries of videos</li><li>Build a processing pipeline with these properties:</li></ol><ul><li>managed (always ready and easy to set up)</li><li>scalable (able to ingest several videos in parallel)</li><li>not costing anything when not used</li></ul><h3>🛠️ Tools</h3><p>A few tools are enough:</p><ul><li>Storage space for videos and results</li><li>A serverless solution to run the code</li><li>A machine learning model to analyze videos</li><li>A library to extract frames from videos</li><li>A library to generate the visual summaries</li></ul><h3>🧱 Architecture</h3><p>Here is a possible architecture using 3 Google Cloud services (<a href="https://cloud.google.com/storage/docs">Cloud Storage</a>, <a href="https://cloud.google.com/functions/docs">Cloud Functions</a>, and <a href="https://cloud.google.com/video-intelligence/docs">Video Intelligence API</a>):</p><figure><img alt="Architecture" src="https://cdn-images-1.medium.com/max/1024/0*WSqvJEpDo9DVkCua.png" /></figure><p>The processing pipeline follows these steps:</p><ol><li>You upload a video to the 1st bucket (a bucket is a storage space in the cloud)</li><li>The upload event automatically triggers the 1st function</li><li>The function sends a request to the Video Intelligence API to detect the shots</li><li>The Video Intelligence API analyzes the video and uploads the results (annotations) to the 2nd bucket</li><li>The upload event triggers the 2nd function</li><li>The function downloads both annotation and video files</li><li>The function renders and uploads the summary to the 3rd bucket</li><li>The video summary is ready!</li></ol><h3>🐍 Python libraries</h3><p>Open source client libraries let you interface with Google Cloud services in idiomatic Python. You’ll use the following:</p><p>Cloud Storage</p><ul><li>To manage downloads and uploads</li><li><a href="https://pypi.org/project/google-cloud-storage">https://pypi.org/project/google-cloud-storage</a></li></ul><p>Video Intelligence API</p><ul><li>To analyze videos</li><li><a href="https://pypi.org/project/google-cloud-videointelligence">https://pypi.org/project/google-cloud-videointelligence</a></li></ul><p>Here is a choice of 2 additional Python libraries for the graphical needs:</p><p>OpenCV</p><ul><li>To extract video frames</li><li>There’s even a headless version (without GUI features), which is ideal for a service</li><li><a href="https://pypi.org/project/opencv-python-headless">https://pypi.org/project/opencv-python-headless</a></li></ul><p>Pillow</p><ul><li>To generate the visual summaries</li><li>Pillow is a very popular imaging library, both extensive and easy to use</li><li><a href="https://pypi.org/project/Pillow">https://pypi.org/project/Pillow</a></li></ul><h3>⚙️ Project setup</h3><p>Assuming you have a Google Cloud account, you can set up the architecture from Cloud Shell with the gcloud and gsutil commands. This lets you script everything from scratch in a reproducible way.</p><h4>Environment variables</h4><pre># Project<br><strong>PROJECT_NAME=&quot;Visual Summary&quot;<br>PROJECT_ID=&quot;visual-summary-REPLACE_WITH_UNIQUE_SUFFIX&quot;<br></strong># Cloud Storage region (https://cloud.google.com/storage/docs/locations)<br><strong>GCS_REGION=&quot;europe-west1&quot;<br></strong># Cloud Functions region (https://cloud.google.com/functions/docs/locations)<br><strong>GCF_REGION=&quot;europe-west1&quot;<br></strong># Source<br><strong>GIT_REPO=&quot;cherry-on-py&quot;<br>PROJECT_SRC=~/$PROJECT_ID/$GIT_REPO/gcf_video_summary<br><br></strong># Cloud Storage buckets (environment variables)<br><strong>export VIDEO_BUCKET=&quot;b1-videos_${PROJECT_ID}&quot;<br>export ANNOTATION_BUCKET=&quot;b2-annotations_${PROJECT_ID}&quot;<br>export SUMMARY_BUCKET=&quot;b3-summaries_${PROJECT_ID}&quot;</strong></pre><blockquote>Note: You can use your GitHub username as a unique suffix.</blockquote><h4>New project</h4><pre><strong>gcloud projects create $PROJECT_ID \<br>  --name=&quot;$PROJECT_NAME&quot; \<br>  --set-as-default</strong></pre><pre>Create in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/PROJECT_ID].<br>Waiting for [operations/cp...] to finish...done.<br>Enabling service [cloudapis.googleapis.com] on project [PROJECT_ID]...<br>Operation &quot;operations/acf...&quot; finished successfully.<br>Updated property [core/project] to [PROJECT_ID].</pre><h4>Billing account</h4><pre># Link project with billing account (single account)<br><strong>BILLING_ACCOUNT=$(gcloud beta billing accounts list \<br>    --format &#39;value(name)&#39;)</strong><br># Link project with billing account (specific one among multiple accounts)<br>BILLING_ACCOUNT=$(gcloud beta billing accounts list  \<br>    --format &#39;value(name)&#39; \<br>    --filter &quot;displayName=&#39;My Billing Account&#39;&quot;)<br><br><strong>gcloud beta billing projects link $PROJECT_ID --billing-account $BILLING_ACCOUNT</strong></pre><pre>billingAccountName: billingAccounts/XXXXXX-YYYYYY-ZZZZZZ<br>billingEnabled: true<br>name: projects/PROJECT_ID/billingInfo<br>projectId: PROJECT_ID</pre><h4>Buckets</h4><pre># Create buckets with uniform bucket-level access<br><strong>gsutil mb -b on -c regional -l $GCS_REGION gs://$VIDEO_BUCKET<br>gsutil mb -b on -c regional -l $GCS_REGION gs://$ANNOTATION_BUCKET<br>gsutil mb -b on -c regional -l $GCS_REGION gs://$SUMMARY_BUCKET</strong></pre><pre>Creating gs://VIDEO_BUCKET/...<br>Creating gs://ANNOTATION_BUCKET/...<br>Creating gs://SUMMARY_BUCKET/...</pre><p>You can check how it looks like in the <a href="https://console.cloud.google.com/storage/browser">Cloud Console</a>:</p><figure><img alt="Cloud Storage buckets" src="https://cdn-images-1.medium.com/max/1024/0*X1EtcCS5kT9NC0mX.png" /></figure><h4>Service account</h4><p>Create a service account. This is for development purposes only (not needed for production). This provides you with credentials to run your code locally.</p><pre><strong>mkdir ~/$PROJECT_ID<br>cd ~/$PROJECT_ID<br><br>SERVICE_ACCOUNT_NAME=&quot;dev-service-account&quot;<br>SERVICE_ACCOUNT=&quot;${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com&quot;<br>gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME<br>gcloud iam service-accounts keys create ~/$PROJECT_ID/key.json --iam-account $SERVICE_ACCOUNT</strong></pre><pre>Created service account [SERVICE_ACCOUNT_NAME].<br>created key [...] of type [json] as [~/PROJECT_ID/key.json] for [SERVICE_ACCOUNT]</pre><p>Set the GOOGLE_APPLICATION_CREDENTIALS environment variable and check that it points to the service account key. When you run the application code in the current shell session, client libraries will use these credentials for authentication. If you open a new shell session, set the variable again.</p><pre><strong>export GOOGLE_APPLICATION_CREDENTIALS=~/$PROJECT_ID/key.json<br>cat $GOOGLE_APPLICATION_CREDENTIALS</strong></pre><pre>{<br>  &quot;type&quot;: &quot;service_account&quot;,<br>  &quot;project_id&quot;: &quot;PROJECT_ID&quot;,<br>  &quot;private_key_id&quot;: &quot;...&quot;,<br>  &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\n...&quot;,<br>  &quot;client_email&quot;: &quot;SERVICE_ACCOUNT&quot;,<br>  ...<br>}</pre><p>Authorize the service account to access the buckets:</p><pre><strong>IAM_BINDING=&quot;serviceAccount:${SERVICE_ACCOUNT}:roles/storage.objectAdmin&quot;<br>gsutil iam ch $IAM_BINDING gs://$VIDEO_BUCKET<br>gsutil iam ch $IAM_BINDING gs://$ANNOTATION_BUCKET<br>gsutil iam ch $IAM_BINDING gs://$SUMMARY_BUCKET</strong></pre><h4>APIs</h4><p>A few APIs are enabled by default:</p><pre><strong>gcloud services list</strong></pre><pre>NAME                              TITLE<br>bigquery.googleapis.com           BigQuery API<br>bigquerystorage.googleapis.com    BigQuery Storage API<br>cloudapis.googleapis.com          Google Cloud APIs<br>clouddebugger.googleapis.com      Cloud Debugger API<br>cloudtrace.googleapis.com         Cloud Trace API<br>datastore.googleapis.com          Cloud Datastore API<br>logging.googleapis.com            Cloud Logging API<br>monitoring.googleapis.com         Cloud Monitoring API<br>servicemanagement.googleapis.com  Service Management API<br>serviceusage.googleapis.com       Service Usage API<br>sql-component.googleapis.com      Cloud SQL<br>storage-api.googleapis.com        Google Cloud Storage JSON API<br>storage-component.googleapis.com  Cloud Storage</pre><p>Enable the Video Intelligence and Cloud Functions APIs:</p><pre><strong>gcloud services enable \<br>  videointelligence.googleapis.com \<br>  cloudfunctions.googleapis.com</strong></pre><pre>Operation &quot;operations/acf...&quot; finished successfully.</pre><h4>Source code</h4><p>Retrieve the source code:</p><pre><strong>cd ~/$PROJECT_ID<br>git clone </strong><a href="https://github.com/PicardParis/$GIT_REPO.git"><strong>https://github.com/PicardParis/$GIT_REPO.git</strong></a></pre><pre>Cloning into &#39;GIT_REPO&#39;...<br>...</pre><h3>🧠 Video analysis</h3><h4>Video shot detection</h4><p>The Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of the multiple features is video shot detection. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the SHOT_CHANGE_DETECTION feature:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/href">https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/href</a></iframe><h4>Local development and tests</h4><p>Before deploying the function, you need to develop and test it. Create a Python 3 virtual environment and activate it:</p><pre><strong>cd ~/$PROJECT_ID<br>python3 -m venv venv<br>source venv/bin/activate</strong></pre><p>Install the dependencies:</p><pre><strong>pip install -r $PROJECT_SRC/gcf1_detect_shots/requirements.txt</strong></pre><p>Check the dependencies:</p><pre><strong>pip list</strong></pre><pre>Package                        Version<br>------------------------------ ----------<br>...<br>google-cloud-storage           1.28.1<br>google-cloud-videointelligence 1.14.0<br>...</pre><p>You can use the main scope to test the function in script mode:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a25fa778a61308c2584487ea157c1104/href">https://medium.com/media/a25fa778a61308c2584487ea157c1104/href</a></iframe><blockquote>Note: You have already exported the ANNOTATION_BUCKET environment variable earlier in the shell session; you will also define it later at deployment stage. This makes the code generic and lets you reuse it independently of the output bucket.</blockquote><p>Test the function:</p><pre><strong>VIDEO_PATH=&quot;cloud-samples-data/video</strong><strong>/gbikes_dinosaur.mp4&quot;<br>VIDEO_URI=&quot;gs://$VIDEO_PATH&quot;<br>python $PROJECT_SRC/gcf1_detect_shots/main.py $VIDEO_URI</strong></pre><pre>Launching shot detection for &lt;gs://cloud-samples-data/video/gbikes_dinosaur.mp4&gt;...</pre><blockquote>Note: The test video &lt;gbikes_dinosaur.mp4&gt; is located in an external bucket. This works because the video is publicly accessible.</blockquote><p>Wait a moment and check that the annotations have been generated:</p><pre><strong>gsutil ls -r gs://$ANNOTATION_BUCKET</strong></pre><pre>964  YYYY-MM-DDThh:mm:ssZ  gs://ANNOTATION_BUCKET/VIDEO_PATH.json<br>TOTAL: 1 objects, 964 bytes (964 B)</pre><p>Check the last 200 bytes of the annotation file:</p><pre><strong>gsutil cat -r -200 gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json</strong></pre><pre>}<br>    }, {<br>      &quot;start_time_offset&quot;: {<br>        &quot;seconds&quot;: 28,<br>        &quot;nanos&quot;: 166666000<br>      },<br>      &quot;end_time_offset&quot;: {<br>        &quot;seconds&quot;: 42,<br>        &quot;nanos&quot;: 766666000<br>      }<br>    } ]<br>  } ]<br>}</pre><blockquote>Note: Those are the start and end positions of the last video shot. Everything seems fine.</blockquote><p>Clean up when you’re finished:</p><pre><strong>gsutil rm gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json<br><br>deactivate<br><br>rm -rf venv</strong></pre><h4>Function entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/href">https://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/href</a></iframe><blockquote>Note: This function will be called whenever a video is uploaded to the bucket defined as a trigger.</blockquote><h4>Function deployment</h4><p>Deploy the 1st function:</p><pre><strong>GCF_NAME=&quot;gcf1_detect_shots&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf1_detect_shots&quot;<br>GCF_ENTRY_POINT=&quot;gcf_detect_shots&quot;<br>GCF_TRIGGER_BUCKET=&quot;$VIDEO_BUCKET&quot;<br>GCF_ENV_VARS=&quot;ANNOTATION_BUCKET=$ANNOTATION_BUCKET&quot;<br>GCF_MEMORY=&quot;128MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><blockquote>Note: The default memory allocated for a Cloud Function is 256 MB (possible values are 128MB, 256MB, 512MB, 1024MB, and 2048MB). As the function has no memory or CPU needs (it sends a simple API request), the minimum memory setting is enough.</blockquote><pre>Deploying function (may take a while - up to 2 minutes)...done.<br>availableMemoryMb: 128<br>entryPoint: gcf_detect_shots<br>environmentVariables:<br>  ANNOTATION_BUCKET: b2-annotations...<br>eventTrigger:<br>  eventType: google.storage.object.finalize<br>...<br>status: ACTIVE<br>timeout: 60s<br>updateTime: &#39;YYYY-MM-DDThh:mm:ss.mmmZ&#39;<br>versionId: &#39;1&#39;</pre><blockquote>Note: The ANNOTATION_BUCKET environment variable is defined with the --update-env-vars flag. Using an environment variable lets you deploy the exact same code with different trigger and output buckets.</blockquote><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">Cloud Console</a>:</p><figure><img alt="Cloud Functions" src="https://cdn-images-1.medium.com/max/1024/0*IPQ5AixblUiLf8G_.png" /></figure><h4>Production tests</h4><p>Make sure to test the function in production. Copy a video into the video bucket:</p><pre><strong>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>SRC_URI=&quot;gs://cloud-samples-data/video</strong><strong>/$VIDEO_NAME&quot;<br>DST_URI=&quot;gs://$VIDEO_BUCKET/$VIDEO_NAME&quot;<br><br>gsutil cp $SRC_URI $DST_URI</strong></pre><pre>Copying gs://cloud-samples-data/video/gbikes_dinosaur.mp4 [Content-Type=video/mp4]...<br>- [1 files][ 62.0 MiB/ 62.0 MiB]<br>Operation completed over 1 objects/62.0 MiB.</pre><p>Query the logs to check that the function has been triggered:</p><pre><strong>gcloud functions logs read --region $GCF_REGION</strong></pre><pre>LEVEL  NAME               EXECUTION_ID  TIME_UTC  LOG<br>D      gcf1_detect_shots  ...           ...       Function execution started<br>I      gcf1_detect_shots  ...           ...       Launching shot detection for &lt;gs://VIDEO_BUCKET/VIDEO_NAME&gt;...<br>D      gcf1_detect_shots  ...           ...       Function execution took 874 ms, finished with status: &#39;ok&#39;</pre><p>Wait a moment and check the annotation bucket:</p><pre><strong>gsutil ls -r gs://$ANNOTATION_BUCKET</strong></pre><p>You should see the annotation file:</p><pre>gs://ANNOTATION_BUCKET/VIDEO_BUCKET/:<br>gs://ANNOTATION_BUCKET/VIDEO_BUCKET/VIDEO_NAME.json</pre><p>The 1st function is operational!</p><h3>🎞️ Visual Summary</h3><h4>Code structure</h4><p>It’s interesting to split the code into 2 main classes:</p><ul><li>StorageHelper for local file and cloud storage object management</li><li>VideoProcessor for graphical processings</li></ul><p>Here is a possible core function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a93998518c177c34a6c86d85593fdb53/href">https://medium.com/media/a93998518c177c34a6c86d85593fdb53/href</a></iframe><blockquote>Note: If exceptions are raised, it’s handy to log them with logging.exception() to get a stack trace in production logs.</blockquote><h4>Class StorageHelper</h4><p>The class manages the following:</p><ul><li>The retrieval and parsing of video shot annotations</li><li>The download of source videos</li><li>The upload of generated visual summaries</li><li>File names</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c45a6726678a903e043c6329077814d5/href">https://medium.com/media/c45a6726678a903e043c6329077814d5/href</a></iframe><p>The source video is handled in the with statement context manager:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/href">https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/href</a></iframe><blockquote>Note: Once downloaded, the video uses memory space in the /tmp RAM disk (the only writable space for the serverless function). It&#39;s best to delete temporary files when they&#39;re not needed anymore, to avoid potential out-of-memory errors on future invocations of the function.</blockquote><p>Annotations are retrieved with the methods storage.Blob.download_as_string() and json.loads():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/href">https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/href</a></iframe><p>The parsing is handled with this VideoShot helper class:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/href">https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/href</a></iframe><p>Video shot info can be exposed with a getter and a generator:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/href">https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/href</a></iframe><p>The naming convention was chosen to keep consistent object paths between the different buckets. This also lets you deduce the video path from the annotation URI:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/href">https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/href</a></iframe><p>The video is directly downloaded with storage.Blob.download_to_filename():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/href">https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/href</a></iframe><p>On the opposite, results can be uploaded with storage.Blob.upload_from_string():</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bf5877b770412025188594002e5f61fa/href">https://medium.com/media/bf5877b770412025188594002e5f61fa/href</a></iframe><blockquote>Note: from_string means from_bytes here (Python 2 legacy). Pillow supports working with memory images, which avoids having to manage local files.</blockquote><p>And finally, here is a possible naming convention for the summary files:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/href">https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/href</a></iframe><h4>Class VideoProcessor</h4><p>The class manages the following:</p><ul><li>Video frame extraction</li><li>Visual summary generation</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a729b2940d13ac7e97016a08622b77b1/href">https://medium.com/media/a729b2940d13ac7e97016a08622b77b1/href</a></iframe><p>Opening and closing the video is handled in the with statement context manager:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/966117816753013e7647fc08971001d8/href">https://medium.com/media/966117816753013e7647fc08971001d8/href</a></iframe><p>The video summary is a grid of cells which can be rendered in a single loop with two generators:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/24491fd8f2a86539b574d4babf76100d/href">https://medium.com/media/24491fd8f2a86539b574d4babf76100d/href</a></iframe><blockquote>Note: shot_ratio is set to 0.5 by default to extract video shot middle frames.</blockquote><p>The first generator yields cell images:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/href">https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/href</a></iframe><p>The second generator yields cell positions:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a04df38e6b81926cf63889b8303847f/href">https://medium.com/media/8a04df38e6b81926cf63889b8303847f/href</a></iframe><p>OpenCV easily allows extracting video frames at a given position:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/href">https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/href</a></iframe><p>Choosing the summary grid composition is arbitrary. Here is an example to compose a summary preserving the video proportions:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6c3961789a20e1e78337737448cd5daa/href">https://medium.com/media/6c3961789a20e1e78337737448cd5daa/href</a></iframe><p>Finally, Pillow gives full control on image serializations:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/028535063c6ee49659586b1ad95dd163/href">https://medium.com/media/028535063c6ee49659586b1ad95dd163/href</a></iframe><blockquote>Note: Working with in-memory images avoids managing local files and uses less memory.</blockquote><h4>Local development and tests</h4><p>You can use the main scope to test the function in script mode:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/href">https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/href</a></iframe><p>Test the function:</p><pre><strong>cd ~/$PROJECT_ID<br>python3 -m venv venv<br>source venv/bin/activate<br><br>pip install -r $PROJECT_SRC/gcf2_generate_summary/requirements.txt<br><br>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>ANNOTATION_URI=&quot;gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$VIDEO_NAME.json&quot;<br><br>python $PROJECT_SRC/gcf2_generate_summary/main.py $ANNOTATION_URI</strong></pre><pre>Downloading -&gt; /tmp/SUMMARY_BUCKET/VIDEO_BUCKET/VIDEO_NAME<br>Generating summary...<br>Uploading -&gt; VIDEO_BUCKET/VIDEO_NAME.summary004.jpeg</pre><blockquote>Note: The uploaded video summary shows 4 shots.</blockquote><p>Clean up:</p><pre><strong>deactivate<br>rm -rf venv</strong></pre><h4>Function entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/631923f995b0f8bbd4edda7b6c837359/href">https://medium.com/media/631923f995b0f8bbd4edda7b6c837359/href</a></iframe><blockquote>Note: This function will be called whenever an annotation file is uploaded to the bucket defined as a trigger.</blockquote><h4>Function deployment</h4><pre><strong>GCF_NAME=&quot;gcf2_generate_summary&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf2_generate_summary&quot;<br>GCF_ENTRY_POINT=&quot;gcf_generate_summary&quot;<br>GCF_TRIGGER_BUCKET=&quot;$ANNOTATION_BUCKET&quot;<br>GCF_ENV_VARS=&quot;SUMMARY_BUCKET=$SUMMARY_BUCKET&quot;<br>GCF_TIMEOUT=&quot;540s&quot;<br>GCF_MEMORY=&quot;512MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --timeout $GCF_TIMEOUT \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><p>Notes:</p><ul><li>The default timeout for a Cloud Function is 60 seconds. As you’re deploying a background function with potentially long processings, set it to the maximum value (540 seconds = 9 minutes).</li><li>You also need to bump up the memory a little for the video and image processings. Depending on the size of your videos and the maximum resolution of your output summaries, or if you need to generate the summary faster (memory size and vCPU speed are correlated), you might use a higher value (1024MB or 2048MB).</li></ul><pre>Deploying function (may take a while - up to 2 minutes)...done.<br>availableMemoryMb: 512<br>entryPoint: gcf_generate_summary<br>environmentVariables:<br>  SUMMARY_BUCKET: b3-summaries...<br>...<br>status: ACTIVE<br>timeout: 540s<br>updateTime: &#39;YYYY-MM-DDThh:mm:ss.mmmZ&#39;<br>versionId: &#39;1&#39;</pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">Cloud Console</a>:</p><figure><img alt="Cloud Functions 2" src="https://cdn-images-1.medium.com/max/1024/0*Ci683g0AQEorLvDE.png" /></figure><h4>Production tests</h4><p>Make sure to test the function in production. You can upload an annotation file in the 2nd bucket:</p><pre><strong>VIDEO_NAME=&quot;gbikes_dinosaur.mp4&quot;<br>ANNOTATION_FILE=&quot;$VIDEO_NAME.json&quot;<br>ANNOTATION_URI=&quot;gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$ANNOTATION_FILE&quot;<br>gsutil cp $ANNOTATION_URI .<br>gsutil cp $ANNOTATION_FILE $ANNOTATION_URI<br>rm $ANNOTATION_FILE</strong></pre><blockquote>Note: This reuses the previous local test annotation file and overwrites it. Overwriting a file in a bucket also triggers attached functions.</blockquote><p>Wait a few seconds and query the logs to check that the function has been triggered:</p><pre><strong>gcloud functions logs read --region $GCF_REGION</strong></pre><pre>LEVEL  NAME                   EXECUTION_ID  TIME_UTC  LOG<br>...<br>D      gcf2_generate_summary  ...           ...       Function execution started<br>I      gcf2_generate_summary  ...           ...       Downloading -&gt; /tmp/SUMMARY_BUCKET/VIDEO_BUCKET/VIDEO_NAME<br>I      gcf2_generate_summary  ...           ...       Generating summary...<br>I      gcf2_generate_summary  ...           ...       Uploading -&gt; VIDEO_BUCKET/VIDEO_NAME.summary004.jpeg<br>D      gcf2_generate_summary  ...           ...       Function execution took 11591 ms, finished with status: &#39;ok&#39;</pre><p>The 2nd function is operational and the pipeline is in place! You can now do end-to-end tests by copying new videos in the 1st bucket.</p><h4>Results</h4><p>Download the generated summary on your computer:</p><pre><strong>cd ~/$PROJECT_ID<br>gsutil cp -r gs://$SUMMARY_BUCKET/**.jpeg .<br>cloudshell download *.jpeg</strong></pre><p>Here is the visual summary for gbikes_dinosaur.mp4 (4 detected shots):</p><figure><img alt="Visual summary for gbikes_dinosaur.mp4" src="https://cdn-images-1.medium.com/max/1024/0*5kElus_pB3EIoDxU.jpeg" /></figure><p>You can also directly preview the file from the <a href="https://console.cloud.google.com/storage/browser/">Cloud Console</a>:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*3iMF9kMT7x7xgtWq.png" /></figure><h3>🍒 Cherry on the Py 🐍</h3><p>Now, the icing on the cake (or the “cherry on the pie” as we say in French)…</p><p>Based on the same architecture and code, you can add a few features:</p><ul><li>Trigger the processing for videos from other buckets</li><li>Generate summaries in multiple formats (such as JPEG, PNG, WEBP)</li><li>Generate animated summaries (also in multiple formats, such as GIF, PNG, WEBP)</li></ul><p>Enrich the architecture to duplicate 2 items:</p><ul><li>The video shot detection function, to get it to run as an HTTP endpoint</li><li>The summary generation function to handle animated images</li></ul><p>Adapt the code to support the new features:</p><ul><li>An animated parameter to generate still or animated summaries</li><li>Save and upload the results in multiple formats</li></ul><h4>Architecture (v2)</h4><figure><img alt="Architecture (v2)" src="https://cdn-images-1.medium.com/max/1024/0*VQdimtG1UCuy4KkK.png" /></figure><ul><li>A. Video shot detection can also be triggered manually with an HTTP GET request</li><li>B. Still and animated summaries are generated in 2 functions in parallel</li><li>C. Summaries are uploaded in multiple image formats</li></ul><h4>HTTP entry point</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e99021dde44910cc73b1f9421ecd53de/href">https://medium.com/media/e99021dde44910cc73b1f9421ecd53de/href</a></iframe><blockquote>Note: This is the same code as gcf_detect_shots with the video URI parameter provided from a GET request.</blockquote><h4>Function deployment</h4><pre>GCF_NAME=&quot;gcf1_detect_shots_http&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf1_detect_shots&quot;<br>GCF_ENTRY_POINT=&quot;gcf_detect_shots_http&quot;<br>GCF_TRIGGER_BUCKET=&quot;$VIDEO_BUCKET&quot;<br>GCF_ENV_VARS=&quot;ANNOTATION_BUCKET=$ANNOTATION_BUCKET&quot;<br>GCF_MEMORY=&quot;128MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS \<br>  --trigger-http \<br>  --region $GCF_REGION \<br>  --memory $GCF_MEMORY \<br>  --quiet</pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">Cloud Console</a>:</p><figure><img alt="Cloud Functions 3" src="https://cdn-images-1.medium.com/max/1024/0*4fPdiARXnY-_41-C.png" /></figure><h4>Animation support</h4><p>Add an animated option in the core function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d305f1c3c310735bc0198d0dd7f0e8ad/href">https://medium.com/media/d305f1c3c310735bc0198d0dd7f0e8ad/href</a></iframe><p>Define the formats you’re interested in generating:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/246476ead5b3109bdfac7fb155a97b35/href">https://medium.com/media/246476ead5b3109bdfac7fb155a97b35/href</a></iframe><p>Add support to generate still and animated summaries in different formats:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a879c7b4ec0215fe7b142d8e4aac464f/href">https://medium.com/media/a879c7b4ec0215fe7b142d8e4aac464f/href</a></iframe><p>The serialization can still take place in a single function:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c65fdf651e5db931dc3150e962575c6d/href">https://medium.com/media/c65fdf651e5db931dc3150e962575c6d/href</a></iframe><blockquote>Note: Pillow is both versatile and consistent, allowing for significant and clean code factorization.</blockquote><p>Add an animated optional parameter to the StorageHelper class:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c9ccce745ad8453ff30ff698dd9ea8f5/href">https://medium.com/media/c9ccce745ad8453ff30ff698dd9ea8f5/href</a></iframe><p>And finally, add an ANIMATED optional environment variable in the entry point:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a81644f5baa8058317a7708f2b9c5f29/href">https://medium.com/media/a81644f5baa8058317a7708f2b9c5f29/href</a></iframe><h4>Function deployment</h4><p>Duplicate the 2nd function with the additional ANIMATED environment variable:</p><pre><strong>GCF_NAME=&quot;gcf2_generate_summary_animated&quot;<br>GCF_SOURCE=&quot;$PROJECT_SRC/gcf2_generate_summary&quot;<br>GCF_ENTRY_POINT=&quot;gcf_generate_summary&quot;<br>GCF_TRIGGER_BUCKET=&quot;$ANNOTATION_BUCKET&quot;<br>GCF_ENV_VARS1=&quot;SUMMARY_BUCKET=$SUMMARY_BUCKET&quot;<br>GCF_ENV_VARS2=&quot;ANIMATED=1&quot;<br>GCF_TIMEOUT=&quot;540s&quot;<br>GCF_MEMORY=&quot;2048MB&quot;<br><br>gcloud functions deploy $GCF_NAME \<br>  --runtime python37  \<br>  --source $GCF_SOURCE \<br>  --entry-point $GCF_ENTRY_POINT \<br>  --update-env-vars $GCF_ENV_VARS1 \<br>  --update-env-vars $GCF_ENV_VARS2 \<br>  --trigger-bucket $GCF_TRIGGER_BUCKET \<br>  --region $GCF_REGION \<br>  --timeout $GCF_TIMEOUT \<br>  --memory $GCF_MEMORY \<br>  --quiet</strong></pre><p>Here is how it looks like in the <a href="https://console.cloud.google.com/functions/list">Cloud Console</a>:</p><figure><img alt="Cloud Functions 4" src="https://cdn-images-1.medium.com/max/1024/0*JgdtT1hHxExMKhg1.png" /></figure><h3>🎉 Final tests</h3><p>The HTTP endpoint lets you trigger the pipeline with a GET request:</p><pre><strong>GCF_NAME=&quot;gcf1_detect_shots_http&quot;<br>VIDEO_URI=&quot;gs://cloud-samples-data/video</strong><strong>/visionapi.mp4&quot;<br>GCF_URL=&quot;https://$GCF_REGION-$PROJECT_ID.cloudfunctions.net/$GCF_NAME?video_uri=$VIDEO_URI&quot;<br><br>curl $GCF_URL -H &quot;Authorization: bearer $(gcloud auth print-identity-token)&quot;</strong></pre><pre>Launched shot detection for video_uri &lt;VIDEO_URI&gt;</pre><blockquote>Note: The test video &lt;visionapi.mp4&gt; is located in an external bucket but is publicly accessible.</blockquote><p>In addition, copy one or several videos into the video bucket. You can drag and drop videos:</p><figure><img alt="Dragging files to a bucket" src="https://cdn-images-1.medium.com/max/1024/0*2piYMwPyLnx4sP61.gif" /></figure><p>The videos are then processed in parallel. Here are a few logs:</p><pre>LEVEL NAME                           EXECUTION_ID ... LOG<br>...<br>D     gcf2_generate_summary_animated f6n6tslsfwdu ... Function execution took 49293 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary          yd1vqabafn17 ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_still.png<br>I     gcf2_generate_summary_animated qv9b03814jjk ... shot_ratio: 43%<br>I     gcf2_generate_summary          yd1vqabafn17 ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_still.webp<br>D     gcf2_generate_summary          yd1vqabafn17 ... Function execution took 54616 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary_animated g4d2wrzxz2st ... shot_ratio: 71%<br>...<br>D     gcf2_generate_summary          amwmov1wk0gn ... Function execution took 65256 ms, finished with status: &#39;ok&#39;<br>I     gcf2_generate_summary_animated 7pp882fz0x84 ... shot_ratio: 57%<br>I     gcf2_generate_summary_animated i3u830hsjz4r ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_anim.png<br>I     gcf2_generate_summary_animated i3u830hsjz4r ... Uploading -&gt; b1-videos.../JaneGoodall.mp4.summary035_anim.webp<br>D     gcf2_generate_summary_animated i3u830hsjz4r ... Function execution took 70862 ms, finished with status: &#39;ok&#39;<br>...</pre><p>In the 3rd bucket, you’ll find all still and animated summaries:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*NTDWw5uXmNWsWr6n.png" /></figure><p>You’ve already seen the still summary for &lt;JaneGoodall.mp4&gt; as an introduction to this tutorial. In the animated version, and in only 6 frames, you get an even better idea of what the <a href="https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4">whole video</a> is about:</p><figure><img alt="Video summary" src="https://cdn-images-1.medium.com/max/1024/0*O18noPhqhyceU85Q.gif" /></figure><p>If you don’t want to keep your project, you can delete it:</p><pre><strong>gcloud projects delete $PROJECT_ID</strong></pre><h3>➕ One more thing</h3><pre><strong>first_line_after_licence=16<br>find $PROJECT_SRC -name &#39;*.py&#39; -exec tail -n +$first_line_after_licence {} \; | grep -v &quot;^$&quot; | wc -l</strong></pre><pre>289</pre><p>You did everything in under 300 lines of Python. Less lines, less bugs!</p><p><strong>🔥🐍 Mission accomplished! 🐍🔥</strong></p><h3>🖖 See you</h3><p>I hope you appreciated this tutorial and would love to read <a href="https://bit.ly/feedback-video-summary">your feedback</a>. You can also <a href="https://twitter.com/PicardParis">follow me on Twitter</a>.</p><h3>⏳ Updates</h3><ul><li><strong>2021–10–08</strong>: Updated <a href="https://github.com/PicardParis/cherry-on-py/tree/main/gcf_video_summary">GitHub version</a> with latest library versions + Python 3.7 → 3.9</li></ul><h3>📜 Also in this series</h3><ol><li>Summarizing videos</li><li><a href="https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=friends_link&amp;sk=c9602c33c77aa950a59282b6de5c0c57">Tracking video objects</a></li><li><a href="https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=friends_link&amp;sk=cc252ab86eab9ed2e8583963d0598661">Face detection and processing</a></li><li><a href="https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=friends_link&amp;sk=a3d6e22e7e77828e411592f46025531e">Processing images</a></li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c2f261c8035c" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c">Summarizing videos in 300 lines of code</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>