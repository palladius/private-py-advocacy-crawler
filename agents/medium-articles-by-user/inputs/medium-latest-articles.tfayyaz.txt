
== Article 1
* Title: 'Apache Spark BigQuery Connector — Optimization tips & example Jupyter Notebooks'
* Author: 'Tahir Fayyaz'
* URL: 'https://medium.com/google-cloud/apache-spark-bigquery-connector-optimization-tips-example-jupyter-notebooks-f17fd8476309?source=rss-61dbd674077e------2'
* PublicationDate: 'Thu, 21 May 2020 12:13:03 GMT'
* Categories: bigquery, google-cloud-platform, apache-spark, jupyter-notebook, big-data

Apache Spark BigQuery Connector — Optimization tips &amp; example Jupyter NotebooksLearn how to use the BigQuery Storage API with Apache Spark on Cloud DataprocBigQuery storage to BigQuery computeGoogle BigQuery is Google Cloud’s fully managed data warehouse and just turned 10 years old (Happy Birthday BigQuery!!!). One of its key features is that it separates compute and storage and recently this led to the development of the BigQuery Storage API which allows you to read data at scale from other platforms, like Apache Spark, where the data will be processed without the need to first export the data to Google Cloud Storage as an intermediate step.BigQuery storage API connecting to Apache Spark, Apache Beam, Presto, TensorFlow and PandasSome examples of this integration with other platforms are Apache Spark (which will be be the focus of this post), Presto, Apache Beam, Tensorflow, and Pandas.Apache Spark can read multiple streams of data from the BigQuery Storage API in parallelThe BigQuery Storage API allows you reads data in parallel which makes it a perfect fit for a parallel processing platform like Apache Spark.Using the Apache Spark BigQuery connector, which is built on top of the BigQuery Storage API and BigQuery API, you can now treat BigQuery as another source to read and write data from Apache Spark.Apache and Spark and JupyterLabHow Cloud Dataproc, Apache Spark, Apache Spark BigQuery Connector and Jupyter notebooks connectJupyter notebooks are a great way to get started with learning how to use the Apache Spark BigQuery connector.You can read this post on using Apache Spark with Jupyter Notebooks on Cloud Dataproc to get set-up and then read on for tips on how to use the connector, how to optimize your jobs and view the example Jupyter notebooks now available on GitHub.Set-up the Apache Spark BigQuery Storage connectorOnce you have your notebook running you just need to include the Apache Spark BigQuery Storage connector package when you create a Spark session.from pyspark.sql import SparkSessionspark = SparkSession.builder \  .appName('Optimize BigQuery Storage') \  .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta') \  .getOrCreate()Based on what version of Scala you are running will need to change the artifact name to one of the following:Scala 2.11: com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-betaScala 2.12: com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.15.1-betaYou can view the release notes of the connector to check what is the latest version of the package.Reading BigQuery DataFollowed by this you simply need to set the read format as "bigquery" and you are ready to load the BigQuery table into your Spark job. These examples will make use of the Wikipedia pageviews public dataset created by Marc Cohen.table = "bigquery-public-data.wikipedia.pageviews_2019"df = spark.read \  .format("bigquery") \  .option("table", table) \  .load()However as the Wikipedia pageviews table is very large at 2TB you should filter the data you actually need for your Apache Spark job.The data will not actually be read now as .load() does not trigger a Spark job. The data will be read when an action is called which is the .show() method we use later.Filtering data to optimize Apache Spark jobsTo optimize the performance of your Apache Spark jobs when using the Apache Spark BigQuery Storage connector here are some steps to show you how to only read the data required for the job.BigQuery partition filteringThe API rebalances records between readers until they all completeBigQuery tables can be partitioned by date or integers in a similar fashion to Hive, Parquet and ORC. The wikipedia public dataset uses date partitions and so you can set the filter option to only read 7 days of data instead of all 365 days.table = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()BigQuery columnar storage filteringOnly the columns required are read by Apache SparkBigQuery uses columnar storage similar to Apache Parquet and ORC. Therefore you can read just the columns you need for your Spark job by selecting certain columns with the filter option.table = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()df_wiki_en = df_wiki_pageviews \  .select("title", "wiki", "views")df_wiki_en.printSchema()BigQuery rows filteringThe BigQuery Storage API supports predicate push-down of filters which means that if you set a filter in the where statement later on in your Apache Spark job it will attempt to push the filter to BigQuery. Therefore the two jobs below would yield the same resultUsing filter and wheretable = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()df_wiki_en = df_wiki_pageviews \  .select("title", "wiki", "views") \  .where("views &gt; 10 AND wiki in ('en', 'en.m')")df_wiki_en.show()Using only filtertable = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08' AND views &gt; 10 AND wiki in ('en', 'en.m')") \  .load()df_wiki_en = df_wiki_pageviews \  .select("title", "wiki", "views")df_wiki_en.show()View job performance in the Spark UIWhen you run your job using data from BigQuery you will want to look at the Spark job performance. Take this aggregation job for example:import pyspark.sql.functions as Ftable = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()df_wiki_en = df_wiki_pageviews \  .select("title", "wiki", "views") \  .where("views &gt; 10 AND wiki in ('en', 'en.m')")df_wiki_en_totals = df_wiki_en \.groupBy("title") \.agg(F.sum('views').alias('total_views'))df_wiki_en_totals.orderBy('total_views', ascending=False).show()Using the Component Gateway feature you can easily access the Spark UI to see how the job performed.This stage of the job here shows that there were 188 tasks created to read data in parallel from BigQuery Storage.Setting Max ParallelismAs we can see, some of these tasks are empty as shown in the summary metrics for the 188 completed tasks. We can reduce the number of tasks reading from the BigQuery Storage by setting the maxParallelism property in the read API to match the cluster size and settings. For most use cases the default setting of attempting to read one partition per 400MB should be adequate.If you have a cluster with a total of 8 executor cores you can consider setting the property as maxParallelism property as:8 total executor cores * 7 partition days = 56table = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("maxParallelism", 56) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()You should test this with your data size, BigQuery format (partitioned vs non-partitioned), filter options and cluster configuration.Currently the maximum parallelism allowed by the BigQuery storage API is 1000. If you set the maxParallelism property value to greater than 1000 you will still only have 1000 tasks reading from 1000 multiple streams within a session.Cache data in memoryThere might be scenarios where you want the data in memory instead of reading from BigQuery Storage every time to improve performance .The job above to find the total page views per title will read the data from BigQuery and push the filter to BigQuery. The aggregation will then be computed in Apache Spark.You can modify the job to include a cache of the filtered data from the table and further filter the data on the wiki column which will be applied in memory by Apache Spark.import pyspark.sql.functions as Ftable = "bigquery-public-data.wikipedia.pageviews_2019"df_wiki_pageviews = spark.read \  .format("bigquery") \  .option("table", table) \  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \  .load()df_wiki_all = df_wiki_pageviews \  .select("title", "wiki", "views") \  .where("views &gt; 10")df_wiki_all.cache()df_wiki_en = df_wiki_all \  .where("wiki in ('en', 'en.m')")df_wiki_en_totals = df_wiki_en \.groupBy("title") \.agg(F.sum('views').alias('total_views'))df_wiki_en_totals.orderBy('total_views', ascending=False).show()If you view the job details you will see that the table was cached as part of the job.You can then filter for another wiki language using the cache instead of reading data from BigQuery storage again.df_wiki_de = df_wiki_all \  .where("wiki in ('de', 'de.m')")df_wiki_de_totals = df_wiki_de \.groupBy("title") \.agg(F.sum('views').alias('total_views'))df_wiki_de_totals.orderBy('total_views', ascending=False).show()The difference in this next job is that the table is now being read from memory and therefore runs much faster.You can remove the cache by runningdf_wiki_all.unpersist()Examples Jupyter NotebooksBigQuery Storage &amp; Spark DataFrames — Python Jupyter notebookExample Jupyter notebooks now available on the official Google Cloud Dataproc Github repo on how the Apache Spark BigQuery Storage connector works with Spark DataFrames, Spark SQL and Spark MLlib to read and write data.Note: These notebooks are designed to work with the Python 3 kernel (not PySpark kernel) as this allows you create your Spark session and include the Apache Spark BigQuery connectorThese notebooks make use of spark.sql.repl.eagerEval to output the results of DataFrames in each step without the need to use df.show() and also improves the formatting of the output.BigQuery Storage &amp; Spark DataFrames Notebookhttps://medium.com/media/bb3802b11526fa248c46acd33c283c18/hrefBigQuery Storage &amp; Spark SQLhttps://medium.com/media/e6c2c74e580f3da56786ebad789d49c9/hrefBigQuery Storage &amp; Spark MLlibhttps://medium.com/media/11f551467735cf71c0fe8fc894c732e2/hrefWhat’s nextGet in touch here on Medium (@tfayyaz) or on Twitter (tfayyaz) if you have ideas for other Apache Spark notebook examples you would like to see.Ask any questions in the comments or on Stackoverflow under the google-cloud-dataproc tag.Leave any feedback or issues for the Apache Spark BigQuery connector on GitHubApache Spark BigQuery Connector — Optimization tips &amp; example Jupyter Notebooks was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 2
* Title: 'Apache Spark and Jupyter Notebooks made easy with Dataproc component gateway'
* Author: 'Tahir Fayyaz'
* URL: 'https://medium.com/google-cloud/apache-spark-and-jupyter-notebooks-made-easy-with-dataproc-component-gateway-fa91d48d6a5a?source=rss-61dbd674077e------2'
* PublicationDate: 'Thu, 12 Mar 2020 12:48:09 GMT'
* Categories: data-science, apache-spark, jupyter-notebook, bigquery, google-cloud-platform

Use the new Dataproc optional components and component gateway features to easily set-up and use Jupyter NotebooksApache Spark and Jupyter Notebooks architecture on Google CloudAs a long time user and fan of Jupyter Notebooks I am always looking for the best ways to set-up and use notebooks especially in the cloud. I believe Jupyter Notebooks are the perfect tool for learning, prototyping, and in some cases production of your data projects as they allow you to interactively run your code and immediately see your results. They are a great tool for collaboration thanks to a background coming from being used and shared in scientific communities.You might have used Jupyter notebooks on your desktop in the past with Python but struggled with handling very large datasets. However with many kernels now available you can make use of Apache Spark for distributed processing of large-scale data in Jupyter but also continue to use your Python libraries in the same notebook.However getting an Apache Spark cluster set-up with Jupyter Notebooks can be complicated and so in Part 1 of this new “Apache Spark and Jupyter Notebooks on Cloud Dataproc” series of posts I will show you how easy it is to get started thanks to new features like optional components and component gateways.Create a Dataproc cluster with Spark and JupyterYou can create a Cloud Dataproc cluster using the Google Cloud Console, gcloud CLI or Dataproc client libraries.We will be using the gcloud CLI from the Cloud Shell where gcloud is already installed (If you’re new to Google Cloud view the Getting Started with Cloud Shell &amp; gcloud codelab).You can also use the gcloud CLI locally by installing the Cloud SDK.To get started once in the cloud shell or your terminal window set your project ID where you will create your Dataproc clustergcloud config set project &lt;project-id&gt;Enable product APIs and IAM rolesRun this command to enable all the APIs required in the Apache Spark and Jupyter Notebooks on Cloud Dataproc series of posts.gcloud services enable dataproc.googleapis.com \  compute.googleapis.com \  storage-component.googleapis.com \  bigquery.googleapis.com \  bigquerystorage.googleapis.comIf you are not the admin or do not have the correct permissions to enable APIs ask the admin for your GCP organization or project to enable the APIs above.They will also need give you the correct Dataproc IAM roles and Google Cloud Storage IAM roles to create and use your Dataproc cluster.Create a GCS bucket to be used by your Dataproc ClusterCreate a Google Cloud Storage bucket in the region closest to your data and give it a unique name. This will be used for the Dataproc cluster.REGION=us-central1BUCKET_NAME=&lt;your-bucket-name&gt;gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME}You should see the following outputCreating gs://&lt;your-bucket-name&gt;/...Create your Dataproc Cluster with Jupyter &amp; Component GatewaySet the env variables for your clusterREGION=us-central1 ZONE=us-central1-a CLUSTER_NAME=spark-jupyter-&lt;your-name&gt; BUCKET_NAME=&lt;your-bucket-name&gt;Then run this gcloud command to create your cluster with all the necessary components to work with Jupyter on your cluster.gcloud beta dataproc clusters create ${CLUSTER_NAME} \  --region=${REGION} \  --zone=${ZONE} \  --image-version=1.5 \  --master-machine-type=n1-standard-4 \  --worker-machine-type=n1-standard-4 \  --bucket=${BUCKET_NAME} \  --optional-components=ANACONDA,JUPYTER \  --enable-component-gateway \  --metadata 'PIP_PACKAGES=google-cloud-bigquery google-cloud-storage' \  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.shYou should see the following output while your cluster is being createdWaiting on operation [projects/spark-jupyter-notebooks/regions/us-central1/operations/random-letters-numbers-abcd123456].Waiting for cluster creation operation…It should take 2 to 3 minutes to create your cluster and once it is ready you will be able to access your cluster from the Dataproc Cloud console UI.You should the following output once the cluster is created:Created [https://dataproc.googleapis.com/v1beta2/projects/project-id/regions/us-central1/clusters/spark-jupyter-your-name] Cluster placed in zone [us-central1-a].Flags used in gcloud dataproc create commandHere is a breakdown of the flags used in the gcloud dataproc create command--region=${REGION} --zone=${ZONE} Specifies the region and zone of where the cluster will be created. You can see the list of available regions here. Zone is optional unless if you are using n2 machine types when you must specify a zone.--image-version=1.4The image version to use in your cluster. You can see the list of available versions here.--bucket=${BUCKET_NAME}Specify the Google Cloud Storage bucket you created earlier to use for the cluster. If you do not supply a GCS bucket it will be created for you.This is also where your notebooks will be saved even if you delete your cluster as the GCS bucket is not deleted.--master-machine-type=n1-standard-4--worker-machine-type=n1-standard-4The machine types to use for your Dataproc cluster. You can see a list of available machine types here.Note: Look out for future post in the series on recommendations for what machine types to use and how to enable auto-scaling--optional-components=ANACONDA,JUPYTERSetting these values for optional components will install all the necessary libraries for Jupyter and Anaconda (which is required for Jupyter notebooks) on your cluster.--enable-component-gatewayEnabling component gateway creates an App Engine link using Apache Knox and Inverting Proxy which gives easy, secure and authenticated access to the Jupyter and JupyterLab web interfaces meaning you no longer need to create SSH tunnels.It will also create links for other tools on the cluster including the Yarn Resource manager and Spark History Server which are useful for seeing the performance of your jobs and cluster usage patterns.--metadata 'PIP_PACKAGES=google-cloud-bigquery google-cloud-storage' --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.shInstalls the latest versions of the Google Cloud BigQuery python library and the Google Cloud Storage python library. These will be used to perform various tasks when working with BigQuery and GCS in your notebooks.Accessing Jupyter or JupyterLab web interfacesOnce the cluster is ready you can find the Component Gateway links to the Jupyter and JupyterLab web interfaces in the Google Cloud console for Dataproc by clicking on the cluster you created and going to the Web Interfaces tab.Alternatively you can get the links by running this gcloud command.REGION=us-central1CLUSTER_NAME=spark-jupyter-&lt;your-name&gt;gcloud beta dataproc clusters describe ${CLUSTER_NAME} \  --region=${REGION}Which will show an output with the links in the following format.clusterName: spark-jupyter-&lt;your-name&gt;clusterUuid: XXXX-1111-2222-3333-XXXXXXconfig:  configBucket: bucket-name  endpointConfig:    enableHttpPortAccess: true    httpPorts:      Jupyter: https://random-characters-dot-us-east1.dataproc.googleusercontent.com/jupyter/      JupyterLab: https://random-characters-dot-us-east1.dataproc.googleusercontent.com/jupyter/lab/...You will notice that you have access to Jupyter which is the classic notebook interface or JupyterLab which is is described as the next-generation UI for Project Jupyter.There are a lot of great new UI features in JupyterLab and so if you are new to using notebooks or looking for the latest improvements it is recommended to go with using JupyterLab as it will eventually replace the classic Jupyter interface according to the official docs.Python 3, PySpark, R, and Scala kernelsBased on the image version you selected when creating your Dataproc cluster you will have different kernels available:Image version 1.3: Python 2 and PySparkImage version 1.4: Python 3, PySpark (Python), R, and Spylon (Scala)Image version Preview (1.5): Python 3, PySpark (Python), R, and Spylon (Scala)You should use image version 1.4 or above so that you can make use of the Python 3 kernel to run PySpark code or the Spylon kernel to run Scala code.Creating your first PySpark Jupyter NotebookFrom the launcher tab click on the Python 3 notebook icon to create a notebook with a Python 3 kernel (not the PySpark kernel) which allows you to configure the SparkSession in the notebook and include the spark-bigquery-connector required to use the BigQuery Storage API.Once your notebook opens in the first cell check the Scala version of your cluster so you can include the correct version of the spark-bigquery-connector jar.Input [1]:!scala -versionOutput [1]:Create a Spark session and include the spark-bigquery-connector jarInput [2]:from pyspark.sql import SparkSessionspark = SparkSession.builder \  .appName('Jupyter BigQuery Storage')\  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \  .getOrCreate()Create a Spark DataFrame by reading in data from a public BigQuery dataset. This makes use of the spark-bigquery-connector and BigQuery Storage API to load the data into the Spark cluster.If your Scala version is 2.11 use the following jargs://spark-lib/bigquery/spark-bigquery-latest.jarIf your Scala version is 2.12 use the following jargs://spark-lib/bigquery/spark-bigquery-latest_2.12.jarWe will create a Spark DataFrame and load data from the BigQuery public dataset for Wikipedia pageviews. You will notice we are not running a query on the data as we are using the bigquery-storage-connector to load the data into Spark where the processing of the data will happen.Input [3]:table = "bigquery-public-data.wikipedia.pageviews_2020"df = spark.read \  .format("bigquery") \  .option("table", table) \  .load()df.printSchema()Output [3]:Create a new aggregated Spark DataFrame and print the schemaInput [4]:df_agg = df \  .select('wiki', 'views') \  .where("datehour = '2020-03-03'") \  .groupBy('wiki') \  .sum('views')df_agg.printSchema()Output [4]:Run the aggregation using the .show() function on the DataFrame which will start the Spark job to process the data and then show the output of the Spark DataFrame limited to the first 20 rows.Input [5]:df_agg.show()Output [5]:You should now have your first Jupyter notebook up and running on your Dataproc cluster. Give your notebook a name and it will be auto-saved to the GCS bucket used when creating the cluster. You can check this using this gsutil command.BUCKET_NAME=&lt;your-bucket-name&gt;gsutil ls gs://${BUCKET_NAME}/notebooks/jupyterExample notebooks for more use casesThe next posts in this series will feature Jupyter notebooks with common Apache Spark patterns for loading data, saving data, and plotting your data with various Google Cloud Platform products and open-source tools:Spark and BigQuery Storage APISpark and Google Cloud StorageSpark and Apache Iceberg / DeltaLakePlotting Spark DataFrames using PandasYou can also access the upcoming examples notebooks on the Cloud Dataproc GitHub repoGiving the cluster’s service account access to dataIn the example above we are accessing a public dataset but for your use case you will most likely be accessing your companies data with restricted access. The Jupyter notebook and Dataproc cluster will attempt to access data in Google Cloud Platform services using the service account of the underlying Google Computer Engine (GCE) VMs and not your own Google credentials.You can find the service account of your cluster by running this command to describe the master VM in GCE which will have the same name of your Dataproc cluster followed by -mZONE=us-central1-a CLUSTER_NAME=spark-jupyter-&lt;your-name&gt;gcloud compute instances describe ${CLUSTER_NAME}-m \  --zone=${ZONE}This will give a long list of attributes including the service account and scopes as shown in this example output.serviceAccounts:-   email: &lt;random-number&gt;-compute@developer.gserviceaccount.com    scopes:  - https://www.googleapis.com/auth/bigquery  - https://www.googleapis.com/auth/bigtable.admin.table  - https://www.googleapis.com/auth/bigtable.data  - https://www.googleapis.com/auth/cloud.useraccounts.readonly  - https://www.googleapis.com/auth/devstorage.full_control  - https://www.googleapis.com/auth/devstorage.read_write  - https://www.googleapis.com/auth/logging.writeAlternatively you can view the service account in the Google Cloud Console by going to the VM Instances tab in your Dataproc cluster and clicking on the master VM instance.Once in the VM page scroll to the bottom and you will see the service account for the VM. This is the same service account for all VM instances in your cluster.You should then give the service account the correct BigQuery IAM roles and GCS IAM roles to access the BigQuery datasets or GCS buckets you need.For more details on providing the correct access read this solution to Help secure the pipeline from your data lake to your data warehouse.Deleting your Dataproc clusterOnce you have have finished all of your work within the Jupyter Notebook and all Spark jobs have finished processing it is recommended to delete the Dataproc cluster which can be done via the Cloud Console or using this gcloud command:REGION=us-central1 CLUSTER_NAME=spark-jupyter-&lt;your-name&gt; gcloud beta dataproc clusters delete ${CLUSTER_NAME} \  --region=${REGION}As mentioned before you can always delete and recreate your cluster and all your notebooks will still be saved in the Google Cloud Storage bucket which is not deleted when you delete your Dataproc cluster.What’s NextLook out for next post series which will cover using the bigquery-storage-connector in a Jupyter Notebook in more depth.Follow me here on Medium (@tfayyaz) and on Twitter (tfayyaz) to hear more about the latest updates about Dataproc and share feedback.Ask any questions in the comments or on Stackoverflow under the google-cloud-dataproc tag.Have fun working with Spark and Jupyter Notebooks on Dataproc.Apache Spark and Jupyter Notebooks made easy with Dataproc component gateway was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.
