
== Article 1
* Title: 'Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851?source=rss-6be63961431c------2'
* PublicationDate: 'Wed, 21 Feb 2024 18:04:07 GMT'
* Categories: gemma, vertex-ai, machine-learning, google-cloud-platform, llm

Gemma just opened ;)Gemma is a family of open, lightweight, and easy-to-use models developed by Google Deepmind. The Gemma models are built from the same research and technology used to create Gemini.This means that we (ML developers &amp; practitioners) now have additional versatile large language models (LLMs) in our toolbox!If you’d rather read code, you can jump straight to this Python notebook:→ Finetune Gemma using KerasNLP and deploy to Vertex AIAvailabilityGemma is available today in Google Cloud and the machine learning (ML) ecosystem. You’ll probably find an ML platform you’re already familiar with:Gemma joins 130+ models in Vertex AI Model GardenGemma joins the Kaggle ModelsGemma joins the Hugging Face ModelsGemma joins Google AI for DevelopersHere are the Gemma Terms of Use.Gemma modelsThe Gemma family launches in two sizes, Gemma 2B and 7B, targeting two typical platform types:| Model    | Parameters  | Platforms                           || -------- | ----------- | ----------------------------------- || Gemma 2B | 2.5 billion | Mobile devices and laptops          || Gemma 7B | 8.5 billion | Desktop computers and small servers |These models are text-to-text English LLMs:Input: a text string, such as a question, a prompt, or a document.Output: generated English text, such as an answer or a summary.They were trained on a diverse and massive dataset of 8 trillion tokens, including web documents, source code, and mathematical text.Each size is available in untuned and tuned versions:Pretrained (untuned): Models were trained on core training data, not using any specific tasks or instructions.Instruction-tuned: Models were additionally trained on human language interactions.That gives us four variants. As an example, here are the corresponding model IDs when using Keras:- `gemma_2b_en`- `gemma_instruct_2b_en`- `gemma_7b_en`- `gemma_instruct_7b_en`Use casesGoogle now offers two families of LLMs: Gemini and Gemma. Gemma is a complement to Gemini, is based on technologies developed for Gemini, and addresses different use cases.Examples of Gemini benefits:Enterprise applicationsMultilingual tasksOptimal qualitative results and complex tasksState-of-the-art multimodality (text, image, video)Groundbreaking features (e.g. Gemini 1.5 Pro 1M token context window)Examples of Gemma benefits:Learning, research, or prototyping based on lightweight modelsFocused tasks such as text generation, summarization, and Q&amp;AFramework or cross-device interoperabilityOffline or real-time text-only processingsThe Gemma model variants are useful in the following use cases:Pretrained (untuned): Consider these models as a lightweight foundation and perform a custom finetuning to optimize them to your own needs.Instruction-tuned: You can use these models for conversational applications, such as a chatbot, or to customize them even further.InteroperabilityAs Gemma models are open, they are virtually interoperable with all ML platforms and frameworks.For the launch, Gemma is supported by the following ML ecosystem players:Google CloudKaggleKeras, which means that Gemma runs on JAX, PyTorch, and TensorFlowHugging FaceNvidiaAnd, of course, Gemma can run on GPUs and TPUs.RequirementsGemma models are lightweight but, in the LLM world, this still means gigabytes.In my tests, when running inferences in half precision (bfloat16), here are the minimum storage and GPU memory that were required:| Model    | Total parameters | Assets size | Min. GPU RAM to run || -------- | ---------------: | ----------: | ------------------: || Gemma 2B |    2,506,172,416 |     4.67 GB |              8.3 GB || Gemma 7B |    8,537,680,896 |    15.90 GB |             20.7 GB |To experiment with Gemma and Vertex AI, I used Colab Enterprise with a g2-standard-8 runtime, which comes with an NVIDIA L4 GPU and 24 GB of GPU RAM. This is a cost-effective configuration to save time and avoid running out-of-memory when prototyping in a notebook.Finetuning GemmaDepending on your preferred frameworks, you’ll find different ways to customize Gemma. Keras is one of them and provides everything you need.KerasNLP lets you load a Gemma model in a single line:import kerasimport keras_nlpgemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma_instruct_2b_en")Then, you can directly generate text:inputs = [    "What's the most famous painting by Monet?",    "Who engineered the Statue of Liberty?",    'Who were "The Lumières"?',]for input in inputs:    response = gemma_lm.generate(input, max_length=25)    output = response[len(input) :]    print(f"{input!r}\n{output!r}\n")With the instruction-tuned version, Gemma gives you expected answers, as would a good LLM-based chatbot:# With "gemma_instruct_2b_en""What's the most famous painting by Monet?"'\n\nThe most famous painting by Monet is "Water Lilies," which''Who engineered the Statue of Liberty?''\n\nThe Statue of Liberty was built by French sculptor Frédéric Auguste Bartholdi between 1''Who were "The Lumières"?''\n\nThe Lumières were a group of French scientists and engineers who played a significant role'Now, try the untuned version:gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma_2b_en")You’ll get different results, which is expected as this version was not trained on any specific task and is designed to be finetuned to your own needs:# With "gemma_2b_en""What's the most famous painting by Monet?""\n\nWhat's the most famous painting by Van Gogh?\n\nWhat"'Who engineered the Statue of Liberty?''\n\nA. George Washington\nB. Napoleon Bonaparte\nC. Robert Fulton\nD''Who were "The Lumières"?'' What did they invent?\n\nIn the following sentence, underline the correct modifier from the'If you’d like, for example, to build a Q&amp;A application, prompt engineering may fix some of these issues but, to be grounded on facts and return consistent results, untuned models need to be finetuned with training data.The Gemma models have billions of trainable parameters. The next step consists of using the Low Rank Adaptation (LoRA) to greatly reduce the number of trainable parameters:# Number of trainable parameters before enabling LoRA: 2.5B# Enable LoRA for the model and set the LoRA rank to 4gemma_lm.backbone.enable_lora(rank=4)# Number of trainable parameters after enabling LoRA: 1.4M (1,800x less)A training can now be performed with reasonable time and GPU memory requirements.For prototyping on a small training dataset, you can launch a local finetuning:training_data: list[str] = [...]# Reduce the input sequence length to limit memory usagegemma_lm.preprocessor.sequence_length = 128# Use AdamW (a common optimizer for transformer models)optimizer = keras.optimizers.AdamW(    learning_rate=5e-5,    weight_decay=0.01,)# Exclude layernorm and bias terms from decayoptimizer.exclude_from_weight_decay(var_names=["bias", "scale"])gemma_lm.compile(    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),    optimizer=optimizer,    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],)gemma_lm.fit(training_data, epochs=1, batch_size=1)After a couple of minutes of training (on 1,000 examples) and using a structured prompt, the finetuned model can now answer questions based on your training data:# With "gemma_2b_en" before finetuning'Who were "The Lumières"?'' What did they invent?\n\nIn the following sentence, underline the correct modifier from the'# With "gemma_2b_en" after finetuning"""Instruction:Who were "The Lumières"?Response:""""The Lumières were the inventors of the first motion picture camera. They were"The prototyping stage is over. Before you proceed to finetuning models in production with large datasets, check out new specific tools to build a responsible Generative AI application.Responsible AIWith great power comes great responsibility, right?The Responsible Generative AI Toolkit will help you build safer AI applications with Gemma. You’ll find expert guidance on safety strategies for the different aspects of your solution.Also, do not miss the Learning Interpretability Tool (LIT) for visualizing and understanding the behavior of your Gemma models. Here’s the tool in action, investigating Gemma’s behavior:Deploying GemmaWe’re in MLOps territory here and you’ll find different LLM-optimized serving frameworks running on GPUs or TPUs.Here are popular serving frameworks:vLLM (GPU)TGI: Text Generation Interface (GPU)Saxml (TPU or GPU)TensorRT-LLM (NVIDIA Triton GPU)These frameworks come with prebuilt container images that you can easily deploy to Vertex AI.Here is a simple notebook to get you started:Finetune Gemma using KerasNLP and deploy to Vertex AIFor production finetuning, you can use Vertex AI custom training jobs. Here are detailed notebooks:Gemma Finetuning (served by vLLM or HexLLM)Gemma Finetuning (served by TGI)Those notebooks focus on deployment and serving:Gemma Deployment (served by vLLM or HexLLM)Gemma Deployment to GKE using TGI on GPUFor more details and updates, check out the documentation:Vertex AIGKEAll the best to Gemini and Gemma!After years of consolidation in machine learning hardware and software, it’s exciting to see the pace at which the overall ML landscape now evolves and in particular with LLMs. LLMs are technologies still in their infancy, so we can expect to see more breakthroughs in the near future.I very much look forward to seeing what the ML community is going to build with Gemma!All the best to the Gemini and Gemma families!Follow me on Twitter or LinkedIn for more cloud explorations…Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 2
* Title: 'Moderating text with the Natural Language API'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c?source=rss-6be63961431c------2'
* PublicationDate: 'Fri, 16 Jun 2023 16:49:45 GMT'
* Categories: google-cloud-platform, moderation, nlp, machine-learning, data

Photo by Da Nina on Unsplash2023–09–12: text moderation got Generally Available (GA) over the summer + added link to Sep. blog postThe Natural Language API lets you extract information from unstructured text using Google machine learning and provides a solution to the following problems:Sentiment analysisEntity analysisEntity sentiment analysisSyntax analysisContent classificationText moderation🔍 Moderation categoriesText moderation lets you detect sensitive or harmful content. The first moderation category that comes to mind is “toxicity”, but there can be many more topics of interest. A PaLM 2-based model powers the predictions and scores 16 categories:| ---------- | --------------------- | ----------------- | -------------- || Toxic      | Insult                | Public Safety     | War &amp; Conflict || Derogatory | Profanity             | Health            | Finance        || Violent    | Death, Harm &amp; Tragedy | Religion &amp; Belief | Politics       || Sexual     | Firearms &amp; Weapons    | Illicit Drugs     | Legal          |⚡ Moderating textLike always, you can call the API through the REST/RPC interfaces or with idiomatic client libraries.Here is an example using the Python client library (google-cloud-language) and the moderate_text method:from google.cloud import languagedef moderate_text(text: str) -&gt; language.ModerateTextResponse:    client = language.LanguageServiceClient()    document = language.Document(        content=text,        type_=language.Document.Type.PLAIN_TEXT,    )    return client.moderate_text(document=document)text = (    "I have to read Ulysses by James Joyce.\n"    "I'm a little over halfway through and I hate it.\n"    "What a pile of garbage!")response = moderate_text(text)🚀 It’s fast! The model latency is very low, allowing real-time analyses.The response contains confidence scores for each moderation category. Let’s sort them out:import pandas as pddef confidence(category: language.ClassificationCategory) -&gt; float:    return category.confidencecolumns = ["category", "confidence"]categories = sorted(    response.moderation_categories,    key=confidence,    reverse=True,)data = ((category.name, category.confidence) for category in categories)df = pd.DataFrame(columns=columns, data=data)print(f"Text analyzed:\n{text}\n")print(f"Moderation categories:\n{df}")You may typically ignore scores below 50% and calibrate your solution by defining upper limits (or buckets) for the confidence scores. In this example, depending on your thresholds, you may flag the text as disrespectful (toxic) and insulting:Text analyzed:I have to read Ulysses by James Joyce.I'm a little over halfway through and I hate it.What a pile of garbage!Moderation categories:                 category  confidence0                   Toxic    0.6808731                  Insult    0.6094752               Profanity    0.4825163                 Violent    0.3333334                Politics    0.2377055   Death, Harm &amp; Tragedy    0.1897596                 Finance    0.1769557       Religion &amp; Belief    0.1510798                   Legal    0.1009469                  Health    0.09630510          Illicit Drugs    0.08333311     Firearms &amp; Weapons    0.07692312             Derogatory    0.07395313         War &amp; Conflict    0.05263214          Public Safety    0.05181315                 Sexual    0.028222🖖 MoreTo try it, run this Colab notebook: Using the Natural Language APISee the supported languagesRead more about text moderationSee the latest blog post on Improving Trust in AI and Online CommunitiesFollow me on Twitter or LinkedIn for more cloud explorationsModerating text with the Natural Language API was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'A Better Way to Use Google Cloud from Colab'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 13 Jun 2023 18:23:54 GMT'
* Categories: jupyter-notebook, python, google-colab, google-cloud-platform, announcements

Photo by Pablo Arroyo on UnsplashOne of the best things about Colab is low friction. You open a notebook and Colab knows who you are based on your current Google account — there’s no need to separately login to Colab (unless you’re not already logged into Google).Problem: Google Cloud provides an amazingly powerful array of services, which work great in a Colab notebook, but you need to separately authenticate yourself to Google Cloud. This is not difficult but, till now, many developers resorted to using a service account, which adds a bit of complexity and, even worse, can lead to accidentally checking service account credentials into a repo, which is a serious security concern.Solution: We simplified the flow for you so that you can now use your Google Cloud project from a Colab notebook in a more straightforward way and hopefully without thinking about creating a service account.How does it work? In order to understand what’s new, let’s do a before-after comparison. Here is what can be found in some notebooks using a service account and downloading a private key:# BEFOREimport osfrom google.colab import authauth.authenticate_user()PROJECT_ID = "YOUR_PROJECT_ID"SA_NAME = "YOUR_SERVICE_ACCOUNT_NAME"SA_EMAIL = f"{SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com"!gcloud config set project $PROJECT_ID!gcloud iam service-accounts create $SA_NAME!gcloud iam service-accounts keys create ./key.json --iam-account $SA_EMAILos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "./key.json"This is not necessary and there are two potential problems here:A service account is created. If you’re not the owner of the project, you may not have the permissions for this.Service account credentials are manually downloaded at source code level. The private key can accidentally be pushed and made public.And here’s the simpler, service-account-less method:# AFTERfrom google.colab import authPROJECT_ID = "YOUR_PROJECT_ID"auth.authenticate_user(project_id=PROJECT_ID)You are authenticated for gcloud CLI commands (like before).You are also authenticated when using Google Cloud Python Client Libraries.Your default project is set.The new version is shorter, simpler, and less error-prone.If you’re new to Colab, would like to get more details or example notebooks, check out “Using Google Cloud from Colab”.A Better Way to Use Google Cloud from Colab was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 4
* Title: 'Using Google Cloud from Colab'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 13 Jun 2023 16:31:33 GMT'
* Categories: machine-learning, google-colab, google-cloud-platform, python, data

Google Colab is an amazing tool for Pythonistas. It can be used for a variety of tasks, from quickly experimenting with Python code to sharing extensive data processing notebooks with the world. Colab runs on Google Cloud, which gives you a boost when accessing cloud services because your code is running inside Google’s high performance network, and also offers a simple way to use Google Cloud services, letting you run powerful cloud workloads from your browser.📦️ Install dependenciesColab comes with hundreds of preinstalled packages, including pandas, numpy, matplotlib, Flask, Pillow, tensorflow, and pytorch. You can install additional required dependencies as needed.For example, to analyze text with the power of machine learning using the Natural Language API, install the google-cloud-language client library:!pip install google-cloud-language&gt;=2.9.1🔑 AuthenticateTo authenticate with your Google Cloud account within the Colab notebook, use the authenticate_user method. A new parameter lets you specify your project ID:from google.colab import authPROJECT_ID = ""  # @param {type:"string"}auth.authenticate_user(project_id=PROJECT_ID)After this step:👍 You are authenticated for gcloud CLI commands.👍 You are also authenticated when using Google Cloud Python client libraries. Note that you generally won’t need to create a service account.👍 Your default project is set.👍 Your notebook can also access your Google Drive, letting you ingest or generate your own files.🔓 Enable APIsEnsure required APIs are enabled. In our example, that’s the service language.googleapis.com:!gcloud services enable language.googleapis.com🤯 Use Google Cloud servicesThat’s it! You can now directly use the service by calling its Python client library:from google.cloud import languagedef analyze_text_sentiment(text: str) -&gt; language.AnalyzeSentimentResponse:    client = language.LanguageServiceClient()    document = language.Document(        content=text,        type_=language.Document.Type.PLAIN_TEXT,    )    return client.analyze_sentiment(document=document)# Inputtext = "Python is a very readable language, ..."  # @param {type:"string"}# Send a request to the APIresponse = analyze_text_sentiment(text)# Use the resultsprint(response)📊 Benefit from notebook advanced featuresColab is hosting a Jupyter notebook. I personally depict Colab as the “Google Drive of notebooks”. As notebooks have additional superpowers, this lets you nicely process and visualize your data, such as tables, images, or charts.In our example, the function show_text_sentiment gathers results in a pandas DataFrame, which renders as a table:✨ Check it outIn these examples, click the “Open in Colab” link:Using Google Cloud from Colab (short)Using the Natural Language API with Python (detailed)💡 You can directly create a new notebook by opening the colab.new URL.👋 Have fun!Read the Google Colab blog to learn more about Colab.Follow me (Twitter / LinkedIn) for more cloud explorations ;)Using Google Cloud from Colab was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 5
* Title: 'From pixels to information with Document AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010?source=rss-6be63961431c------2'
* PublicationDate: 'Wed, 15 Mar 2023 17:17:47 GMT'
* Categories: programming, python, gcp-app-dev, machine-learning, technology

We’re seeing successively difficult problems getting solved thanks to machine learning (ML) models. For example, Natural Language AI and Vision AI extract insights from text and images, with human-like results. They solve problems central to the way we communicate:✅ Natural language processing (NLP)✅ Optical character recognition (OCR)✅ Handwriting recognition (HWR)What’s next? Well, it’s already here, with Document AI, and keeps growing:✅ Document understanding (DU)Document AI builds on these foundations to let you extract information from documents, in many forms:TextStructureSemantic entitiesNormalized valuesEnrichmentsQualitative signalsIn this article, you’ll see the following:An overview of Document AIPractical and visual examplesA document processing demo (source code included)Processing documentsProcessor typesThere are as many document types as you can imagine so, to meet all needs, document processors are at the heart of Document AI. They can be of the following types:General processors handle common tasks such as detecting document tables or parsing forms.Specialized processors analyze specific documents such as invoices, receipts, pay slips, bank statements, identity documents, official forms, etc.Custom processors meet other specific needs, letting you automatically train private ML models based on your own documents, and perform tasks such as custom document classification or custom entity detection.Processor locationsWhen you create a processor, you specify its location. This helps control where the documents will be processed. Here are the current multi-region locations:In addition, some processors are available in single-region locations; this lets you address local regulation requirements. If you regularly process documents in real-time or large batches of documents, this can also help get responses with even lower latencies. As an example, here are the current locations for the “Document OCR” general processor:Note: API endpoints are named according to the convention {location}-documentai.googleapis.com.For more information, check out Regional and multi-regional support.Processor versionsProcessors can evolve over time, to offer more precise results, new features, or fixes. For example, here are the current versions available for the “Expense Parser” specialized processor:You may typically do the following:Use Stable version 1.3 in production.Use Release Candidate version 1.4 to benefit from new features or test a future version.To stay updated, follow the Release notes.InterfacesDocument AI is available to developers and practitioners through the usual interfaces:REST API (universal JSON-based interface)RPC API (low-latency gRPC interface, used by all Google services)Client Libraries (gRPC wrappers, to develop in your preferred programming language)Cloud Console (web admin user interface)RequestsThere are two ways you can process documents:Synchronously with online requests, to analyze a single document and directly use the resultsAsynchronously with batch requests, to launch a batch processing operation on multiple or larger documentsExtracting information from pixelsLet’s start by analyzing this simple screenshot with the “Document OCR” general processor, and check how pixels become structured data.Input imageDocument AI responseThe response contains a Document instance.The entire text of the input, detected in natural reading order, is serialized under Document.text.The structured data is returned on a per page basis (a single page here).The “Document OCR” processor returns a structural skeleton common to all processors.The examples in this article show snake-case field names (like mime_type), using the convention used by gRPC and programming languages like Python. For camel-case environments (like REST/JSON), there is a direct mapping between the two conventions:mime_type ↔ mimeTypepage_number ↔ pageNumber…Text levelsFor each page, four levels of text detection are returned:blocksparagraphslinestokensIn these lists, each item exposes a layout including the item's position relative to the page in bounding_poly.normalized_vertices.This lets us, for example, highlight the 22 detected tokens:Here is the last token:Note: Float values are presented truncated for the sake of readability.Language supportDocuments are often written using one single language, but sometimes use multiple languages. You can retrieve the detected languages at different text levels.In our previous example, two blocks are detected ( pages[0].blocks[]). Let's highlight them:The left block is a mix of German, French, and English, while the right block is English only. Here is how the three languages are reported at the page level:Note: At this level, the language confidence ratios roughly correspond to the proportion of text detected in each language.Now, let’s highlight the five detected lines ( pages[0].lines[]):Each language is also reported at the line level:If needed, you can get language info at the token level too. “Question” is the same word in French and in English, and is adequately returned as an English token in this context:In the screenshot, did you notice something peculiar in the left block?Well, punctuation rules can be different between languages. French uses a typographical space for double punctuation marks (“double” as “written in two parts”, such as in "!", "?", """,...). Punctuation is an important part of languages that can get "lost in translation". Here, the space is preserved in the transcription of "Bienvenue !". Nice touch Document AI or, should I say, touché!For more information, see Language support.Handwriting detectionNow — a much harder problem — let’s check how handwriting is handled.This example is a mix of printed and handwritten text, where I wrote both a question and an answer. Here are the detected tokens:I am pleasantly surprised to see my own handwriting transcribed:I asked my family (used to writing French) to do the same. Each unique handwriting sample also gets correctly detected:… and transcribed:This can look magical but that’s one of the goals of ML models: return results as close as possible to human responses.Confidence scoresWe make mistakes, and so can ML models. To better appreciate the structured data you get, results include confidence scores:Confidence scores do not represent accuracy.They represent how confident the model is with the extracted results.They let you — and your users — ponder the model’s extractions.Let’s overlay confidence scores on top of the previous example:After grouping them in buckets, confidence scores appear to be generally high, with a few outliers:The lowest confidence score here is 57%. It corresponds to a handwritten word (token) that is both short (less context given for confidence) and not particularly legible indeed:For best results, keep in mind these general rules of thumb:ML results are best guesses, which can be correct or incorrect.Results with lower confidence scores are more likely to be incorrect guesses.If we can’t smoothly read a part of a document, or need to think twice about it, it’s probably also harder for the ML model.Although all text is correctly transcribed in the presented examples, this won’t always be the case depending on the input document. To build safer solutions — especially with critical business applications — you may consider the following:Be clear with your users that results are auto-generated.Communicate the scope and limitations of your solution.Improve the user experience with interpretable information or filterable results.Design your processes to trigger human intervention on lower confidence scores.Monitor your solution to detect changes over time (drift of stable metrics, decline in user satisfaction, etc.).To learn more about AI principles and best practices, check out Responsible AI practices.Rotation, skew, distortionHow many times did you scan a document upside down by mistake? Well, this shouldn’t be a concern anymore. Text detection is very robust to rotation, skew, and other distortions.In this example, the webcam input is not only upside down but also skewed, blurry, and with text in unusual orientations:Before further analysis, Document AI considers the best reading orientation, at the page level, and preprocesses (deskews) each page if needed. This gives you results that can be used and visualized in a more natural way. Once processed by Document AI, the preceding example gets easier to read, without straining your neck:In the results, each page has an image field by default. This represents the image - deskewed if needed - used by Document AI to extract information. All the page results and coordinates are relative to this image. When a page has been deskewed, a transforms element is present and contains the list of transformation matrices applied to the image:Notes:Page images can be in different formats. For instance, if your input is a JPEG image, the response will include either the same JPEG or a deskewed PNG (as in the earlier example).Deskewed images have larger dimensions (deskewing adds blank outer areas).If you don’t need visual results in your solution, you can specify a field_mask in your request to receive lighter responses, with only your fields of interest.OrientationDocuments don’t always have all of their text in a single orientation, in which case deskewing alone is not enough. In this example, the sentence is broken out in four different orientations. Each part gets properly recognized and processed in its natural orientation:Orientations are reported in the layout field:Note: Orientations are returned at each OCR level ( blocks, paragraphs, lines, and tokens).NoiseWhen documents come from our analog world, you can expect … the unexpected. As ML models are trained from real-world samples — containing real-life noise — a very interesting outcome is that Document AI is also significantly robust to noise.In this example with crumpled paper, the text starts to be difficult to read but still gets correctly transcribed by the OCR model:Documents can also be dirty or stained. With the same sample, this keeps working after adding some layers of noise:In both cases, the exact same text is correctly detected:You’ve seen most core features. They are supported by the “Document OCR” general processor as well as the other processors, which leverage these features to focus on more specific document types and provide additional information. Let’s check the next-level processor: the “Form Parser” processor.Form fieldsThe “Form Parser” processor lets you detect form fields. A form field is the combination of a field name and a field value, also called a key-value pair.In this example, printed and handwritten text is detected as seen before:In addition, the form parser returns a list of form_fields:Here is how the detected key-value pairs are returned:And here are their detected bounding boxes:Note: Form fields can follow flexible layouts. In this example, keys and values are in a left-right order. You’ll see a right-left example next. Those are just simple arbitrary examples. It also works with vertical or free layouts where keys and values are logically (visually) related.CheckboxesThe form parser also detects checkboxes. A checkbox is actually a particular form field value.This example is a French exam with affirmations that should be checked when exact. To test this, I used checkboxes of different kinds, printed or handmade. All form fields are detected, with the affirmations as field names and the corresponding checkboxes as field values:When a checkbox is detected, the form field contains an additional value_type field, which value is either unfilled_checkbox or filled_checkbox:Being able to analyze forms can lead to huge time savings, by consolidating — or even autoprocessing — content for you. The preceding checkbox detection example was actually an evolution of a prior experiment to autocorrect my wife’s pile of exam copies. The proof of concept got better using checkboxes, but was already conclusive enough with True/False handwritten answers. Here is how it can autocorrect and autograde:TablesThe form parser can also detect another important structural element: tables.In this example, words are presented in a tabular layout without any borders. The form parser finds a table very close to the (hidden) layout. Here are the detected cells:In this other example, some cells are filled with text while others are blank. There are enough signals for the form parser to detect a tabular structure:When tables are detected, the form parser returns a list of tables with their rows and cells. Here is how the table is returned:And here is the first cell:Specialized processorsSpecialized processors focus on domain-specific documents and extract entities. They cover many different document types that can currently be classified in the following families:Procurement — receipts, invoices, utility bills, purchase orders,…Lending — bank statements, pay slips, official forms,…Identity — national IDs, driver licenses, passports,…Contract — legal agreementsFor example, procurement processors typically detect the total_amount and currency entities:For more information, check out Fields detected.ExpensesThe “Expense Parser” lets you process receipts of various types. Let’s analyze this actual (French) receipt:A few remarks:All expected entities are extracted.Ideally (to be picky), I’d like to get the tax rate too. If there’s customer demand for it, this may be a supported entity in a future version.Due to the receipt’s very thin paper, the text on the back side is visible by transparency (another type of noise).The supplier name is wrong (it’s actually mirrored text from the back side) but with a very low confidence (4%). You’d typically handle it with special care (it’s shown differently here) or ignore it.The actual supplier info is hidden (the top part of the receipt is folded) on purpose. You’ll see another example with supplier data a bit later.Receipts are often printed on single (sometimes on multiple) pages. The expense parser supports analyzing expense documents of up to 10 pages.Procurement documents often list parts of the data in tabular layouts. Here, they’re returned as many line_item/* entities. When the entities are detected as part of a hierarchical structure, the results are nested in the properties field of a parent entity, providing an additional level of information. Here's an excerpt:For more information, see the Expense Parser details.Entity normalizationGetting results is generally not enough. Results often need to be handled in a post-processing stage, which can be both time consuming and a source of errors. To address this, specialized processors also return normalized values when possible. This lets you directly use standard values consolidated from the context of the whole document.Let’s check it with this other receipt:First, the receipt currency is returned with its standard code under normalized_value:Then, the receipt is dated 11/12/2022. But is it Nov. 12 or Dec. 11? Document AI uses the context of the document (a French receipt) and provides a normalized value that removes all ambiguity:Likewise, the receipt contains a purchase time, written in a non-standard way. The result also includes a canonical value that avoids any interpretation:Normalized values simplify the post-processing stage:They provide standard values that are straightforward to use (e.g. enable direct storage in a data warehouse).They prevent bugs (esp. the recurring developer mistakes we make when converting data).They remove ambiguity and avoid incorrect interpretations by using the context of the whole document.For more information, check out the NormalizedValue structure.Entity enrichmentDid you notice there was more information in the receipt?“Maison Jeanne d’Arc, Place de Gaulle” mentions a Joan of Arc’s House and a place name. Nonetheless, there is no address, zip code, city, 

== Article 6
* Title: 'Automate identity document processing with Document AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 02 Jun 2022 12:32:10 GMT'
* Categories: technology, machine-learning, ai, document-management, programming

How many times have you filled out forms requesting personal information? It’s probably too many times to count. When online and signed in, you can save a lot of time thanks to your browser’s autofill feature. In other cases, you often have to provide the same data manually, again and again. The first Document AI identity processors are now generally available and can help you solve this problem.In this post, you’ll see how to…Process identity documents with Document AICreate your own identity form autofillerUse casesHere are a few situations that you’ve probably encountered:Financial accounts: Companies need to validate the identity of individuals. When creating a customer account, you need to present a government-issued ID for manual validation.Transportation networks: To handle subscriptions, operators often manage fleets of custom identity-like cards. These cards are used for in-person validation, and they require an ID photo.Identity gates: When crossing a border (or even when flying domestically), you need to pass an identity check. The main gates have streamlined processes and are generally well equipped to scale with the traffic. On the contrary, smaller gates along borders can have manual processes — sometimes on the way in and the way out — which can lead to long lines and delays.Hotels: When traveling abroad and checking in, you often need to show your passport for a scan. Sometimes, you also need to fill out a longer paper form and write down the same data.Customer benefits: For benefit certificates or loyalty cards, you generally have to provide personal info, which can include a portrait photo.In these examples, the requested info — including the portrait photo — is already on your identity document. Moreover, an official authority has already validated it. Checking or retrieving the data directly from this source of truth would not only make processes faster and more effective, but also remove a lot of friction for end users.Identity processorsProcessor typesEach Document AI identity processor is a machine learning model trained to extract information from a standard ID document such as:Driver licenseNational IDPassportNote: an ID can have information on both sides, so identity processors support up to two pages per document.AvailabilityGenerally available as of June 2022, you can use two US identity processors in production:Currently available in Preview:The Identity Doc Fraud Detector, to check whether an ID document has been tampered withThree French identity processorsNotes:More identity processors are in the pipe.To request access to processors in Preview, please fill out the Access Request Form.Processor creationYou can create a processor:Manually from Cloud Console (web admin UI)Programmatically with the APIProcessors are location-based. This helps guarantee where processing will occur for each processor.Here are the current multi-region locations:Once you’ve created a processor, you reference it with its ID (PROCESSOR_ID hereafter).Note: To manage processors programmatically, see the codelab Managing Document AI processors with Python.Document processingYou can process documents in two ways:Synchronously with an online request, to analyze a single document and directly use the resultsAsynchronously with a batch request, to launch a batch processing operation on multiple or larger documentsOnline requestsExample of a REST online request:The method is named process.The input document here is a PNG image (base64 encoded).This request is processed in the European Union.The response is returned synchronously.Batch requestsExample of a REST batch request:The method is named batchProcess.The batchProcess method launches the batch processing of multiple documents.This request is processed in the United States.The response is returned asynchronously; output files will be stored under my-storage-bucket/output/.InterfacesDocument AI is available through the usual Google Cloud interfaces:The RPC API (low-latency gRPC)The REST API (JSON requests and responses)Client libraries (gRPC wrappers, currently available for Python, Node.js, and Java)Cloud Console (web admin UI)Note: With the client libraries, you can develop in your preferred programming language. You’ll see an example later in this post.Identity fieldsA typical REST response looks like the following:The text and pages fields include the OCR data detected by the underlying ML models. This part is common to all Document AI processors.The entities list contains the fields specifically detected by the identity processor.Here are the detectable identity fields:Please note that Address and MRZ Code are optional fields. For example, a US passport contains an MRZ but no address.Fraud detectionAvailable in preview, the Identity Doc Fraud Detector helps detect tampering attempts. Typically, when an identity document does not “pass” the fraud detector, your automated process can block the attempt or trigger a human validation.Here is an example of signals returned:Sample demoYou can process a document live with just a few lines of code.Here is a Python example:This function uses the Python client library:The input is a file (any format supported by the processor).client is an API wrapper (configured for processing to take place in the desired location).process_document calls the API process method, which returns results in seconds.The output is a structured Document.You can collect the detected fields by parsing the document entities:Note: This function builds a mapping ready to be sent to a frontend. A similar function can be used for other specialized processors.Finalize your app:Define your user experience and architectureImplement your backend and its APIImplement your frontend with a mix of HTML + CSS + JSAdd a couple of features: file uploads, document samples, or webcam capturesThat’s it; you’ve built an identity form autofillerHere is a sample web app in action:Here is the processing of a French national ID, dropping images from the client:Note: For documents with multiple pages, you can use a PDF or TIFF container. In this example, the two uploaded PNG images are merged by the backend and processed as a TIFF file.And this is the processing of a US driver license, captured with a laptop 720p webcam:Notes:Did you notice that the webcam capture is skewed and the detected portrait image straight? That’s because Document AI automatically deskews the input at the page level. Documents can even be upside down.Some fields (such as the dates) are returned with their normalized values. This can make storing and processing these values a lot easier — and less error-prone — for developers.The source code for this demo is available in our Document AI sample repository.MoreCheck out the official announcementTry Document AI in your browserDocument AI documentationDocument AI how-to guidesSending a processing requestFull processor and detail listRelease notesCodelab — Specialized processors with Document AICode — Document AI samplesFor more cloud content, follow me on Twitter (@PicardParis) or LinkedIn (in/PicardParis), and feel free to get in touch with any feedback or questions.Stay tuned; the family of Document AI processors keeps growing and growing.Originally published on the Google Cloud Blog on June 1, 2022.Automate identity document processing with Document AI was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 7
* Title: 'Deploy a coloring page generator in minutes with Cloud Run'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 07 Apr 2022 07:56:59 GMT'
* Categories: gcp-app-dev, technology, programming, image-processing, python

👋 HelloHave you ever written a script to transform an image? Did you share the script with others or did you run it on multiple computers? How many times did you need to update the script or the setup instructions? Did you end up making it a service or an online app? If your script is useful, you’ll likely want to make it available to others. Deploying processing services is a recurring need — one that comes with its own set of challenges. Serverless technologies let you solve these challenges easily and efficiently.In this post, you’ll see how to…Create an image processing service that generates coloring pagesMake it available online using minimal resources…and do it all in less than 200 lines of Python and JavaScript!🛠️ ToolsTo build and deploy a coloring page generator, you’ll need a few tools:A library to process imagesA web application frameworkA web serverA serverless solution to make the demo available 24/7🧱 ArchitectureHere is one possible architecture for a coloring page generator using Cloud Run:And here is the workflow:1. The user opens the web app: the browser requests the main page.2. Cloud Run serves the app HTML code.3. The browser requests the additional needed resources.4. Cloud Run serves the CSS, JavaScript, and other resources.A. The user selects an image and the frontend sends the image to the /api/coloring-page endpoint.B. The backend processes the input image and returns an output image, which the user can then visualize, download, or print via the browser.🐍 Software stackOf course, there are many different software stacks that you could use to implement such an architecture.Here is a good one based on Python:It includes:Gunicorn: A production-grade WSGI HTTP serverFlask: A popular web app frameworkscikit-image: An extensive image processing libraryDefine these app dependencies in a file named requirements.txt:🎨 Image processingHow do you remove colors from an image? One way is by detecting the object edges and removing everything but the edges in the result image. This can be done with a Sobel filter, a convolution filter that detects the regions in which the image intensity changes the most.Create a Python file named main.py, define an image processing function, and within it use the Sobel filter and other functions from scikit-image:Note: The NumPy and Pillow libraries are automatically installed as dependencies of scikit-image.As an example, here is how the Cloud Run logo is processed at each step:✨ Web appBackendTo expose both endpoints ( GET / and POST /api/coloring-page), add Flask routes in main.py:FrontendOn the browser side, write a JavaScript function that calls the /api/coloring-page endpoint and receives the processed image:The base of your app is there. Now you just need to add a mix of HTML + CSS + JS to complete the desired user experience.Local developmentTo develop and test the app on your computer, once your environment is set up, make sure you have the needed dependencies:Add the following block to main.py. It will only execute when you run your app manually:Run your app:Flask starts a local web server:Note: In this mode, you’re using a development web server (one that is not suited for production). You’ll next set up the deployment to serve your app with Gunicorn, a production-grade server.You’re all set. Open localhost:8080 in your browser, test, refine, and iterate.🚀 DeploymentOnce your app is ready for prime time, you can define how it will be served with this single line in a file named Procfile:At this stage, here are the files found in a typical project:That’s it, you can now deploy your app from the source folder:⚙️ Under the hoodThe command line output details all the different steps:Cloud Build is indirectly called to containerize your app. One of its core components is Google Cloud Buildpacks, which automatically builds a production-ready container image from your source code. Here are the main steps:Cloud Build fetches the source code.Buildpacks autodetects the app language (Python, in this case) and uses the corresponding secure base image.Buildpacks installs the app dependencies (defined in requirements.txt for Python).Buildpacks configures the service entrypoint (defined in Procfile for Python).Cloud Build pushes the container image to Artifact Registry.Cloud Run creates a new revision of the service based on this container image.Cloud Run routes production traffic to it.Notes:- Buildpacks currently supports the following runtimes: Go, Java, .NET, Node.js, and Python.- The base image is actively maintained by Google, scanned for security vulnerabilities, and patched against known issues. This means that, when you deploy an update, your service is based on an image that is as secure as possible.- If you need to build your own container image, for example with a custom runtime, you can add your own Dockerfile and Buildpacks will use it instead.💫 UpdatesMore testing from real-life users shows some issues.First, the app does not handle pictures taken with digital cameras in non-native orientations. You can fix this using the EXIF orientation data:In addition, the app is too sensitive to details in the input image. Textures in paintings, or noise in pictures, can generate many edges in the processed image. You can improve the processing algorithm by adding a denoising step upfront:This additional step makes the coloring page cleaner and reduces the quantity of ink used if you print it:Redeploy, and the app is automatically updated:🎉 It’s aliveThe app is visible as a service in Cloud Run:The service dashboard gives you an overview of app usage:That’s it; your image processing app is in production!🤯 It’s serverlessThere are many benefits to using Cloud Run in this architecture:Your app is available 24/7.The environment is fully managed: you can focus on your code and not worry about the infrastructure.Your app is automatically available through HTTPS.You can map your app to a custom domain.Cloud Run scales the number of instances automatically and the billing includes only the resources used when your code runs.If your app is not used, Cloud Run scales down to zero.If your app gets more traffic (imagine it makes the news), Cloud Run scales up to the number of instances needed.You can control performance and cost by fine-tuning many settings: CPU, memory, concurrency, minimum instances, maximum instances, and more.Every month, the free tier offers the first 50 vCPU-hours, 100 GiB-hours, and 2 million requests for no cost.💾 Source codeThe project includes just seven files and less than 200 lines of Python + JavaScript code.You can reuse this demo as a base to build your own image processing app:Check out the source code on GitHub.For step-by-step instructions on deploying the app yourself in a few minutes, see “Deploying from scratch”.🖖 MoreTry the demo and generate your own coloring pages.Learn more about Cloud Run.For more cloud content, follow me on Twitter (@PicardParis) or LinkedIn (in/PicardParis), and feel free to get in touch with any feedback or questions.📜 Also in this seriesSummarizing videosTracking video objectsFace detection and processingProcessing imagesOriginally published on the Google Cloud Blog on April 5, 2022.Deploy a coloring page generator in minutes with Cloud Run was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Face detection and processing in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 29 Sep 2020 14:19:14 GMT'
* Categories: programming, machine-learning, gcp-app-dev, python, technology

⏳ 2021–10–08 updateUpdated GitHub version with latest library versions + Python 3.7 → 3.9👋 Hello!In this article, you’ll see the following:how to detect faces in pictures,how to automatically anonymize, crop,… a picture with faces,how to make this a serverless online demo,in less than 300 lines of Python code.Here is a famous face that has been automatically anonymized and cropped.Do you guess who this is?Note: We’re talking about face detection, not face recognition. Though technically possible, face recognition can have harmful applications. Responsible companies have established AI principles and avoid exposing such potentially harmful technologies (e.g. Google AI Principles).🛠️ ToolsA few tools will do:a machine learning model to analyze images,a library to process images,a web application framework,a serverless solution to keep the demo available 24/7 and at minimal cost.🧱 ArchitectureHere is an architecture using 2 Google Cloud services (App Engine + Vision API):The workflow is the following:Open the demo: App Engine serves the home page.Take a selfie: the frontend sends it to the /analyze-image endpoint.The backend sends a request to the Vision API: the image is analyzed and the results (annotations) are returned.The backend returns the annotations, in addition to the number of detected faces (to display the info directly in the web page).The frontend sends image, annotations, and processing options to the /process-image endpoint.The backend processes the image with the given options and returns the result image.Change the options: steps 5 and 6 are repeated.Get the image with new options.This is one of many possible architectures. The advantages of this one are the following:The web browser caches both the selfie and the annotations: no storage is involved and no private images are stored anywhere in the cloud.The Vision API is only called once per image.🐍 Python librariesGoogle Cloud VisionThe client library that wraps calls to the Vision API.https://pypi.org/project/google-cloud-visionPillowA very popular imaging library, both extensive and easy to use.https://pypi.org/project/PillowFlaskOne of the most popular web app frameworks.https://pypi.org/project/FlaskDependenciesDefine the dependencies in the requirements.txt file:google-cloud-vision==1.0.0Pillow==7.2.0Flask==1.1.2Notes:- As a best practice, also specify the dependency versions. This freezes your production environment in a known state and prevents newer versions from potentially breaking future deployments.- App Engine will automatically deploy these dependencies.🧠 Image analysisVision APIThe Vision API gives access to state-of-the-art machine learning models for image analysis. One of the multiple features is face detection. Here is a way to detect faces in an image:https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/hrefBackend endpointExposing an API endpoint with Flask consists in wrapping a function with a route. Here is a possible POST endpoint:https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/hrefFrontend requestHere is a javascript function to call the API from the frontend:https://medium.com/media/80380594bf727837b452b5a8d62a8242/href🎨 Image processingFace bounding box and landmarksThe Vision API provides the bounding box of the detected faces and the position of 30+ face landmarks (mouth, nose, eyes,…). Here is a way to visualize them with Pillow (PIL):https://medium.com/media/9875815330881f6d59e489872744d4ab/hrefAmerican Gothic (Wikimedia)Face anonymizationHere is way to anonymize the faces thanks to the bounding boxes:https://medium.com/media/8ed328faf40629175a2bd873aeda793a/hrefAmerican Gothic (Wikimedia)Face croppingSimilarly, to focus on the detected faces, you can crop everything around the faces:https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/hrefAmerican Gothic (Wikimedia)🍒 Cherry on Py 🐍Now, the icing on the cake (or the “cherry on the pie” as we say in French):Having independent rendering functions lets you combine multiple options at once.Knowing the bounding box for all faces allows cropping the image to the minimal bounding box.Using the location of the nose and the mouth, you can add a moustache to everyone.If your functions have parameters to render a single frame, you can generate animations with a few lines of code.Once your Flask app works locally, you can deploy and keep it available 24/7 at minimal cost.Here is what’s detected on famous photorealistic paintings:American Gothic (Wikimedia)Girl with a Pearl Earring (Wikimedia)Shakespeare (Wikimedia)Here are some animated versions:American Gothic (Wikimedia)Girl with a Pearl Earring (Wikimedia)Shakespeare (Wikimedia)Note: animations are a bit degraded (GIF version) as Medium does not support animated PNGs. The demo below lets you generate them in GIF, PNG, and WebP.And, of course, this works even better on real pictures:Personal pictures (aged from 2 to 44)Yes, I’ve had a moustache for over 42 years, and my sister too ;)And, finally, here is our famous anonymous from the beginning:Mona Lisa (Wikimedia)🚀 Source code and deploymentSource codeThe Python code for the backend takes less than 300 lines of code.See the source on GitHub.DeploymentYou can deploy this demo in 4 minutes.See “Deploying from scratch”.🎉 Online demoTry the demo by yourself:➡️ https://face-detection.lolo.dev ⬅️https://face-detection.lolo.dev🖖 See youFeedback, questions? I’d love to read from you! Follow me on Twitter for more…⏳ Updates2021–10–08: Updated GitHub version with latest library versions + Python 3.7 → 3.9📜 Also in this seriesSummarizing videosTracking video objectsFace detection and processingProcessing imagesFace detection and processing in 300 lines of code was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'Tracking video objects in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 25 Jun 2020 17:46:01 GMT'
* Categories: python, gcp-app-dev, technology, programming, machine-learning

⏳ 2021–10–08 updateUpdated GitHub version with latest library versions + Python 3.7 → 3.9👋 Hello!In this article, you’ll see the following:how to track objects present in a video,with an automated processing pipeline,in less than 300 lines of Python code.Here is an example of an auto-generated object summary for the video &lt;animals.mp4&gt;:🛠️ ToolsA few tools will do:Storage space for videos and resultsA serverless solution to run the codeA machine learning model to analyze videosA library to extract frames from videosA library to render the objects🧱 ArchitectureHere is a possible architecture using 3 Google Cloud services (Cloud Storage, Cloud Functions, and the Video Intelligence API):The processing pipeline follows these steps:You upload a videoThe upload event automatically triggers the tracking functionThe function sends a request to the Video Intelligence APIThe Video Intelligence API analyzes the video and uploads the results (annotations)The upload event triggers the rendering functionThe function downloads both annotation and video filesThe function renders and uploads the objectsYou know which objects are present in your video!🐍 Python librariesVideo Intelligence APITo analyze videoshttps://pypi.org/project/google-cloud-videointelligenceCloud StorageTo manage downloads and uploadshttps://pypi.org/project/google-cloud-storageOpenCVTo extract video framesOpenCV offers a headless version (without GUI features, ideal for a service)https://pypi.org/project/opencv-python-headlessPillowTo render and annotate object imagesPillow is a very popular imaging library, both extensive and easy to usehttps://pypi.org/project/Pillow🧠 Video analysisVideo Intelligence APIThe Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of its multiple features is detecting and tracking objects. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the OBJECT_TRACKING feature:https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/hrefCloud Function entry pointhttps://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/hrefNotes:• This function will be called when a video is uploaded to the bucket defined as a trigger.• Using an environment variable makes the code more portable and lets you deploy the exact same code with different trigger and output buckets.🎨 Object renderingCode structureIt’s interesting to split the code into 2 main classes:StorageHelper for managing local files and cloud storage objectsVideoProcessor for all graphical processingsHere is a possible core function for the 2nd Cloud Function:https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/hrefCloud Function entry pointhttps://medium.com/media/43f8e86165e95433fed65f48610591b0/hrefNote: This function will be called when an annotation file is uploaded to the bucket defined as a trigger.Frame renderingOpenCV and Pillow easily let you extract video frames and compose over them:https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/hrefNote: It would probably be possible to only use OpenCV but I found it more productive developing with Pillow (code is more readable and intuitive).🔎 ResultsHere are the main objects found in the video &lt;JaneGoodall.mp4&gt;:Notes:• The machine learning model has correctly identified different wildlife species: those are “true positives”. It has also incorrectly identified our planet as “packaged goods”: this is a “false positive”. Machine learning models keep learning by being trained with new samples so, with time, their precision keeps increasing (resulting in less false positives).• The current code filters out objects detected with a confidence below 70% or with less than 10 frames. Lower the thresholds to get more results.🍒 Cherry on Py 🐍Now, the icing on the cake (or the “cherry on the pie” as we say in French), you can enrich the architecture to add new possibilities:Trigger the processing for videos from any bucket (including external public buckets)Generate individual object animations (in parallel to object summaries)Architecture (v2)A — Video object tracking can also be triggered manually with an HTTP GET requestB — The same rendering code is deployed in 2 sibling functions, differentiated with an environment variableC — Object summaries and animations are generated in parallelCloud Function HTTP entry pointhttps://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/hrefNote: This is the same code as gcf_track_objects() with the video URI parameter specified by the caller through a GET request.🎉 ResultsHere are some auto-generated trackings for the video &lt;animals.mp4&gt;:The left elephant (a big object ;) is detected:The right elephant is perfectly isolated too:The veterinarian is correctly identified:The animal he’s feeding too:Moving objects or static objects in moving shots are tracked too, as in &lt;beyond-the-map-rio.mp4&gt;:A building in a moving shot:Neighbor buildings are tracked too:Persons in a moving shot:A surfer crossing the shot:Here are some others for the video &lt;JaneGoodall.mp4&gt;:A butterfly (easy?):An insect, in larval stage, climbing a moving twig:An ape in a tree far away (hard?):A monkey jumping from the top of a tree (harder?):Now, a trap! If we can be fooled, current machine learning state of the art can too:🚀 Source code and deploymentSource codeThe Python source is less than 300 lines of code.See the source on GitHub.DeploymentYou can deploy this architecture in less than 8 minutes.See “Deploying from scratch”.🖖 See youDo you want more, do you have questions? I’d love to read your feedback. You can also follow me on Twitter.⏳ Updates2021–10–08: Updated GitHub version with latest library versions + Python 3.7 → 3.9📜 Also in this seriesSummarizing videosTracking video objectsFace detection and processingProcessing imagesTracking video objects in 300 lines of code was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Summarizing videos in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=rss-6be63961431c------2'
* PublicationDate: 'Sat, 30 May 2020 13:54:30 GMT'
* Categories: gcp-app-dev, technology, programming, python, machine-learning

⏳ 2021–10–08 updateUpdated GitHub version with latest library versions + Python 3.7 → 3.9👋 Hello!Dear developers,Do you like the adage “a picture is worth a thousand words”? I do! Let’s check if it also works for “a picture is worth a thousand frames”.In this tutorial, you’ll see the following:how to understand the content of a video in a blink,in less than 300 lines of Python (3.7) code.A visual summary generated from a 2'42" video made of 35 sequences (shots). The summary is a grid where each cell is a frame representing a video shot.🔭 ObjectivesThis tutorial has 2 objectives, 1 practical and 1 technical:Automatically generate visual summaries of videosBuild a processing pipeline with these properties:managed (always ready and easy to set up)scalable (able to ingest several videos in parallel)not costing anything when not used🛠️ ToolsA few tools are enough:Storage space for videos and resultsA serverless solution to run the codeA machine learning model to analyze videosA library to extract frames from videosA library to generate the visual summaries🧱 ArchitectureHere is a possible architecture using 3 Google Cloud services (Cloud Storage, Cloud Functions, and Video Intelligence API):The processing pipeline follows these steps:You upload a video to the 1st bucket (a bucket is a storage space in the cloud)The upload event automatically triggers the 1st functionThe function sends a request to the Video Intelligence API to detect the shotsThe Video Intelligence API analyzes the video and uploads the results (annotations) to the 2nd bucketThe upload event triggers the 2nd functionThe function downloads both annotation and video filesThe function renders and uploads the summary to the 3rd bucketThe video summary is ready!🐍 Python librariesOpen source client libraries let you interface with Google Cloud services in idiomatic Python. You’ll use the following:Cloud StorageTo manage downloads and uploadshttps://pypi.org/project/google-cloud-storageVideo Intelligence APITo analyze videoshttps://pypi.org/project/google-cloud-videointelligenceHere is a choice of 2 additional Python libraries for the graphical needs:OpenCVTo extract video framesThere’s even a headless version (without GUI features), which is ideal for a servicehttps://pypi.org/project/opencv-python-headlessPillowTo generate the visual summariesPillow is a very popular imaging library, both extensive and easy to usehttps://pypi.org/project/Pillow⚙️ Project setupAssuming you have a Google Cloud account, you can set up the architecture from Cloud Shell with the gcloud and gsutil commands. This lets you script everything from scratch in a reproducible way.Environment variables# ProjectPROJECT_NAME="Visual Summary"PROJECT_ID="visual-summary-REPLACE_WITH_UNIQUE_SUFFIX"# Cloud Storage region (https://cloud.google.com/storage/docs/locations)GCS_REGION="europe-west1"# Cloud Functions region (https://cloud.google.com/functions/docs/locations)GCF_REGION="europe-west1"# SourceGIT_REPO="cherry-on-py"PROJECT_SRC=~/$PROJECT_ID/$GIT_REPO/gcf_video_summary# Cloud Storage buckets (environment variables)export VIDEO_BUCKET="b1-videos_${PROJECT_ID}"export ANNOTATION_BUCKET="b2-annotations_${PROJECT_ID}"export SUMMARY_BUCKET="b3-summaries_${PROJECT_ID}"Note: You can use your GitHub username as a unique suffix.New projectgcloud projects create $PROJECT_ID \  --name="$PROJECT_NAME" \  --set-as-defaultCreate in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/PROJECT_ID].Waiting for [operations/cp...] to finish...done.Enabling service [cloudapis.googleapis.com] on project [PROJECT_ID]...Operation "operations/acf..." finished successfully.Updated property [core/project] to [PROJECT_ID].Billing account# Link project with billing account (single account)BILLING_ACCOUNT=$(gcloud beta billing accounts list \    --format 'value(name)')# Link project with billing account (specific one among multiple accounts)BILLING_ACCOUNT=$(gcloud beta billing accounts list  \    --format 'value(name)' \    --filter "displayName='My Billing Account'")gcloud beta billing projects link $PROJECT_ID --billing-account $BILLING_ACCOUNTbillingAccountName: billingAccounts/XXXXXX-YYYYYY-ZZZZZZbillingEnabled: truename: projects/PROJECT_ID/billingInfoprojectId: PROJECT_IDBuckets# Create buckets with uniform bucket-level accessgsutil mb -b on -c regional -l $GCS_REGION gs://$VIDEO_BUCKETgsutil mb -b on -c regional -l $GCS_REGION gs://$ANNOTATION_BUCKETgsutil mb -b on -c regional -l $GCS_REGION gs://$SUMMARY_BUCKETCreating gs://VIDEO_BUCKET/...Creating gs://ANNOTATION_BUCKET/...Creating gs://SUMMARY_BUCKET/...You can check how it looks like in the Cloud Console:Service accountCreate a service account. This is for development purposes only (not needed for production). This provides you with credentials to run your code locally.mkdir ~/$PROJECT_IDcd ~/$PROJECT_IDSERVICE_ACCOUNT_NAME="dev-service-account"SERVICE_ACCOUNT="${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com"gcloud iam service-accounts create $SERVICE_ACCOUNT_NAMEgcloud iam service-accounts keys create ~/$PROJECT_ID/key.json --iam-account $SERVICE_ACCOUNTCreated service account [SERVICE_ACCOUNT_NAME].created key [...] of type [json] as [~/PROJECT_ID/key.json] for [SERVICE_ACCOUNT]Set the GOOGLE_APPLICATION_CREDENTIALS environment variable and check that it points to the service account key. When you run the application code in the current shell session, client libraries will use these credentials for authentication. If you open a new shell session, set the variable again.export GOOGLE_APPLICATION_CREDENTIALS=~/$PROJECT_ID/key.jsoncat $GOOGLE_APPLICATION_CREDENTIALS{  "type": "service_account",  "project_id": "PROJECT_ID",  "private_key_id": "...",  "private_key": "-----BEGIN PRIVATE KEY-----\n...",  "client_email": "SERVICE_ACCOUNT",  ...}Authorize the service account to access the buckets:IAM_BINDING="serviceAccount:${SERVICE_ACCOUNT}:roles/storage.objectAdmin"gsutil iam ch $IAM_BINDING gs://$VIDEO_BUCKETgsutil iam ch $IAM_BINDING gs://$ANNOTATION_BUCKETgsutil iam ch $IAM_BINDING gs://$SUMMARY_BUCKETAPIsA few APIs are enabled by default:gcloud services listNAME                              TITLEbigquery.googleapis.com           BigQuery APIbigquerystorage.googleapis.com    BigQuery Storage APIcloudapis.googleapis.com          Google Cloud APIsclouddebugger.googleapis.com      Cloud Debugger APIcloudtrace.googleapis.com         Cloud Trace APIdatastore.googleapis.com          Cloud Datastore APIlogging.googleapis.com            Cloud Logging APImonitoring.googleapis.com         Cloud Monitoring APIservicemanagement.googleapis.com  Service Management APIserviceusage.googleapis.com       Service Usage APIsql-component.googleapis.com      Cloud SQLstorage-api.googleapis.com        Google Cloud Storage JSON APIstorage-component.googleapis.com  Cloud StorageEnable the Video Intelligence and Cloud Functions APIs:gcloud services enable \  videointelligence.googleapis.com \  cloudfunctions.googleapis.comOperation "operations/acf..." finished successfully.Source codeRetrieve the source code:cd ~/$PROJECT_IDgit clone https://github.com/PicardParis/$GIT_REPO.gitCloning into 'GIT_REPO'......🧠 Video analysisVideo shot detectionThe Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of the multiple features is video shot detection. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the SHOT_CHANGE_DETECTION feature:https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/hrefLocal development and testsBefore deploying the function, you need to develop and test it. Create a Python 3 virtual environment and activate it:cd ~/$PROJECT_IDpython3 -m venv venvsource venv/bin/activateInstall the dependencies:pip install -r $PROJECT_SRC/gcf1_detect_shots/requirements.txtCheck the dependencies:pip listPackage                        Version------------------------------ ----------...google-cloud-storage           1.28.1google-cloud-videointelligence 1.14.0...You can use the main scope to test the function in script mode:https://medium.com/media/a25fa778a61308c2584487ea157c1104/hrefNote: You have already exported the ANNOTATION_BUCKET environment variable earlier in the shell session; you will also define it later at deployment stage. This makes the code generic and lets you reuse it independently of the output bucket.Test the function:VIDEO_PATH="cloud-samples-data/video/gbikes_dinosaur.mp4"VIDEO_URI="gs://$VIDEO_PATH"python $PROJECT_SRC/gcf1_detect_shots/main.py $VIDEO_URILaunching shot detection for &lt;gs://cloud-samples-data/video/gbikes_dinosaur.mp4&gt;...Note: The test video &lt;gbikes_dinosaur.mp4&gt; is located in an external bucket. This works because the video is publicly accessible.Wait a moment and check that the annotations have been generated:gsutil ls -r gs://$ANNOTATION_BUCKET964  YYYY-MM-DDThh:mm:ssZ  gs://ANNOTATION_BUCKET/VIDEO_PATH.jsonTOTAL: 1 objects, 964 bytes (964 B)Check the last 200 bytes of the annotation file:gsutil cat -r -200 gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json}    }, {      "start_time_offset": {        "seconds": 28,        "nanos": 166666000      },      "end_time_offset": {        "seconds": 42,        "nanos": 766666000      }    } ]  } ]}Note: Those are the start and end positions of the last video shot. Everything seems fine.Clean up when you’re finished:gsutil rm gs://$ANNOTATION_BUCKET/$VIDEO_PATH.jsondeactivaterm -rf venvFunction entry pointhttps://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/hrefNote: This function will be called whenever a video is uploaded to the bucket defined as a trigger.Function deploymentDeploy the 1st function:GCF_NAME="gcf1_detect_shots"GCF_SOURCE="$PROJECT_SRC/gcf1_detect_shots"GCF_ENTRY_POINT="gcf_detect_shots"GCF_TRIGGER_BUCKET="$VIDEO_BUCKET"GCF_ENV_VARS="ANNOTATION_BUCKET=$ANNOTATION_BUCKET"GCF_MEMORY="128MB"gcloud functions deploy $GCF_NAME \  --runtime python37  \  --source $GCF_SOURCE \  --entry-point $GCF_ENTRY_POINT \  --update-env-vars $GCF_ENV_VARS \  --trigger-bucket $GCF_TRIGGER_BUCKET \  --region $GCF_REGION \  --memory $GCF_MEMORY \  --quietNote: The default memory allocated for a Cloud Function is 256 MB (possible values are 128MB, 256MB, 512MB, 1024MB, and 2048MB). As the function has no memory or CPU needs (it sends a simple API request), the minimum memory setting is enough.Deploying function (may take a while - up to 2 minutes)...done.availableMemoryMb: 128entryPoint: gcf_detect_shotsenvironmentVariables:  ANNOTATION_BUCKET: b2-annotations...eventTrigger:  eventType: google.storage.object.finalize...status: ACTIVEtimeout: 60supdateTime: 'YYYY-MM-DDThh:mm:ss.mmmZ'versionId: '1'Note: The ANNOTATION_BUCKET environment variable is defined with the --update-env-vars flag. Using an environment variable lets you deploy the exact same code with different trigger and output buckets.Here is how it looks like in the Cloud Console:Production testsMake sure to test the function in production. Copy a video into the video bucket:VIDEO_NAME="gbikes_dinosaur.mp4"SRC_URI="gs://cloud-samples-data/video/$VIDEO_NAME"DST_URI="gs://$VIDEO_BUCKET/$VIDEO_NAME"gsutil cp $SRC_URI $DST_URICopying gs://cloud-samples-data/video/gbikes_dinosaur.mp4 [Content-Type=video/mp4]...- [1 files][ 62.0 MiB/ 62.0 MiB]Operation completed over 1 objects/62.0 MiB.Query the logs to check that the function has been triggered:gcloud functions logs read --region $GCF_REGIONLEVEL  NAME               EXECUTION_ID  TIME_UTC  LOGD      gcf1_detect_shots  ...           ...       Function execution startedI      gcf1_detect_shots  ...           ...       Launching shot detection for &lt;gs://VIDEO_BUCKET/VIDEO_NAME&gt;...D      gcf1_detect_shots  ...           ...       Function execution took 874 ms, finished with status: 'ok'Wait a moment and check the annotation bucket:gsutil ls -r gs://$ANNOTATION_BUCKETYou should see the annotation file:gs://ANNOTATION_BUCKET/VIDEO_BUCKET/:gs://ANNOTATION_BUCKET/VIDEO_BUCKET/VIDEO_NAME.jsonThe 1st function is operational!🎞️ Visual SummaryCode structureIt’s interesting to split the code into 2 main classes:StorageHelper for local file and cloud storage object managementVideoProcessor for graphical processingsHere is a possible core function:https://medium.com/media/a93998518c177c34a6c86d85593fdb53/hrefNote: If exceptions are raised, it’s handy to log them with logging.exception() to get a stack trace in production logs.Class StorageHelperThe class manages the following:The retrieval and parsing of video shot annotationsThe download of source videosThe upload of generated visual summariesFile nameshttps://medium.com/media/c45a6726678a903e043c6329077814d5/hrefThe source video is handled in the with statement context manager:https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/hrefNote: Once downloaded, the video uses memory space in the /tmp RAM disk (the only writable space for the serverless function). It's best to delete temporary files when they're not needed anymore, to avoid potential out-of-memory errors on future invocations of the function.Annotations are retrieved with the methods storage.Blob.download_as_string() and json.loads():https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/hrefThe parsing is handled with this VideoShot helper class:https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/hrefVideo shot info can be exposed with a getter and a generator:https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/hrefThe naming convention was chosen to keep consistent object paths between the different buckets. This also lets you deduce the video path from the annotation URI:https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/hrefThe video is directly downloaded with storage.Blob.download_to_filename():https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/hrefOn the opposite, results can be uploaded with storage.Blob.upload_from_string():https://medium.com/media/bf5877b770412025188594002e5f61fa/hrefNote: from_string means from_bytes here (Python 2 legacy). Pillow supports working with memory images, which avoids having to manage local files.And finally, here is a possible naming convention for the summary files:https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/hrefClass VideoProcessorThe class manages the following:Video frame extractionVisual summary generationhttps://medium.com/media/a729b2940d13ac7e97016a08622b77b1/hrefOpening and closing the video is handled in the with statement context manager:https://medium.com/media/966117816753013e7647fc08971001d8/hrefThe video summary is a grid of cells which can be rendered in a single loop with two generators:https://medium.com/media/24491fd8f2a86539b574d4babf76100d/hrefNote: shot_ratio is set to 0.5 by default to extract video shot middle frames.The first generator yields cell images:https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/hrefThe second generator yields cell positions:https://medium.com/media/8a04df38e6b81926cf63889b8303847f/hrefOpenCV easily allows extracting video frames at a given position:https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/hrefChoosing the summary grid composition is arbitrary. Here is an example to compose a summary preserving the video proportions:https://medium.com/media/6c3961789a20e1e78337737448cd5daa/hrefFinally, Pillow gives full control on image serializations:https://medium.com/media/028535063c6ee49659586b1ad95dd163/hrefNote: Working with in-memory images avoids managing local files and uses less memory.Local development and testsYou can use the main scope to test the function in script mode:https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/hrefTest the function:cd ~/$PROJECT_IDpython3 -m venv venvsource venv/bin/activatepip install -r $PROJECT_SRC/gcf2_generate_summary/requirements.txtVIDEO_NAME="gbikes_dinosaur.mp4"ANNOTATION_URI="gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$VIDEO_NAME.json"python $PROJECT_SRC/gcf2_generate
