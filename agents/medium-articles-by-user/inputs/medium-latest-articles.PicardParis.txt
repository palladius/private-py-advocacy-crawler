
== Article 1
* Title: 'Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/making-ai-more-open-and-accessible-to-cloud-developers-with-gemma-on-vertex-ai-4b0fc2a14851?source=rss-6be63961431c------2'
* PublicationDate: 'Wed, 21 Feb 2024 18:04:07 GMT'
* Categories: gemma, vertex-ai, machine-learning, google-cloud-platform, llm

Gemma just openedÂ ;)Gemma is a family of open, lightweight, and easy-to-use models developed by Google Deepmind. The Gemma models are built from the same research and technology used to createÂ Gemini.This means that we (ML developers &amp; practitioners) now have additional versatile large language models (LLMs) in ourÂ toolbox!If youâ€™d rather read code, you can jump straight to this Python notebook:â†’ Finetune Gemma using KerasNLP and deploy to VertexÂ AIAvailabilityGemma is available today in Google Cloud and the machine learning (ML) ecosystem. Youâ€™ll probably find an ML platform youâ€™re already familiarÂ with:Gemma joins 130+ models in Vertex AI ModelÂ GardenGemma joins the KaggleÂ ModelsGemma joins the Hugging FaceÂ ModelsGemma joins Google AI for DevelopersHere are the Gemma Terms ofÂ Use.Gemma modelsThe Gemma family launches in two sizes, Gemma 2B and 7B, targeting two typical platformÂ types:| Model    | Parameters  | Platforms                           || -------- | ----------- | ----------------------------------- || Gemma 2B | 2.5 billion | Mobile devices and laptops          || Gemma 7B | 8.5 billion | Desktop computers and small servers |These models are text-to-text EnglishÂ LLMs:Input: a text string, such as a question, a prompt, or a document.Output: generated English text, such as an answer or aÂ summary.They were trained on a diverse and massive dataset of 8 trillion tokens, including web documents, source code, and mathematical text.Each size is available in untuned and tuned versions:Pretrained (untuned): Models were trained on core training data, not using any specific tasks or instructions.Instruction-tuned: Models were additionally trained on human language interactions.That gives us four variants. As an example, here are the corresponding model IDs when usingÂ Keras:- `gemma_2b_en`- `gemma_instruct_2b_en`- `gemma_7b_en`- `gemma_instruct_7b_en`Use casesGoogle now offers two families of LLMs: Gemini and Gemma. Gemma is a complement to Gemini, is based on technologies developed for Gemini, and addresses different useÂ cases.Examples of Gemini benefits:Enterprise applicationsMultilingual tasksOptimal qualitative results and complexÂ tasksState-of-the-art multimodality (text, image,Â video)Groundbreaking features (e.g. Gemini 1.5 Pro 1M token contextÂ window)Examples of Gemma benefits:Learning, research, or prototyping based on lightweight modelsFocused tasks such as text generation, summarization, andÂ Q&amp;AFramework or cross-device interoperabilityOffline or real-time text-only processingsThe Gemma model variants are useful in the following useÂ cases:Pretrained (untuned): Consider these models as a lightweight foundation and perform a custom finetuning to optimize them to your ownÂ needs.Instruction-tuned: You can use these models for conversational applications, such as a chatbot, or to customize them evenÂ further.InteroperabilityAs Gemma models are open, they are virtually interoperable with all ML platforms and frameworks.For the launch, Gemma is supported by the following ML ecosystem players:Google CloudKaggleKeras, which means that Gemma runs on JAX, PyTorch, and TensorFlowHugging FaceNvidiaAnd, of course, Gemma can run on GPUs andÂ TPUs.RequirementsGemma models are lightweight but, in the LLM world, this still means gigabytes.In my tests, when running inferences in half precision (bfloat16), here are the minimum storage and GPU memory that were required:| Model    | Total parameters | Assets size | Min. GPU RAM to run || -------- | ---------------: | ----------: | ------------------: || Gemma 2B |    2,506,172,416 |     4.67 GB |              8.3 GB || Gemma 7B |    8,537,680,896 |    15.90 GB |             20.7 GB |To experiment with Gemma and Vertex AI, I used Colab Enterprise with a g2-standard-8 runtime, which comes with an NVIDIA L4 GPU and 24 GB of GPU RAM. This is a cost-effective configuration to save time and avoid running out-of-memory when prototyping in a notebook.Finetuning GemmaDepending on your preferred frameworks, youâ€™ll find different ways to customize Gemma. Keras is one of them and provides everything youÂ need.KerasNLP lets you load a Gemma model in a singleÂ line:import kerasimport keras_nlpgemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma_instruct_2b_en")Then, you can directly generateÂ text:inputs = [    "What's the most famous painting by Monet?",    "Who engineered the Statue of Liberty?",    'Who were "The LumiÃ¨res"?',]for input in inputs:    response = gemma_lm.generate(input, max_length=25)    output = response[len(input) :]    print(f"{input!r}\n{output!r}\n")With the instruction-tuned version, Gemma gives you expected answers, as would a good LLM-based chatbot:# With "gemma_instruct_2b_en""What's the most famous painting by Monet?"'\n\nThe most famous painting by Monet is "Water Lilies," which''Who engineered the Statue of Liberty?''\n\nThe Statue of Liberty was built by French sculptor FrÃ©dÃ©ric Auguste Bartholdi between 1''Who were "The LumiÃ¨res"?''\n\nThe LumiÃ¨res were a group of French scientists and engineers who played a significant role'Now, try the untunedÂ version:gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma_2b_en")Youâ€™ll get different results, which is expected as this version was not trained on any specific task and is designed to be finetuned to your ownÂ needs:# With "gemma_2b_en""What's the most famous painting by Monet?""\n\nWhat's the most famous painting by Van Gogh?\n\nWhat"'Who engineered the Statue of Liberty?''\n\nA. George Washington\nB. Napoleon Bonaparte\nC. Robert Fulton\nD''Who were "The LumiÃ¨res"?'' What did they invent?\n\nIn the following sentence, underline the correct modifier from the'If youâ€™d like, for example, to build a Q&amp;A application, prompt engineering may fix some of these issues but, to be grounded on facts and return consistent results, untuned models need to be finetuned with trainingÂ data.The Gemma models have billions of trainable parameters. The next step consists of using the Low Rank Adaptation (LoRA) to greatly reduce the number of trainable parameters:# Number of trainable parameters before enabling LoRA: 2.5B# Enable LoRA for the model and set the LoRA rank to 4gemma_lm.backbone.enable_lora(rank=4)# Number of trainable parameters after enabling LoRA: 1.4M (1,800x less)A training can now be performed with reasonable time and GPU memory requirements.For prototyping on a small training dataset, you can launch a local finetuning:training_data: list[str] = [...]# Reduce the input sequence length to limit memory usagegemma_lm.preprocessor.sequence_length = 128# Use AdamW (a common optimizer for transformer models)optimizer = keras.optimizers.AdamW(    learning_rate=5e-5,    weight_decay=0.01,)# Exclude layernorm and bias terms from decayoptimizer.exclude_from_weight_decay(var_names=["bias", "scale"])gemma_lm.compile(    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),    optimizer=optimizer,    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],)gemma_lm.fit(training_data, epochs=1, batch_size=1)After a couple of minutes of training (on 1,000 examples) and using a structured prompt, the finetuned model can now answer questions based on your trainingÂ data:# With "gemma_2b_en" before finetuning'Who were "The LumiÃ¨res"?'' What did they invent?\n\nIn the following sentence, underline the correct modifier from the'# With "gemma_2b_en" after finetuning"""Instruction:Who were "The LumiÃ¨res"?Response:""""The LumiÃ¨res were the inventors of the first motion picture camera. They were"The prototyping stage is over. Before you proceed to finetuning models in production with large datasets, check out new specific tools to build a responsible Generative AI application.Responsible AIWith great power comes great responsibility, right?The Responsible Generative AI Toolkit will help you build safer AI applications with Gemma. Youâ€™ll find expert guidance on safety strategies for the different aspects of your solution.Also, do not miss the Learning Interpretability Tool (LIT) for visualizing and understanding the behavior of your Gemma models. Hereâ€™s the tool in action, investigating Gemmaâ€™s behavior:Deploying GemmaWeâ€™re in MLOps territory here and youâ€™ll find different LLM-optimized serving frameworks running on GPUs orÂ TPUs.Here are popular serving frameworks:vLLM (GPU)TGI: Text Generation Interface (GPU)Saxml (TPU orÂ GPU)TensorRT-LLM (NVIDIA TritonÂ GPU)These frameworks come with prebuilt container images that you can easily deploy to VertexÂ AI.Here is a simple notebook to get youÂ started:Finetune Gemma using KerasNLP and deploy to VertexÂ AIFor production finetuning, you can use Vertex AI custom training jobs. Here are detailed notebooks:Gemma Finetuning (served by vLLM orÂ HexLLM)Gemma Finetuning (served byÂ TGI)Those notebooks focus on deployment andÂ serving:Gemma Deployment (served by vLLM orÂ HexLLM)Gemma Deployment to GKE using TGI onÂ GPUFor more details and updates, check out the documentation:Vertex AIGKEAll the best to Gemini andÂ Gemma!After years of consolidation in machine learning hardware and software, itâ€™s exciting to see the pace at which the overall ML landscape now evolves and in particular with LLMs. LLMs are technologies still in their infancy, so we can expect to see more breakthroughs in the nearÂ future.I very much look forward to seeing what the ML community is going to build withÂ Gemma!All the best to the Gemini and Gemma families!Follow me on Twitter or LinkedIn for more cloud explorationsâ€¦Making AI more Open and Accessible to Cloud Developers with Gemma on Vertex AI was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 2
* Title: 'Moderating text with the Natural Language API'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/moderating-text-with-the-natural-language-api-5d379727da2c?source=rss-6be63961431c------2'
* PublicationDate: 'Fri, 16 Jun 2023 16:49:45 GMT'
* Categories: google-cloud-platform, moderation, nlp, machine-learning, data

Photo by Da Nina onÂ Unsplash2023â€“09â€“12: text moderation got Generally Available (GA) over the summer + added link to Sep. blogÂ postThe Natural Language API lets you extract information from unstructured text using Google machine learning and provides a solution to the following problems:Sentiment analysisEntity analysisEntity sentiment analysisSyntax analysisContent classificationText moderationğŸ” Moderation categoriesText moderation lets you detect sensitive or harmful content. The first moderation category that comes to mind is â€œtoxicityâ€, but there can be many more topics of interest. A PaLM 2-based model powers the predictions and scores 16 categories:| ---------- | --------------------- | ----------------- | -------------- || Toxic      | Insult                | Public Safety     | War &amp; Conflict || Derogatory | Profanity             | Health            | Finance        || Violent    | Death, Harm &amp; Tragedy | Religion &amp; Belief | Politics       || Sexual     | Firearms &amp; Weapons    | Illicit Drugs     | Legal          |âš¡ Moderating textLike always, you can call the API through the REST/RPC interfaces or with idiomatic client libraries.Here is an example using the Python client library (google-cloud-language) and the moderate_text method:from google.cloud import languagedef moderate_text(text: str) -&gt; language.ModerateTextResponse:    client = language.LanguageServiceClient()    document = language.Document(        content=text,        type_=language.Document.Type.PLAIN_TEXT,    )    return client.moderate_text(document=document)text = (    "I have to read Ulysses by James Joyce.\n"    "I'm a little over halfway through and I hate it.\n"    "What a pile of garbage!")response = moderate_text(text)ğŸš€ Itâ€™s fast! The model latency is very low, allowing real-time analyses.The response contains confidence scores for each moderation category. Letâ€™s sort themÂ out:import pandas as pddef confidence(category: language.ClassificationCategory) -&gt; float:    return category.confidencecolumns = ["category", "confidence"]categories = sorted(    response.moderation_categories,    key=confidence,    reverse=True,)data = ((category.name, category.confidence) for category in categories)df = pd.DataFrame(columns=columns, data=data)print(f"Text analyzed:\n{text}\n")print(f"Moderation categories:\n{df}")You may typically ignore scores below 50% and calibrate your solution by defining upper limits (or buckets) for the confidence scores. In this example, depending on your thresholds, you may flag the text as disrespectful (toxic) and insulting:Text analyzed:I have to read Ulysses by James Joyce.I'm a little over halfway through and I hate it.What a pile of garbage!Moderation categories:                 category  confidence0                   Toxic    0.6808731                  Insult    0.6094752               Profanity    0.4825163                 Violent    0.3333334                Politics    0.2377055   Death, Harm &amp; Tragedy    0.1897596                 Finance    0.1769557       Religion &amp; Belief    0.1510798                   Legal    0.1009469                  Health    0.09630510          Illicit Drugs    0.08333311     Firearms &amp; Weapons    0.07692312             Derogatory    0.07395313         War &amp; Conflict    0.05263214          Public Safety    0.05181315                 Sexual    0.028222ğŸ–– MoreTo try it, run this Colab notebook: Using the Natural LanguageÂ APISee the supported languagesRead more about text moderationSee the latest blog post on Improving Trust in AI and Online CommunitiesFollow me on Twitter or LinkedIn for more cloud explorationsModerating text with the Natural Language API was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'A Better Way to Use Google Cloud from Colab'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-colab/a-better-way-to-use-google-cloud-from-colab-bb93f88b5021?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 13 Jun 2023 18:23:54 GMT'
* Categories: jupyter-notebook, python, google-colab, google-cloud-platform, announcements

Photo by Pablo Arroyo onÂ UnsplashOne of the best things about Colab is low friction. You open a notebook and Colab knows who you are based on your current Google accountâ€Šâ€”â€Šthereâ€™s no need to separately login to Colab (unless youâ€™re not already logged intoÂ Google).Problem: Google Cloud provides an amazingly powerful array of services, which work great in a Colab notebook, but you need to separately authenticate yourself to Google Cloud. This is not difficult but, till now, many developers resorted to using a service account, which adds a bit of complexity and, even worse, can lead to accidentally checking service account credentials into a repo, which is a serious securityÂ concern.Solution: We simplified the flow for you so that you can now use your Google Cloud project from a Colab notebook in a more straightforward way and hopefully without thinking about creating a serviceÂ account.How does it work? In order to understand whatâ€™s new, letâ€™s do a before-after comparison. Here is what can be found in some notebooks using a service account and downloading a privateÂ key:# BEFOREimport osfrom google.colab import authauth.authenticate_user()PROJECT_ID = "YOUR_PROJECT_ID"SA_NAME = "YOUR_SERVICE_ACCOUNT_NAME"SA_EMAIL = f"{SA_NAME}@{PROJECT_ID}.iam.gserviceaccount.com"!gcloud config set project $PROJECT_ID!gcloud iam service-accounts create $SA_NAME!gcloud iam service-accounts keys create ./key.json --iam-account $SA_EMAILos.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "./key.json"This is not necessary and there are two potential problemsÂ here:A service account is created. If youâ€™re not the owner of the project, you may not have the permissions forÂ this.Service account credentials are manually downloaded at source code level. The private key can accidentally be pushed and madeÂ public.And hereâ€™s the simpler, service-account-less method:# AFTERfrom google.colab import authPROJECT_ID = "YOUR_PROJECT_ID"auth.authenticate_user(project_id=PROJECT_ID)You are authenticated for gcloud CLI commands (likeÂ before).You are also authenticated when using Google Cloud Python Client Libraries.Your default project isÂ set.The new version is shorter, simpler, and less error-prone.If youâ€™re new to Colab, would like to get more details or example notebooks, check out â€œUsing Google Cloud fromÂ Colabâ€.A Better Way to Use Google Cloud from Colab was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 4
* Title: 'Using Google Cloud from Colab'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/using-google-cloud-from-colab-75691f4a731?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 13 Jun 2023 16:31:33 GMT'
* Categories: machine-learning, google-colab, google-cloud-platform, python, data

Google Colab is an amazing tool for Pythonistas. It can be used for a variety of tasks, from quickly experimenting with Python code to sharing extensive data processing notebooks with the world. Colab runs on Google Cloud, which gives you a boost when accessing cloud services because your code is running inside Googleâ€™s high performance network, and also offers a simple way to use Google Cloud services, letting you run powerful cloud workloads from yourÂ browser.ğŸ“¦ï¸ Install dependenciesColab comes with hundreds of preinstalled packages, including pandas, numpy, matplotlib, Flask, Pillow, tensorflow, and pytorch. You can install additional required dependencies asÂ needed.For example, to analyze text with the power of machine learning using the Natural Language API, install the google-cloud-language clientÂ library:!pip install google-cloud-language&gt;=2.9.1ğŸ”‘ AuthenticateTo authenticate with your Google Cloud account within the Colab notebook, use the authenticate_user method. A new parameter lets you specify your projectÂ ID:from google.colab import authPROJECT_ID = ""  # @param {type:"string"}auth.authenticate_user(project_id=PROJECT_ID)After thisÂ step:ğŸ‘ You are authenticated for gcloud CLI commands.ğŸ‘ You are also authenticated when using Google Cloud Python client libraries. Note that you generally wonâ€™t need to create a serviceÂ account.ğŸ‘ Your default project isÂ set.ğŸ‘ Your notebook can also access your Google Drive, letting you ingest or generate your ownÂ files.ğŸ”“ EnableÂ APIsEnsure required APIs are enabled. In our example, thatâ€™s the service language.googleapis.com:!gcloud services enable language.googleapis.comğŸ¤¯ Use Google CloudÂ servicesThatâ€™s it! You can now directly use the service by calling its Python clientÂ library:from google.cloud import languagedef analyze_text_sentiment(text: str) -&gt; language.AnalyzeSentimentResponse:    client = language.LanguageServiceClient()    document = language.Document(        content=text,        type_=language.Document.Type.PLAIN_TEXT,    )    return client.analyze_sentiment(document=document)# Inputtext = "Python is a very readable language, ..."  # @param {type:"string"}# Send a request to the APIresponse = analyze_text_sentiment(text)# Use the resultsprint(response)ğŸ“Š Benefit from notebook advancedÂ featuresColab is hosting a Jupyter notebook. I personally depict Colab as the â€œGoogle Drive of notebooksâ€. As notebooks have additional superpowers, this lets you nicely process and visualize your data, such as tables, images, orÂ charts.In our example, the function show_text_sentiment gathers results in a pandas DataFrame, which renders as aÂ table:âœ¨ Check itÂ outIn these examples, click the â€œOpen in Colabâ€Â link:Using Google Cloud from ColabÂ (short)Using the Natural Language API with Python (detailed)ğŸ’¡ You can directly create a new notebook by opening the colab.new URL.ğŸ‘‹ HaveÂ fun!Read the Google Colab blog to learn more aboutÂ Colab.Follow me (Twitter / LinkedIn) for more cloud explorationsÂ ;)Using Google Cloud from Colab was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 5
* Title: 'From pixels to information with Document AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/from-pixels-to-information-with-document-ai-3476431c3010?source=rss-6be63961431c------2'
* PublicationDate: 'Wed, 15 Mar 2023 17:17:47 GMT'
* Categories: programming, python, gcp-app-dev, machine-learning, technology

Weâ€™re seeing successively difficult problems getting solved thanks to machine learning (ML) models. For example, Natural Language AI and Vision AI extract insights from text and images, with human-like results. They solve problems central to the way we communicate:âœ… Natural language processing (NLP)âœ… Optical character recognition (OCR)âœ… Handwriting recognition (HWR)Whatâ€™s next? Well, itâ€™s already here, with Document AI, and keepsÂ growing:âœ… Document understanding (DU)Document AI builds on these foundations to let you extract information from documents, in manyÂ forms:TextStructureSemantic entitiesNormalized valuesEnrichmentsQualitative signalsIn this article, youâ€™ll see the following:An overview of DocumentÂ AIPractical and visualÂ examplesA document processing demo (source code included)Processing documentsProcessor typesThere are as many document types as you can imagine so, to meet all needs, document processors are at the heart of Document AI. They can be of the following types:General processors handle common tasks such as detecting document tables or parsingÂ forms.Specialized processors analyze specific documents such as invoices, receipts, pay slips, bank statements, identity documents, official forms,Â etc.Custom processors meet other specific needs, letting you automatically train private ML models based on your own documents, and perform tasks such as custom document classification or custom entity detection.Processor locationsWhen you create a processor, you specify its location. This helps control where the documents will be processed. Here are the current multi-region locations:In addition, some processors are available in single-region locations; this lets you address local regulation requirements. If you regularly process documents in real-time or large batches of documents, this can also help get responses with even lower latencies. As an example, here are the current locations for the â€œDocument OCRâ€ general processor:Note: API endpoints are named according to the convention {location}-documentai.googleapis.com.For more information, check out Regional and multi-regional support.Processor versionsProcessors can evolve over time, to offer more precise results, new features, or fixes. For example, here are the current versions available for the â€œExpense Parserâ€ specialized processor:You may typically do the following:Use Stable version 1.3 in production.Use Release Candidate version 1.4 to benefit from new features or test a futureÂ version.To stay updated, follow the ReleaseÂ notes.InterfacesDocument AI is available to developers and practitioners through the usual interfaces:REST API (universal JSON-based interface)RPC API (low-latency gRPC interface, used by all Google services)Client Libraries (gRPC wrappers, to develop in your preferred programming language)Cloud Console (web admin user interface)RequestsThere are two ways you can process documents:Synchronously with online requests, to analyze a single document and directly use theÂ resultsAsynchronously with batch requests, to launch a batch processing operation on multiple or larger documentsExtracting information fromÂ pixelsLetâ€™s start by analyzing this simple screenshot with the â€œDocument OCRâ€ general processor, and check how pixels become structured data.Input imageDocument AIÂ responseThe response contains a Document instance.The entire text of the input, detected in natural reading order, is serialized under Document.text.The structured data is returned on a per page basis (a single pageÂ here).The â€œDocument OCRâ€ processor returns a structural skeleton common to all processors.The examples in this article show snake-case field names (like mime_type), using the convention used by gRPC and programming languages like Python. For camel-case environments (like REST/JSON), there is a direct mapping between the two conventions:mime_type â†”Â mimeTypepage_number â†” pageNumberâ€¦Text levelsFor each page, four levels of text detection are returned:blocksparagraphslinestokensIn these lists, each item exposes a layout including the item's position relative to the page in bounding_poly.normalized_vertices.This lets us, for example, highlight the 22 detectedÂ tokens:Here is the lastÂ token:Note: Float values are presented truncated for the sake of readability.Language supportDocuments are often written using one single language, but sometimes use multiple languages. You can retrieve the detected languages at different textÂ levels.In our previous example, two blocks are detected ( pages[0].blocks[]). Let's highlight them:The left block is a mix of German, French, and English, while the right block is English only. Here is how the three languages are reported at the pageÂ level:Note: At this level, the language confidence ratios roughly correspond to the proportion of text detected in each language.Now, letâ€™s highlight the five detected lines ( pages[0].lines[]):Each language is also reported at the lineÂ level:If needed, you can get language info at the token level too. â€œQuestionâ€ is the same word in French and in English, and is adequately returned as an English token in thisÂ context:In the screenshot, did you notice something peculiar in the leftÂ block?Well, punctuation rules can be different between languages. French uses a typographical space for double punctuation marks (â€œdoubleâ€ as â€œwritten in two partsâ€, such as in "!", "?", """,...). Punctuation is an important part of languages that can get "lost in translation". Here, the space is preserved in the transcription of "BienvenueÂ !". Nice touch Document AI or, should I say,Â touchÃ©!For more information, see LanguageÂ support.Handwriting detectionNowâ€Šâ€”â€Ša much harder problemâ€Šâ€”â€Šletâ€™s check how handwriting isÂ handled.This example is a mix of printed and handwritten text, where I wrote both a question and an answer. Here are the detectedÂ tokens:I am pleasantly surprised to see my own handwriting transcribed:I asked my family (used to writing French) to do the same. Each unique handwriting sample also gets correctly detected:â€¦ and transcribed:This can look magical but thatâ€™s one of the goals of ML models: return results as close as possible to human responses.Confidence scoresWe make mistakes, and so can ML models. To better appreciate the structured data you get, results include confidence scores:Confidence scores do not represent accuracy.They represent how confident the model is with the extracted results.They let youâ€Šâ€”â€Šand your usersâ€Šâ€”â€Šponder the modelâ€™s extractions.Letâ€™s overlay confidence scores on top of the previousÂ example:After grouping them in buckets, confidence scores appear to be generally high, with a few outliers:The lowest confidence score here is 57%. It corresponds to a handwritten word (token) that is both short (less context given for confidence) and not particularly legibleÂ indeed:For best results, keep in mind these general rules ofÂ thumb:ML results are best guesses, which can be correct or incorrect.Results with lower confidence scores are more likely to be incorrect guesses.If we canâ€™t smoothly read a part of a document, or need to think twice about it, itâ€™s probably also harder for the MLÂ model.Although all text is correctly transcribed in the presented examples, this wonâ€™t always be the case depending on the input document. To build safer solutionsâ€Šâ€”â€Šespecially with critical business applicationsâ€Šâ€”â€Šyou may consider the following:Be clear with your users that results are auto-generated.Communicate the scope and limitations of your solution.Improve the user experience with interpretable information or filterable results.Design your processes to trigger human intervention on lower confidence scores.Monitor your solution to detect changes over time (drift of stable metrics, decline in user satisfaction, etc.).To learn more about AI principles and best practices, check out Responsible AI practices.Rotation, skew, distortionHow many times did you scan a document upside down by mistake? Well, this shouldnâ€™t be a concern anymore. Text detection is very robust to rotation, skew, and other distortions.In this example, the webcam input is not only upside down but also skewed, blurry, and with text in unusual orientations:Before further analysis, Document AI considers the best reading orientation, at the page level, and preprocesses (deskews) each page if needed. This gives you results that can be used and visualized in a more natural way. Once processed by Document AI, the preceding example gets easier to read, without straining yourÂ neck:In the results, each page has an image field by default. This represents the image - deskewed if needed - used by Document AI to extract information. All the page results and coordinates are relative to this image. When a page has been deskewed, a transforms element is present and contains the list of transformation matrices applied to theÂ image:Notes:Page images can be in different formats. For instance, if your input is a JPEG image, the response will include either the same JPEG or a deskewed PNG (as in the earlier example).Deskewed images have larger dimensions (deskewing adds blank outerÂ areas).If you donâ€™t need visual results in your solution, you can specify a field_mask in your request to receive lighter responses, with only your fields of interest.OrientationDocuments donâ€™t always have all of their text in a single orientation, in which case deskewing alone is not enough. In this example, the sentence is broken out in four different orientations. Each part gets properly recognized and processed in its natural orientation:Orientations are reported in the layoutÂ field:Note: Orientations are returned at each OCR level ( blocks, paragraphs, lines, andÂ tokens).NoiseWhen documents come from our analog world, you can expectÂ â€¦ the unexpected. As ML models are trained from real-world samplesâ€Šâ€”â€Šcontaining real-life noiseâ€Šâ€”â€Ša very interesting outcome is that Document AI is also significantly robust toÂ noise.In this example with crumpled paper, the text starts to be difficult to read but still gets correctly transcribed by the OCRÂ model:Documents can also be dirty or stained. With the same sample, this keeps working after adding some layers ofÂ noise:In both cases, the exact same text is correctly detected:Youâ€™ve seen most core features. They are supported by the â€œDocument OCRâ€ general processor as well as the other processors, which leverage these features to focus on more specific document types and provide additional information. Letâ€™s check the next-level processor: the â€œForm Parserâ€ processor.Form fieldsThe â€œForm Parserâ€ processor lets you detect form fields. A form field is the combination of a field name and a field value, also called a key-value pair.In this example, printed and handwritten text is detected as seenÂ before:In addition, the form parser returns a list of form_fields:Here is how the detected key-value pairs are returned:And here are their detected boundingÂ boxes:Note: Form fields can follow flexible layouts. In this example, keys and values are in a left-right order. Youâ€™ll see a right-left example next. Those are just simple arbitrary examples. It also works with vertical or free layouts where keys and values are logically (visually) related.CheckboxesThe form parser also detects checkboxes. A checkbox is actually a particular form fieldÂ value.This example is a French exam with affirmations that should be checked when exact. To test this, I used checkboxes of different kinds, printed or handmade. All form fields are detected, with the affirmations as field names and the corresponding checkboxes as fieldÂ values:When a checkbox is detected, the form field contains an additional value_type field, which value is either unfilled_checkbox or filled_checkbox:Being able to analyze forms can lead to huge time savings, by consolidatingâ€Šâ€”â€Šor even autoprocessingâ€Šâ€”â€Šcontent for you. The preceding checkbox detection example was actually an evolution of a prior experiment to autocorrect my wifeâ€™s pile of exam copies. The proof of concept got better using checkboxes, but was already conclusive enough with True/False handwritten answers. Here is how it can autocorrect and autograde:TablesThe form parser can also detect another important structural element:Â tables.In this example, words are presented in a tabular layout without any borders. The form parser finds a table very close to the (hidden) layout. Here are the detectedÂ cells:In this other example, some cells are filled with text while others are blank. There are enough signals for the form parser to detect a tabular structure:When tables are detected, the form parser returns a list of tables with their rows and cells. Here is how the table is returned:And here is the firstÂ cell:Specialized processorsSpecialized processors focus on domain-specific documents and extract entities. They cover many different document types that can currently be classified in the following families:Procurementâ€Šâ€”â€Šreceipts, invoices, utility bills, purchaseÂ orders,â€¦Lendingâ€Šâ€”â€Šbank statements, pay slips, officialÂ forms,â€¦Identityâ€Šâ€”â€Šnational IDs, driver licenses, passports,â€¦Contractâ€Šâ€”â€Šlegal agreementsFor example, procurement processors typically detect the total_amount and currency entities:For more information, check out Fields detected.ExpensesThe â€œExpense Parserâ€ lets you process receipts of various types. Letâ€™s analyze this actual (French)Â receipt:A fewÂ remarks:All expected entities are extracted.Ideally (to be picky), Iâ€™d like to get the tax rate too. If thereâ€™s customer demand for it, this may be a supported entity in a futureÂ version.Due to the receiptâ€™s very thin paper, the text on the back side is visible by transparency (another type ofÂ noise).The supplier name is wrong (itâ€™s actually mirrored text from the back side) but with a very low confidence (4%). Youâ€™d typically handle it with special care (itâ€™s shown differently here) or ignoreÂ it.The actual supplier info is hidden (the top part of the receipt is folded) on purpose. Youâ€™ll see another example with supplier data a bitÂ later.Receipts are often printed on single (sometimes on multiple) pages. The expense parser supports analyzing expense documents of up to 10Â pages.Procurement documents often list parts of the data in tabular layouts. Here, theyâ€™re returned as many line_item/* entities. When the entities are detected as part of a hierarchical structure, the results are nested in the properties field of a parent entity, providing an additional level of information. Here's anÂ excerpt:For more information, see the Expense ParserÂ details.Entity normalizationGetting results is generally not enough. Results often need to be handled in a post-processing stage, which can be both time consuming and a source of errors. To address this, specialized processors also return normalized values when possible. This lets you directly use standard values consolidated from the context of the whole document.Letâ€™s check it with this otherÂ receipt:First, the receipt currency is returned with its standard code under normalized_value:Then, the receipt is dated 11/12/2022. But is it Nov. 12 or Dec. 11? Document AI uses the context of the document (a French receipt) and provides a normalized value that removes all ambiguity:Likewise, the receipt contains a purchase time, written in a non-standard way. The result also includes a canonical value that avoids any interpretation:Normalized values simplify the post-processing stage:They provide standard values that are straightforward to use (e.g. enable direct storage in a data warehouse).They prevent bugs (esp. the recurring developer mistakes we make when converting data).They remove ambiguity and avoid incorrect interpretations by using the context of the whole document.For more information, check out the NormalizedValue structure.Entity enrichmentDid you notice there was more information in theÂ receipt?â€œMaison Jeanne dâ€™Arc, Place de Gaulleâ€ mentions a Joan of Arcâ€™s House and a place name. Nonetheless, there is no address, zip code, city, 

== Article 6
* Title: 'Automate identity document processing with Document AI'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/automate-identity-document-processing-with-document-ai-912e1011e164?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 02 Jun 2022 12:32:10 GMT'
* Categories: technology, machine-learning, ai, document-management, programming

How many times have you filled out forms requesting personal information? Itâ€™s probably too many times to count. When online and signed in, you can save a lot of time thanks to your browserâ€™s autofill feature. In other cases, you often have to provide the same data manually, again and again. The first Document AI identity processors are now generally available and can help you solve thisÂ problem.In this post, youâ€™ll see howÂ toâ€¦Process identity documents with DocumentÂ AICreate your own identity form autofillerUse casesHere are a few situations that youâ€™ve probably encountered:Financial accounts: Companies need to validate the identity of individuals. When creating a customer account, you need to present a government-issued ID for manual validation.Transportation networks: To handle subscriptions, operators often manage fleets of custom identity-like cards. These cards are used for in-person validation, and they require an IDÂ photo.Identity gates: When crossing a border (or even when flying domestically), you need to pass an identity check. The main gates have streamlined processes and are generally well equipped to scale with the traffic. On the contrary, smaller gates along borders can have manual processesâ€Šâ€”â€Šsometimes on the way in and the way outâ€Šâ€”â€Šwhich can lead to long lines andÂ delays.Hotels: When traveling abroad and checking in, you often need to show your passport for a scan. Sometimes, you also need to fill out a longer paper form and write down the sameÂ data.Customer benefits: For benefit certificates or loyalty cards, you generally have to provide personal info, which can include a portraitÂ photo.In these examples, the requested infoâ€Šâ€”â€Šincluding the portrait photoâ€Šâ€”â€Šis already on your identity document. Moreover, an official authority has already validated it. Checking or retrieving the data directly from this source of truth would not only make processes faster and more effective, but also remove a lot of friction for endÂ users.Identity processorsProcessor typesEach Document AI identity processor is a machine learning model trained to extract information from a standard ID document suchÂ as:Driver licenseNational IDPassportNote: an ID can have information on both sides, so identity processors support up to two pages per document.AvailabilityGenerally available as of June 2022, you can use two US identity processors in production:Currently available inÂ Preview:The Identity Doc Fraud Detector, to check whether an ID document has been tamperedÂ withThree French identity processorsNotes:More identity processors are in theÂ pipe.To request access to processors in Preview, please fill out the Access RequestÂ Form.Processor creationYou can create a processor:Manually from Cloud Console (web adminÂ UI)Programmatically with theÂ APIProcessors are location-based. This helps guarantee where processing will occur for each processor.Here are the current multi-region locations:Once youâ€™ve created a processor, you reference it with its ID (PROCESSOR_ID hereafter).Note: To manage processors programmatically, see the codelab Managing Document AI processors withÂ Python.Document processingYou can process documents in twoÂ ways:Synchronously with an online request, to analyze a single document and directly use theÂ resultsAsynchronously with a batch request, to launch a batch processing operation on multiple or larger documentsOnline requestsExample of a REST onlineÂ request:The method is namedÂ process.The input document here is a PNG image (base64 encoded).This request is processed in the EuropeanÂ Union.The response is returned synchronously.Batch requestsExample of a REST batchÂ request:The method is named batchProcess.The batchProcess method launches the batch processing of multiple documents.This request is processed in the UnitedÂ States.The response is returned asynchronously; output files will be stored under my-storage-bucket/output/.InterfacesDocument AI is available through the usual Google Cloud interfaces:The RPC API (low-latency gRPC)The REST API (JSON requests and responses)Client libraries (gRPC wrappers, currently available for Python, Node.js, andÂ Java)Cloud Console (web adminÂ UI)Note: With the client libraries, you can develop in your preferred programming language. Youâ€™ll see an example later in thisÂ post.Identity fieldsA typical REST response looks like the following:The text and pages fields include the OCR data detected by the underlying ML models. This part is common to all Document AI processors.The entities list contains the fields specifically detected by the identity processor.Here are the detectable identityÂ fields:Please note that Address and MRZ Code are optional fields. For example, a US passport contains an MRZ but noÂ address.Fraud detectionAvailable in preview, the Identity Doc Fraud Detector helps detect tampering attempts. Typically, when an identity document does not â€œpassâ€ the fraud detector, your automated process can block the attempt or trigger a human validation.Here is an example of signals returned:Sample demoYou can process a document live with just a few lines ofÂ code.Here is a PythonÂ example:This function uses the Python clientÂ library:The input is a file (any format supported by the processor).client is an API wrapper (configured for processing to take place in the desired location).process_document calls the API process method, which returns results inÂ seconds.The output is a structured Document.You can collect the detected fields by parsing the document entities:Note: This function builds a mapping ready to be sent to a frontend. A similar function can be used for other specialized processors.Finalize yourÂ app:Define your user experience and architectureImplement your backend and itsÂ APIImplement your frontend with a mix of HTML + CSS +Â JSAdd a couple of features: file uploads, document samples, or webcamÂ capturesThatâ€™s it; youâ€™ve built an identity form autofillerHere is a sample web app inÂ action:Here is the processing of a French national ID, dropping images from theÂ client:Note: For documents with multiple pages, you can use a PDF or TIFF container. In this example, the two uploaded PNG images are merged by the backend and processed as a TIFFÂ file.And this is the processing of a US driver license, captured with a laptop 720pÂ webcam:Notes:Did you notice that the webcam capture is skewed and the detected portrait image straight? Thatâ€™s because Document AI automatically deskews the input at the page level. Documents can even be upsideÂ down.Some fields (such as the dates) are returned with their normalized values. This can make storing and processing these values a lot easierâ€Šâ€”â€Šand less error-proneâ€Šâ€”â€Šfor developers.The source code for this demo is available in our Document AI sample repository.MoreCheck out the official announcementTry Document AI in yourÂ browserDocument AI documentationDocument AI how-toÂ guidesSending a processing requestFull processor and detailÂ listRelease notesCodelabâ€Šâ€”â€ŠSpecialized processors with DocumentÂ AICodeâ€Šâ€”â€ŠDocument AIÂ samplesFor more cloud content, follow me on Twitter (@PicardParis) or LinkedIn (in/PicardParis), and feel free to get in touch with any feedback or questions.Stay tuned; the family of Document AI processors keeps growing andÂ growing.Originally published on the Google Cloud Blog on June 1,Â 2022.Automate identity document processing with Document AI was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 7
* Title: 'Deploy a coloring page generator in minutes with Cloud Run'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/deploy-a-coloring-page-generator-in-minutes-with-cloud-run-bff59e59d890?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 07 Apr 2022 07:56:59 GMT'
* Categories: gcp-app-dev, technology, programming, image-processing, python

ğŸ‘‹ HelloHave you ever written a script to transform an image? Did you share the script with others or did you run it on multiple computers? How many times did you need to update the script or the setup instructions? Did you end up making it a service or an online app? If your script is useful, youâ€™ll likely want to make it available to others. Deploying processing services is a recurring needâ€Šâ€”â€Šone that comes with its own set of challenges. Serverless technologies let you solve these challenges easily and efficiently.In this post, youâ€™ll see howÂ toâ€¦Create an image processing service that generates coloringÂ pagesMake it available online using minimal resourcesâ€¦and do it all in less than 200 lines of Python and JavaScript!ğŸ› ï¸ ToolsTo build and deploy a coloring page generator, youâ€™ll need a fewÂ tools:A library to processÂ imagesA web application frameworkA webÂ serverA serverless solution to make the demo available 24/7ğŸ§± ArchitectureHere is one possible architecture for a coloring page generator using CloudÂ Run:And here is the workflow:1. The user opens the web app: the browser requests the mainÂ page.2. Cloud Run serves the app HTMLÂ code.3. The browser requests the additional needed resources.4. Cloud Run serves the CSS, JavaScript, and other resources.A. The user selects an image and the frontend sends the image to the /api/coloring-page endpoint.B. The backend processes the input image and returns an output image, which the user can then visualize, download, or print via theÂ browser.ğŸ SoftwareÂ stackOf course, there are many different software stacks that you could use to implement such an architecture.Here is a good one based onÂ Python:It includes:Gunicorn: A production-grade WSGI HTTPÂ serverFlask: A popular web app frameworkscikit-image: An extensive image processing libraryDefine these app dependencies in a file named requirements.txt:ğŸ¨ Image processingHow do you remove colors from an image? One way is by detecting the object edges and removing everything but the edges in the result image. This can be done with a Sobel filter, a convolution filter that detects the regions in which the image intensity changes theÂ most.Create a Python file named main.py, define an image processing function, and within it use the Sobel filter and other functions from scikit-image:Note: The NumPy and Pillow libraries are automatically installed as dependencies of scikit-image.As an example, here is how the Cloud Run logo is processed at eachÂ step:âœ¨ WebÂ appBackendTo expose both endpoints ( GET / and POST /api/coloring-page), add Flask routes inÂ main.py:FrontendOn the browser side, write a JavaScript function that calls the /api/coloring-page endpoint and receives the processed image:The base of your app is there. Now you just need to add a mix of HTML + CSS + JS to complete the desired user experience.Local developmentTo develop and test the app on your computer, once your environment is set up, make sure you have the needed dependencies:Add the following block to main.py. It will only execute when you run your app manually:Run yourÂ app:Flask starts a local webÂ server:Note: In this mode, youâ€™re using a development web server (one that is not suited for production). Youâ€™ll next set up the deployment to serve your app with Gunicorn, a production-grade server.Youâ€™re all set. Open localhost:8080 in your browser, test, refine, andÂ iterate.ğŸš€ DeploymentOnce your app is ready for prime time, you can define how it will be served with this single line in a file named Procfile:At this stage, here are the files found in a typicalÂ project:Thatâ€™s it, you can now deploy your app from the sourceÂ folder:âš™ï¸ Under theÂ hoodThe command line output details all the different steps:Cloud Build is indirectly called to containerize your app. One of its core components is Google Cloud Buildpacks, which automatically builds a production-ready container image from your source code. Here are the mainÂ steps:Cloud Build fetches the sourceÂ code.Buildpacks autodetects the app language (Python, in this case) and uses the corresponding secure baseÂ image.Buildpacks installs the app dependencies (defined in requirements.txt forÂ Python).Buildpacks configures the service entrypoint (defined in Procfile forÂ Python).Cloud Build pushes the container image to Artifact Registry.Cloud Run creates a new revision of the service based on this container image.Cloud Run routes production traffic toÂ it.Notes:- Buildpacks currently supports the following runtimes: Go, Java,Â .NET, Node.js, and Python.- The base image is actively maintained by Google, scanned for security vulnerabilities, and patched against known issues. This means that, when you deploy an update, your service is based on an image that is as secure as possible.- If you need to build your own container image, for example with a custom runtime, you can add your own Dockerfile and Buildpacks will use itÂ instead.ğŸ’« UpdatesMore testing from real-life users shows someÂ issues.First, the app does not handle pictures taken with digital cameras in non-native orientations. You can fix this using the EXIF orientation data:In addition, the app is too sensitive to details in the input image. Textures in paintings, or noise in pictures, can generate many edges in the processed image. You can improve the processing algorithm by adding a denoising stepÂ upfront:This additional step makes the coloring page cleaner and reduces the quantity of ink used if you printÂ it:Redeploy, and the app is automatically updated:ğŸ‰ Itâ€™sÂ aliveThe app is visible as a service in CloudÂ Run:The service dashboard gives you an overview of appÂ usage:Thatâ€™s it; your image processing app is in production!ğŸ¤¯ Itâ€™s serverlessThere are many benefits to using Cloud Run in this architecture:Your app is available 24/7.The environment is fully managed: you can focus on your code and not worry about the infrastructure.Your app is automatically available throughÂ HTTPS.You can map your app to a customÂ domain.Cloud Run scales the number of instances automatically and the billing includes only the resources used when your codeÂ runs.If your app is not used, Cloud Run scales down toÂ zero.If your app gets more traffic (imagine it makes the news), Cloud Run scales up to the number of instances needed.You can control performance and cost by fine-tuning many settings: CPU, memory, concurrency, minimum instances, maximum instances, andÂ more.Every month, the free tier offers the first 50 vCPU-hours, 100 GiB-hours, and 2 million requests for noÂ cost.ğŸ’¾ SourceÂ codeThe project includes just seven files and less than 200 lines of Python + JavaScript code.You can reuse this demo as a base to build your own image processing app:Check out the source code onÂ GitHub.For step-by-step instructions on deploying the app yourself in a few minutes, see â€œDeploying from scratchâ€.ğŸ–– MoreTry the demo and generate your own coloringÂ pages.Learn more about CloudÂ Run.For more cloud content, follow me on Twitter (@PicardParis) or LinkedIn (in/PicardParis), and feel free to get in touch with any feedback or questions.ğŸ“œ Also in thisÂ seriesSummarizing videosTracking videoÂ objectsFace detection and processingProcessing imagesOriginally published on the Google Cloud Blog on April 5,Â 2022.Deploy a coloring page generator in minutes with Cloud Run was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Face detection and processing in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/face-detection-and-processing-in-300-lines-of-code-38dc51a115d4?source=rss-6be63961431c------2'
* PublicationDate: 'Tue, 29 Sep 2020 14:19:14 GMT'
* Categories: programming, machine-learning, gcp-app-dev, python, technology

â³ 2021â€“10â€“08 updateUpdated GitHub version with latest library versions + Python 3.7 â†’Â 3.9ğŸ‘‹ Hello!In this article, youâ€™ll see the following:how to detect faces in pictures,how to automatically anonymize, crop,â€¦ a picture withÂ faces,how to make this a serverless onlineÂ demo,in less than 300 lines of PythonÂ code.Here is a famous face that has been automatically anonymized andÂ cropped.Do you guess who thisÂ is?Note: Weâ€™re talking about face detection, not face recognition. Though technically possible, face recognition can have harmful applications. Responsible companies have established AI principles and avoid exposing such potentially harmful technologies (e.g. Google AI Principles).ğŸ› ï¸ ToolsA few tools willÂ do:a machine learning model to analyzeÂ images,a library to processÂ images,a web application framework,a serverless solution to keep the demo available 24/7 and at minimalÂ cost.ğŸ§± ArchitectureHere is an architecture using 2 Google Cloud services (App Engine + VisionÂ API):The workflow is the following:Open the demo: App Engine serves the homeÂ page.Take a selfie: the frontend sends it to the /analyze-image endpoint.The backend sends a request to the Vision API: the image is analyzed and the results (annotations) are returned.The backend returns the annotations, in addition to the number of detected faces (to display the info directly in the webÂ page).The frontend sends image, annotations, and processing options to the /process-image endpoint.The backend processes the image with the given options and returns the resultÂ image.Change the options: steps 5 and 6 are repeated.Get the image with newÂ options.This is one of many possible architectures. The advantages of this one are the following:The web browser caches both the selfie and the annotations: no storage is involved and no private images are stored anywhere in theÂ cloud.The Vision API is only called once perÂ image.ğŸ Python librariesGoogle CloudÂ VisionThe client library that wraps calls to the VisionÂ API.https://pypi.org/project/google-cloud-visionPillowA very popular imaging library, both extensive and easy toÂ use.https://pypi.org/project/PillowFlaskOne of the most popular web app frameworks.https://pypi.org/project/FlaskDependenciesDefine the dependencies in the requirements.txt file:google-cloud-vision==1.0.0Pillow==7.2.0Flask==1.1.2Notes:- As a best practice, also specify the dependency versions. This freezes your production environment in a known state and prevents newer versions from potentially breaking future deployments.- App Engine will automatically deploy these dependencies.ğŸ§  ImageÂ analysisVision APIThe Vision API gives access to state-of-the-art machine learning models for image analysis. One of the multiple features is face detection. Here is a way to detect faces in anÂ image:https://medium.com/media/1bc70fc4769e5468900fc4377aee3080/hrefBackend endpointExposing an API endpoint with Flask consists in wrapping a function with a route. Here is a possible POST endpoint:https://medium.com/media/9d9bff745ecaadfd4b8ae3175481e66c/hrefFrontend requestHere is a javascript function to call the API from the frontend:https://medium.com/media/80380594bf727837b452b5a8d62a8242/hrefğŸ¨ Image processingFace bounding box and landmarksThe Vision API provides the bounding box of the detected faces and the position of 30+ face landmarks (mouth, nose, eyes,â€¦). Here is a way to visualize them with PillowÂ (PIL):https://medium.com/media/9875815330881f6d59e489872744d4ab/hrefAmerican Gothic (Wikimedia)Face anonymizationHere is way to anonymize the faces thanks to the boundingÂ boxes:https://medium.com/media/8ed328faf40629175a2bd873aeda793a/hrefAmerican Gothic (Wikimedia)Face croppingSimilarly, to focus on the detected faces, you can crop everything around theÂ faces:https://medium.com/media/9f72e084a4f94f76f573e4dc7f61c9a3/hrefAmerican Gothic (Wikimedia)ğŸ’ Cherry on PyÂ ğŸNow, the icing on the cake (or the â€œcherry on the pieâ€ as we say inÂ French):Having independent rendering functions lets you combine multiple options atÂ once.Knowing the bounding box for all faces allows cropping the image to the minimal boundingÂ box.Using the location of the nose and the mouth, you can add a moustache to everyone.If your functions have parameters to render a single frame, you can generate animations with a few lines ofÂ code.Once your Flask app works locally, you can deploy and keep it available 24/7 at minimalÂ cost.Here is whatâ€™s detected on famous photorealistic paintings:American Gothic (Wikimedia)Girl with a Pearl Earring (Wikimedia)Shakespeare (Wikimedia)Here are some animated versions:American Gothic (Wikimedia)Girl with a Pearl Earring (Wikimedia)Shakespeare (Wikimedia)Note: animations are a bit degraded (GIF version) as Medium does not support animated PNGs. The demo below lets you generate them in GIF, PNG, andÂ WebP.And, of course, this works even better on real pictures:Personal pictures (aged from 2 toÂ 44)Yes, Iâ€™ve had a moustache for over 42 years, and my sister tooÂ ;)And, finally, here is our famous anonymous from the beginning:Mona Lisa (Wikimedia)ğŸš€ Source code and deploymentSource codeThe Python code for the backend takes less than 300 lines ofÂ code.See the source onÂ GitHub.DeploymentYou can deploy this demo in 4Â minutes.See â€œDeploying from scratchâ€.ğŸ‰ OnlineÂ demoTry the demo by yourself:â¡ï¸ https://face-detection.lolo.dev â¬…ï¸https://face-detection.lolo.devğŸ–– SeeÂ youFeedback, questions? Iâ€™d love to read from you! Follow me on Twitter forÂ moreâ€¦â³ Updates2021â€“10â€“08: Updated GitHub version with latest library versions + Python 3.7 â†’Â 3.9ğŸ“œ Also in thisÂ seriesSummarizing videosTracking videoÂ objectsFace detection and processingProcessing imagesFace detection and processing in 300 lines of code was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'Tracking video objects in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/video-object-tracking-as-a-service-18eb4227df34?source=rss-6be63961431c------2'
* PublicationDate: 'Thu, 25 Jun 2020 17:46:01 GMT'
* Categories: python, gcp-app-dev, technology, programming, machine-learning

â³ 2021â€“10â€“08 updateUpdated GitHub version with latest library versions + Python 3.7 â†’Â 3.9ğŸ‘‹ Hello!In this article, youâ€™ll see the following:how to track objects present in aÂ video,with an automated processing pipeline,in less than 300 lines of PythonÂ code.Here is an example of an auto-generated object summary for the video &lt;animals.mp4&gt;:ğŸ› ï¸ ToolsA few tools willÂ do:Storage space for videos andÂ resultsA serverless solution to run theÂ codeA machine learning model to analyzeÂ videosA library to extract frames fromÂ videosA library to render theÂ objectsğŸ§± ArchitectureHere is a possible architecture using 3 Google Cloud services (Cloud Storage, Cloud Functions, and the Video Intelligence API):The processing pipeline follows theseÂ steps:You upload aÂ videoThe upload event automatically triggers the trackingÂ functionThe function sends a request to the Video Intelligence APIThe Video Intelligence API analyzes the video and uploads the results (annotations)The upload event triggers the rendering functionThe function downloads both annotation and videoÂ filesThe function renders and uploads theÂ objectsYou know which objects are present in yourÂ video!ğŸ Python librariesVideo Intelligence APITo analyzeÂ videoshttps://pypi.org/project/google-cloud-videointelligenceCloud StorageTo manage downloads andÂ uploadshttps://pypi.org/project/google-cloud-storageOpenCVTo extract videoÂ framesOpenCV offers a headless version (without GUI features, ideal for aÂ service)https://pypi.org/project/opencv-python-headlessPillowTo render and annotate objectÂ imagesPillow is a very popular imaging library, both extensive and easy toÂ usehttps://pypi.org/project/PillowğŸ§  VideoÂ analysisVideo Intelligence APIThe Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of its multiple features is detecting and tracking objects. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the OBJECT_TRACKING feature:https://medium.com/media/8a64a3c78d7f90be9bdea2a2525f0189/hrefCloud Function entryÂ pointhttps://medium.com/media/49ed5269b70727a1fa83a5f30fcdce89/hrefNotes:â€¢ This function will be called when a video is uploaded to the bucket defined as a trigger.â€¢ Using an environment variable makes the code more portable and lets you deploy the exact same code with different trigger and outputÂ buckets.ğŸ¨ Object renderingCode structureItâ€™s interesting to split the code into 2 mainÂ classes:StorageHelper for managing local files and cloud storageÂ objectsVideoProcessor for all graphical processingsHere is a possible core function for the 2nd Cloud Function:https://medium.com/media/02a54ddd6ee288746d69f59c26a48b07/hrefCloud Function entryÂ pointhttps://medium.com/media/43f8e86165e95433fed65f48610591b0/hrefNote: This function will be called when an annotation file is uploaded to the bucket defined as aÂ trigger.Frame renderingOpenCV and Pillow easily let you extract video frames and compose overÂ them:https://medium.com/media/c5f67819bddc6217f60fc87480e76a37/hrefNote: It would probably be possible to only use OpenCV but I found it more productive developing with Pillow (code is more readable and intuitive).ğŸ” ResultsHere are the main objects found in the video &lt;JaneGoodall.mp4&gt;:Notes:â€¢ The machine learning model has correctly identified different wildlife species: those are â€œtrue positivesâ€. It has also incorrectly identified our planet as â€œpackaged goodsâ€: this is a â€œfalse positiveâ€. Machine learning models keep learning by being trained with new samples so, with time, their precision keeps increasing (resulting in less false positives).â€¢ The current code filters out objects detected with a confidence below 70% or with less than 10 frames. Lower the thresholds to get moreÂ results.ğŸ’ Cherry on PyÂ ğŸNow, the icing on the cake (or the â€œcherry on the pieâ€ as we say in French), you can enrich the architecture to add new possibilities:Trigger the processing for videos from any bucket (including external publicÂ buckets)Generate individual object animations (in parallel to object summaries)Architecture (v2)Aâ€Šâ€”â€ŠVideo object tracking can also be triggered manually with an HTTP GETÂ requestBâ€Šâ€”â€ŠThe same rendering code is deployed in 2 sibling functions, differentiated with an environment variableCâ€Šâ€”â€ŠObject summaries and animations are generated inÂ parallelCloud Function HTTP entryÂ pointhttps://medium.com/media/42072aef5f825d5b2298aab3e7ff778c/hrefNote: This is the same code as gcf_track_objects() with the video URI parameter specified by the caller through a GETÂ request.ğŸ‰ ResultsHere are some auto-generated trackings for the video &lt;animals.mp4&gt;:The left elephant (a big objectÂ ;) is detected:The right elephant is perfectly isolatedÂ too:The veterinarian is correctly identified:The animal heâ€™s feedingÂ too:Moving objects or static objects in moving shots are tracked too, as in &lt;beyond-the-map-rio.mp4&gt;:A building in a movingÂ shot:Neighbor buildings are trackedÂ too:Persons in a movingÂ shot:A surfer crossing theÂ shot:Here are some others for the video &lt;JaneGoodall.mp4&gt;:A butterfly (easy?):An insect, in larval stage, climbing a movingÂ twig:An ape in a tree far awayÂ (hard?):A monkey jumping from the top of a tree (harder?):Now, a trap! If we can be fooled, current machine learning state of the art canÂ too:ğŸš€ Source code and deploymentSource codeThe Python source is less than 300 lines ofÂ code.See the source onÂ GitHub.DeploymentYou can deploy this architecture in less than 8Â minutes.See â€œDeploying from scratchâ€.ğŸ–– SeeÂ youDo you want more, do you have questions? Iâ€™d love to read your feedback. You can also follow me onÂ Twitter.â³ Updates2021â€“10â€“08: Updated GitHub version with latest library versions + Python 3.7 â†’Â 3.9ğŸ“œ Also in thisÂ seriesSummarizing videosTracking videoÂ objectsFace detection and processingProcessing imagesTracking video objects in 300 lines of code was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Summarizing videos in 300 lines of code'
* Author: 'Laurent Picard'
* URL: 'https://medium.com/google-cloud/%EF%B8%8F-auto-generate-video-summaries-with-a-machine-learning-model-and-a-serverless-pipeline-c2f261c8035c?source=rss-6be63961431c------2'
* PublicationDate: 'Sat, 30 May 2020 13:54:30 GMT'
* Categories: gcp-app-dev, technology, programming, python, machine-learning

â³ 2021â€“10â€“08 updateUpdated GitHub version with latest library versions + Python 3.7 â†’Â 3.9ğŸ‘‹ Hello!Dear developers,Do you like the adage â€œa picture is worth a thousand wordsâ€? I do! Letâ€™s check if it also works for â€œa picture is worth a thousandÂ framesâ€.In this tutorial, youâ€™ll see the following:how to understand the content of a video in aÂ blink,in less than 300 lines of Python (3.7)Â code.A visual summary generated from a 2'42" video made of 35 sequences (shots). The summary is a grid where each cell is a frame representing a videoÂ shot.ğŸ”­ ObjectivesThis tutorial has 2 objectives, 1 practical and 1 technical:Automatically generate visual summaries ofÂ videosBuild a processing pipeline with these properties:managed (always ready and easy to setÂ up)scalable (able to ingest several videos in parallel)not costing anything when notÂ usedğŸ› ï¸ ToolsA few tools areÂ enough:Storage space for videos andÂ resultsA serverless solution to run theÂ codeA machine learning model to analyzeÂ videosA library to extract frames fromÂ videosA library to generate the visual summariesğŸ§± ArchitectureHere is a possible architecture using 3 Google Cloud services (Cloud Storage, Cloud Functions, and Video Intelligence API):The processing pipeline follows theseÂ steps:You upload a video to the 1st bucket (a bucket is a storage space in theÂ cloud)The upload event automatically triggers the 1stÂ functionThe function sends a request to the Video Intelligence API to detect theÂ shotsThe Video Intelligence API analyzes the video and uploads the results (annotations) to the 2ndÂ bucketThe upload event triggers the 2ndÂ functionThe function downloads both annotation and videoÂ filesThe function renders and uploads the summary to the 3rdÂ bucketThe video summary isÂ ready!ğŸ Python librariesOpen source client libraries let you interface with Google Cloud services in idiomatic Python. Youâ€™ll use the following:Cloud StorageTo manage downloads andÂ uploadshttps://pypi.org/project/google-cloud-storageVideo Intelligence APITo analyzeÂ videoshttps://pypi.org/project/google-cloud-videointelligenceHere is a choice of 2 additional Python libraries for the graphical needs:OpenCVTo extract videoÂ framesThereâ€™s even a headless version (without GUI features), which is ideal for aÂ servicehttps://pypi.org/project/opencv-python-headlessPillowTo generate the visual summariesPillow is a very popular imaging library, both extensive and easy toÂ usehttps://pypi.org/project/Pillowâš™ï¸ ProjectÂ setupAssuming you have a Google Cloud account, you can set up the architecture from Cloud Shell with the gcloud and gsutil commands. This lets you script everything from scratch in a reproducible way.Environment variables# ProjectPROJECT_NAME="Visual Summary"PROJECT_ID="visual-summary-REPLACE_WITH_UNIQUE_SUFFIX"# Cloud Storage region (https://cloud.google.com/storage/docs/locations)GCS_REGION="europe-west1"# Cloud Functions region (https://cloud.google.com/functions/docs/locations)GCF_REGION="europe-west1"# SourceGIT_REPO="cherry-on-py"PROJECT_SRC=~/$PROJECT_ID/$GIT_REPO/gcf_video_summary# Cloud Storage buckets (environment variables)export VIDEO_BUCKET="b1-videos_${PROJECT_ID}"export ANNOTATION_BUCKET="b2-annotations_${PROJECT_ID}"export SUMMARY_BUCKET="b3-summaries_${PROJECT_ID}"Note: You can use your GitHub username as a uniqueÂ suffix.New projectgcloud projects create $PROJECT_ID \  --name="$PROJECT_NAME" \  --set-as-defaultCreate in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/PROJECT_ID].Waiting for [operations/cp...] to finish...done.Enabling service [cloudapis.googleapis.com] on project [PROJECT_ID]...Operation "operations/acf..." finished successfully.Updated property [core/project] to [PROJECT_ID].Billing account# Link project with billing account (single account)BILLING_ACCOUNT=$(gcloud beta billing accounts list \    --format 'value(name)')# Link project with billing account (specific one among multiple accounts)BILLING_ACCOUNT=$(gcloud beta billing accounts list  \    --format 'value(name)' \    --filter "displayName='My Billing Account'")gcloud beta billing projects link $PROJECT_ID --billing-account $BILLING_ACCOUNTbillingAccountName: billingAccounts/XXXXXX-YYYYYY-ZZZZZZbillingEnabled: truename: projects/PROJECT_ID/billingInfoprojectId: PROJECT_IDBuckets# Create buckets with uniform bucket-level accessgsutil mb -b on -c regional -l $GCS_REGION gs://$VIDEO_BUCKETgsutil mb -b on -c regional -l $GCS_REGION gs://$ANNOTATION_BUCKETgsutil mb -b on -c regional -l $GCS_REGION gs://$SUMMARY_BUCKETCreating gs://VIDEO_BUCKET/...Creating gs://ANNOTATION_BUCKET/...Creating gs://SUMMARY_BUCKET/...You can check how it looks like in the CloudÂ Console:Service accountCreate a service account. This is for development purposes only (not needed for production). This provides you with credentials to run your codeÂ locally.mkdir ~/$PROJECT_IDcd ~/$PROJECT_IDSERVICE_ACCOUNT_NAME="dev-service-account"SERVICE_ACCOUNT="${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com"gcloud iam service-accounts create $SERVICE_ACCOUNT_NAMEgcloud iam service-accounts keys create ~/$PROJECT_ID/key.json --iam-account $SERVICE_ACCOUNTCreated service account [SERVICE_ACCOUNT_NAME].created key [...] of type [json] as [~/PROJECT_ID/key.json] for [SERVICE_ACCOUNT]Set the GOOGLE_APPLICATION_CREDENTIALS environment variable and check that it points to the service account key. When you run the application code in the current shell session, client libraries will use these credentials for authentication. If you open a new shell session, set the variableÂ again.export GOOGLE_APPLICATION_CREDENTIALS=~/$PROJECT_ID/key.jsoncat $GOOGLE_APPLICATION_CREDENTIALS{  "type": "service_account",  "project_id": "PROJECT_ID",  "private_key_id": "...",  "private_key": "-----BEGIN PRIVATE KEY-----\n...",  "client_email": "SERVICE_ACCOUNT",  ...}Authorize the service account to access theÂ buckets:IAM_BINDING="serviceAccount:${SERVICE_ACCOUNT}:roles/storage.objectAdmin"gsutil iam ch $IAM_BINDING gs://$VIDEO_BUCKETgsutil iam ch $IAM_BINDING gs://$ANNOTATION_BUCKETgsutil iam ch $IAM_BINDING gs://$SUMMARY_BUCKETAPIsA few APIs are enabled byÂ default:gcloud services listNAME                              TITLEbigquery.googleapis.com           BigQuery APIbigquerystorage.googleapis.com    BigQuery Storage APIcloudapis.googleapis.com          Google Cloud APIsclouddebugger.googleapis.com      Cloud Debugger APIcloudtrace.googleapis.com         Cloud Trace APIdatastore.googleapis.com          Cloud Datastore APIlogging.googleapis.com            Cloud Logging APImonitoring.googleapis.com         Cloud Monitoring APIservicemanagement.googleapis.com  Service Management APIserviceusage.googleapis.com       Service Usage APIsql-component.googleapis.com      Cloud SQLstorage-api.googleapis.com        Google Cloud Storage JSON APIstorage-component.googleapis.com  Cloud StorageEnable the Video Intelligence and Cloud Functions APIs:gcloud services enable \  videointelligence.googleapis.com \  cloudfunctions.googleapis.comOperation "operations/acf..." finished successfully.Source codeRetrieve the sourceÂ code:cd ~/$PROJECT_IDgit clone https://github.com/PicardParis/$GIT_REPO.gitCloning into 'GIT_REPO'......ğŸ§  VideoÂ analysisVideo shot detectionThe Video Intelligence API is a pre-trained machine learning model that can analyze videos. One of the multiple features is video shot detection. For the 1st Cloud Function, here is a possible core function calling annotate_video() with the SHOT_CHANGE_DETECTION feature:https://medium.com/media/a149fabef417bf720977f28c1ca4afb0/hrefLocal development andÂ testsBefore deploying the function, you need to develop and test it. Create a Python 3 virtual environment and activateÂ it:cd ~/$PROJECT_IDpython3 -m venv venvsource venv/bin/activateInstall the dependencies:pip install -r $PROJECT_SRC/gcf1_detect_shots/requirements.txtCheck the dependencies:pip listPackage                        Version------------------------------ ----------...google-cloud-storage           1.28.1google-cloud-videointelligence 1.14.0...You can use the main scope to test the function in scriptÂ mode:https://medium.com/media/a25fa778a61308c2584487ea157c1104/hrefNote: You have already exported the ANNOTATION_BUCKET environment variable earlier in the shell session; you will also define it later at deployment stage. This makes the code generic and lets you reuse it independently of the outputÂ bucket.Test the function:VIDEO_PATH="cloud-samples-data/video/gbikes_dinosaur.mp4"VIDEO_URI="gs://$VIDEO_PATH"python $PROJECT_SRC/gcf1_detect_shots/main.py $VIDEO_URILaunching shot detection for &lt;gs://cloud-samples-data/video/gbikes_dinosaur.mp4&gt;...Note: The test video &lt;gbikes_dinosaur.mp4&gt; is located in an external bucket. This works because the video is publicly accessible.Wait a moment and check that the annotations have been generated:gsutil ls -r gs://$ANNOTATION_BUCKET964  YYYY-MM-DDThh:mm:ssZ  gs://ANNOTATION_BUCKET/VIDEO_PATH.jsonTOTAL: 1 objects, 964 bytes (964 B)Check the last 200 bytes of the annotation file:gsutil cat -r -200 gs://$ANNOTATION_BUCKET/$VIDEO_PATH.json}    }, {      "start_time_offset": {        "seconds": 28,        "nanos": 166666000      },      "end_time_offset": {        "seconds": 42,        "nanos": 766666000      }    } ]  } ]}Note: Those are the start and end positions of the last video shot. Everything seemsÂ fine.Clean up when youâ€™re finished:gsutil rm gs://$ANNOTATION_BUCKET/$VIDEO_PATH.jsondeactivaterm -rf venvFunction entryÂ pointhttps://medium.com/media/61fd9a41967bfac4ab496eda6b118c78/hrefNote: This function will be called whenever a video is uploaded to the bucket defined as aÂ trigger.Function deploymentDeploy the 1st function:GCF_NAME="gcf1_detect_shots"GCF_SOURCE="$PROJECT_SRC/gcf1_detect_shots"GCF_ENTRY_POINT="gcf_detect_shots"GCF_TRIGGER_BUCKET="$VIDEO_BUCKET"GCF_ENV_VARS="ANNOTATION_BUCKET=$ANNOTATION_BUCKET"GCF_MEMORY="128MB"gcloud functions deploy $GCF_NAME \  --runtime python37  \  --source $GCF_SOURCE \  --entry-point $GCF_ENTRY_POINT \  --update-env-vars $GCF_ENV_VARS \  --trigger-bucket $GCF_TRIGGER_BUCKET \  --region $GCF_REGION \  --memory $GCF_MEMORY \  --quietNote: The default memory allocated for a Cloud Function is 256 MB (possible values are 128MB, 256MB, 512MB, 1024MB, and 2048MB). As the function has no memory or CPU needs (it sends a simple API request), the minimum memory setting isÂ enough.Deploying function (may take a while - up to 2 minutes)...done.availableMemoryMb: 128entryPoint: gcf_detect_shotsenvironmentVariables:  ANNOTATION_BUCKET: b2-annotations...eventTrigger:  eventType: google.storage.object.finalize...status: ACTIVEtimeout: 60supdateTime: 'YYYY-MM-DDThh:mm:ss.mmmZ'versionId: '1'Note: The ANNOTATION_BUCKET environment variable is defined with the --update-env-vars flag. Using an environment variable lets you deploy the exact same code with different trigger and outputÂ buckets.Here is how it looks like in the CloudÂ Console:Production testsMake sure to test the function in production. Copy a video into the videoÂ bucket:VIDEO_NAME="gbikes_dinosaur.mp4"SRC_URI="gs://cloud-samples-data/video/$VIDEO_NAME"DST_URI="gs://$VIDEO_BUCKET/$VIDEO_NAME"gsutil cp $SRC_URI $DST_URICopying gs://cloud-samples-data/video/gbikes_dinosaur.mp4 [Content-Type=video/mp4]...- [1 files][ 62.0 MiB/ 62.0 MiB]Operation completed over 1 objects/62.0 MiB.Query the logs to check that the function has been triggered:gcloud functions logs read --region $GCF_REGIONLEVEL  NAME               EXECUTION_ID  TIME_UTC  LOGD      gcf1_detect_shots  ...           ...       Function execution startedI      gcf1_detect_shots  ...           ...       Launching shot detection for &lt;gs://VIDEO_BUCKET/VIDEO_NAME&gt;...D      gcf1_detect_shots  ...           ...       Function execution took 874 ms, finished with status: 'ok'Wait a moment and check the annotation bucket:gsutil ls -r gs://$ANNOTATION_BUCKETYou should see the annotation file:gs://ANNOTATION_BUCKET/VIDEO_BUCKET/:gs://ANNOTATION_BUCKET/VIDEO_BUCKET/VIDEO_NAME.jsonThe 1st function is operational!ğŸï¸ VisualÂ SummaryCode structureItâ€™s interesting to split the code into 2 mainÂ classes:StorageHelper for local file and cloud storage object managementVideoProcessor for graphical processingsHere is a possible core function:https://medium.com/media/a93998518c177c34a6c86d85593fdb53/hrefNote: If exceptions are raised, itâ€™s handy to log them with logging.exception() to get a stack trace in production logs.Class StorageHelperThe class manages the following:The retrieval and parsing of video shot annotationsThe download of sourceÂ videosThe upload of generated visual summariesFile nameshttps://medium.com/media/c45a6726678a903e043c6329077814d5/hrefThe source video is handled in the with statement contextÂ manager:https://medium.com/media/eef563f23b6bcfc6caf00d35528c7f27/hrefNote: Once downloaded, the video uses memory space in the /tmp RAM disk (the only writable space for the serverless function). It's best to delete temporary files when they're not needed anymore, to avoid potential out-of-memory errors on future invocations of the function.Annotations are retrieved with the methods storage.Blob.download_as_string() and json.loads():https://medium.com/media/6af2853cabbc5c5c4429d00e1a87c8ad/hrefThe parsing is handled with this VideoShot helperÂ class:https://medium.com/media/d95d8f8b8a79dde2f0187f8edf4b94c4/hrefVideo shot info can be exposed with a getter and a generator:https://medium.com/media/32f9d146cee581b4cebc6f9abcb3c8f3/hrefThe naming convention was chosen to keep consistent object paths between the different buckets. This also lets you deduce the video path from the annotation URI:https://medium.com/media/da802bd2fde9d19c901cd7e5ba4b4eae/hrefThe video is directly downloaded with storage.Blob.download_to_filename():https://medium.com/media/56207a18cb0eb2250511f6581ced3dce/hrefOn the opposite, results can be uploaded with storage.Blob.upload_from_string():https://medium.com/media/bf5877b770412025188594002e5f61fa/hrefNote: from_string means from_bytes here (Python 2 legacy). Pillow supports working with memory images, which avoids having to manage localÂ files.And finally, here is a possible naming convention for the summaryÂ files:https://medium.com/media/3248045d7d7ec64ef8794070a06cf3bc/hrefClass VideoProcessorThe class manages the following:Video frame extractionVisual summary generationhttps://medium.com/media/a729b2940d13ac7e97016a08622b77b1/hrefOpening and closing the video is handled in the with statement contextÂ manager:https://medium.com/media/966117816753013e7647fc08971001d8/hrefThe video summary is a grid of cells which can be rendered in a single loop with two generators:https://medium.com/media/24491fd8f2a86539b574d4babf76100d/hrefNote: shot_ratio is set to 0.5 by default to extract video shot middleÂ frames.The first generator yields cellÂ images:https://medium.com/media/97dcbe6f13eeab56ee5c318c2f30fdab/hrefThe second generator yields cell positions:https://medium.com/media/8a04df38e6b81926cf63889b8303847f/hrefOpenCV easily allows extracting video frames at a given position:https://medium.com/media/042c493a92fa43c8c4076bf48c752a1d/hrefChoosing the summary grid composition is arbitrary. Here is an example to compose a summary preserving the video proportions:https://medium.com/media/6c3961789a20e1e78337737448cd5daa/hrefFinally, Pillow gives full control on image serializations:https://medium.com/media/028535063c6ee49659586b1ad95dd163/hrefNote: Working with in-memory images avoids managing local files and uses lessÂ memory.Local development andÂ testsYou can use the main scope to test the function in scriptÂ mode:https://medium.com/media/a8a7d0f626fcb78a072f9d4a5d66a752/hrefTest the function:cd ~/$PROJECT_IDpython3 -m venv venvsource venv/bin/activatepip install -r $PROJECT_SRC/gcf2_generate_summary/requirements.txtVIDEO_NAME="gbikes_dinosaur.mp4"ANNOTATION_URI="gs://$ANNOTATION_BUCKET/$VIDEO_BUCKET/$VIDEO_NAME.json"python $PROJECT_SRC/gcf2_generate
