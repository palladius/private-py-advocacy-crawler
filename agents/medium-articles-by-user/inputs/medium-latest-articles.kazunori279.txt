
== Article 1
* Title: 'AutoML Vision と RasPi でリビングのいろいろな音を認識する'
* Author: 'Kaz Sato'
* URL: 'https://medium.com/@kazunori279/automl-vision-%E3%81%A8-raspi-%E3%81%A7%E3%83%AA%E3%83%93%E3%83%B3%E3%82%B0%E3%81%AE%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E9%9F%B3%E3%82%92%E8%AA%8D%E8%AD%98%E3%81%99%E3%82%8B-a9d5d6a1a1b9?source=rss-4b21e207ea2c------2'
* PublicationDate: 'Tue, 08 Jan 2019 23:03:32 GMT'
* Categories: machine-learning, iot, google-cloud-platform

#raspi #おうちIoT #お手軽ML #gcpjaCloud AutoML Vision は、画像とその分類ラベルをクラウドにアップロードするだけで画像認識の機械学習モデルを作成できるサービス。ラーメン二郎のどんぶり画像から 95% 精度で店舗を当てたりできる高性能だけど、機械学習のディープな知識や経験がなくても使える。で、これでまず試してみたかったのが、音の認識だ。画像じゃなくて音。2 年くらい前に、画像認識用の CNN を使って音声を認識する論文が Microsoft Research から出てて、へぇーっと思った。スペクトログラムっていう、よく犯罪捜査で出てくる声紋のアレを使って音を画像にして、その模様から音の特徴を認識する。CNN で音声認識（Microsoft Research）なるほどなあ。。つまり、世の中のどんなデータでも、捉えたい特徴を画像の特徴として表せれば、画像認識で識別できる……ってことだ。そして、AutoML Vision で高精度の画像認識モデルを作れるということは、スペクトログラムによる音認識も、自分で TensorFlow で CNN いじったりしなくても簡単に試せるのか。まじか。と思った。そこで、年末年始のヒマな時間を使って試してみたら、4 日くらいでこんなのができた。https://medium.com/media/9b76ccb532b6d2e9e9a576f2175f5de4/hrefリビングで聴こえるいろんな音を RasPi のマイクで拾い、その種類を認識する。来客ベル、電子レンジの終了チャイム、人の声、キッチンで何かしてる音、くしゃみやせき、鼻をかむ、掃除機、ドアの閉まる音、その他、の 9 種類を検知できた。これは何に使えるか。例えば、お風呂が沸いた・ご飯が炊けた・来客ベルが鳴ったことを、IFTTT 経由の LINE 通知や Google Home の push 通知（非公式）でお知らせリビングで誰かがしゃべったり、掃除機かけたりといった生活行動を検知。Google Sheet でライフログ集めたり Nature Remo や Switchbot で家電を動かすくしゃみ・せき・鼻をかむ音の回数が多くなったらスマートスピーカーが「もしかしてカゼですか？ あったかくしてくださいね」と気づかい、加湿器が動き出し、アマゾンで勝手に薬を発注するってあたりが思いつく（それぞれ実利的かはおいといて）。リビングだけじゃなくて、オフィスや野外など、音が拾える場所ならどこでも使える。開発に必要なものは、 RasPi と USB マイク。学習に使う AutoML Vision は、そこそこの精度でいいなら無償枠で十分。5 万円かけてじっくり学習すれば、もっと精度の高いモデルができる。これを、ML 専門家でなくてもさくっと作れるのがいいところ。以下、この音認識ガジェットを作るためのおおまかな手順を紹介したい。今回は bash スクリプトですべて済ませた。コードはここ。RasPi3 で録音の設定まずは RasPi3 に USB マイクをつなげて録音する準備。この記事を参考に、arecord と sox が動くことを確認。いろいろ試したのち、arecord はこんな設定で落ち着いた。arecord --max-file-time 1 —-use-strftime raw_%Y%m%d-%H%M%S.wav \  -q -c 1 -D plughw:1,0 -r 16000 -f S32_LE &amp;-f S32_LE ：32 bit little endian の wav ファイルで保存-q ：いろいろ細かいメッセージは出さない-c 1 ：チャンネル数は 1（モノラル）-r 16000：サンプリングレート 16 kHz で録音-D plughw:1,0：lsusb で確認した USB マイクのカード番号 0 とサブデバイス番号 1--max-file-time 1 ：wav ファイルを 1 秒ごとに保存--use-strftime：wav ファイル名のフォーマットを指定（例：raw_20190101–120000.wav）これで、 ctrl-c で止めるまでずっと録音されっぱなしになり、1 秒ごとに wav ファイルが、raw_20190101-120000.wavraw_20190101-120001.wavraw_20190101-120002.wav...って感じに保存される。音が鳴ったら切り出すここからは、sox で音を加工するbash スクリプトを書いていく。sox は音を切ったり貼ったりたいていの音加工はなんでもできるスゴいツールだ。arecord から出てくる細切れの wav ファイルから、ある程度の音量の音だけを切り出したい。まずは 2 秒分をひとつの wav にマージ。sox ＜マージするファイル名のリスト＞ ＜保存ファイル名＞これで 2 秒間の wav ファイルができる。つぎに、この記事を参考に sox の silence フィルタを使って無音部分をカット。sox ＜元ファイル名＞ ＜保存ファイル名＞ silence 1 0.1 3%音量 3% 以上の音が 0.1 秒以上続いたところから後を保存する。ここで、1 秒間に満たないファイルができた場合は削除して処理終了。1 秒以上のファイルになった場合は、冒頭の 1 秒間だけを取り出す。sox ＜元ファイル名＞ ＜保存ファイル名＞ trim 0 1これで、「何かの音が鳴ったらそこだけ切り取って 1 秒間の wav ファイルにするスクリプト」ができた。1 秒間って長さは適当である。いずれにせよ、長さがバラバラよりも揃えておいた方が認識しやすいだろうなと思った。この bash スクリプトを半日くらい動かしたら、2700 個の wav ファイルが貯まった。例えばこんな音。https://medium.com/media/ae46bb0ee2d587496a56bbad5153054e/hrefhttps://medium.com/media/dcd249d9454a3004223f8ba5e711b3a6/hrefhttps://medium.com/media/8ea4004de70f1d50bc9419a75cf3b258/href音のラベル付けこの音にラベルを付けてくためのスクリプトを書いた。wav ファイルをひとつずつ再生して、ラベルを手入力する。$ ./label.shLabel for 20190107-094321_out.wav (r: replay, d: delete): rLabel for 20190107-094321_out.wav (r: replay, d: delete): bellLabel for 20190107-094322_out.wav (r: replay, d: delete): bell実際にラベル付けする時には、ひとつ前の音と同じラベルを入れることが多い。なので、ひとつ前のラベルをデフォルト値として出し、あとは改行だけでぽんぽんラベル付けできるようにした。ラベル付けにかかった時間は 1〜2 時間くらい。ラベル付けといっても大したことはしてなくて、ラベルと同じ名前のディレクトリにファイルを移動してるだけ。今回はこんなラベルを付けたけど、用途に応じて好きなラベルを定義すればよい。0：その他kitchen：キッチンで何かやってる音voice：しゃべり声door：ドアが閉まる音bell：来客ベルblow：鼻をかむ音cough：せきとくしゃみcleaner：掃除機microwave：電子レンジのできましたチャイム「お風呂が沸きました」や「ご飯が炊けました」の認識も試したかったけど、どちらも一日一回しか鳴らないので音をたくさん集めるのが難しく、まだやってない。音の data augmentation2,700 個の音が集まったけど、学習するには数が少ないなと思ったので、data augmentation すなわち学習データの水増しの方法を考えた。画像認識の場合は、元画像に対して回転・移動・変形・ノイズ追加・白色化など、いろいろ適用して数十倍に増やすやり方がよく使われ、 Keras 等ではそうした data augmentation 機能が簡単に呼び出せる。でも、音認識の場合はどうするんだろう。。とググったら、いくつか論文やライブラリが出てきた。音程を変えたり、ノイズを加えたり、いくつかのやり方が見つかった。しかし今回の用途では音源もマイクも変化しないので、音程を変えたりノイズ入れたりしてもあんまり意味なさそう。なので、今回は音に normalization をかける方法だけ使った。これは小さい音も大きな音も一定の音量になるよう揃える加工である。sox の norm フィルタを使い、for ((i=-7; i != -2; i++));  sox --norm=$i ＜元ファイル名＞ ＜保存ファイル名＞doneってループを書いて、-7 dB から -3 dB まで、それぞれの音量に normalize された 5 つの wav ファイルを作った。つまり 5 倍の水増しで、合計 13,500 個になった。この data augmentation が認識精度にどれだけ貢献しているのか……果たして他のフィルタの方がよいのか……めんどくさくて検証してないからわからない。俺たちは雰囲気で ML をやっている。スペクトログラム画像を作る学習データ作成の仕上げ。こんなふうにしてスペクトログラム画像を作る。sox なんでもできるな。sox ＜元ファイル名＞ -c 1 -n rate 16k spectrogram -r -h -0 \  ＜保存ファイル名＞-c 1：チャンネル数は 1（モノラル）-n：これ何だっけ。。rate 16k：16 kHz 音声データとして読むspectrogram：スペクトログラム画像を生成-r：デフォルトで画像内に追加される凡例を削除-h：色の派手な high-color 配色を使用。こっちの方が音の特徴が出るらしい-o：png ファイル名を指定ここまで、2,700 個の wav ファイルに対して normalization による水増しとスペクトログラム生成を RasPi3 上で実行すると、1〜2 時間くらいかかる。終わると、こんな画像ができる。ドアが閉まる音電子レンジの終了チャイムキッチンで何かしてる音これを人の目で「この音！」と見分けるのは難しい。電子レンジの音は特徴的な横線が入っててわかりやすいけど。AutoML Vision で見分けられるかやってみる。AutoML Vision にアップロードAutoML Vision を初めて使うときの手順はここで説明されているので省略。スペクトログラム画像を AutoML Vision にアップロードして学習を行うのだけど、その方法は 2 つある。zip にまとめてブラウザからアップロードする（画像ファイルを収めたディレクトリ名がラベルとして認識される）Google Cloud Storage に画像ファイルをアップロードしておいて、そのURI とラベルを並べた CSV ファイルをアップロードする今回はスペクトログラム画像が 13,500 枚、1.9 GB ぶんある。なんとかブラウザからアップロードできる大きさなので、てっとり早く前者の zip ファイル方式を選んだ。アップロードが終わると、AutoML Vision のダッシュボードに以下のようにサムネと読み込み件数が表示される。重複した画像は自動的に取り除かれるので、件数がすこし減っている。AutoML Vision に画像をアップロードしたところ各ラベルの件数も比較できる。ラベルごとの画像枚数普通に日常生活をしながら音を集めてると、どうしても偏ってしまう。そして後ほど実際に試してわかったけど、やはり集めた数の少ない音は認識精度も低くなる。なので、上記件数のうち bell や door は繰り返し録音して件数を増やしたものだ。ドアをなんどもバタンバタンしたり。繰り返し録音できない音はちょっときびしそうだ…… USB マイクをいくつもつなげて同時録音するしかないかな。また、data augmentation をもうちょっと工夫して、多すぎる音を間引いたり少なすぎる音を水増ししたりすれば、より精度が向上するかもしれない。AutoML Vision で学習学習用の画像とラベルが正しく読み込まれたことを確認したら、TRAIN タブをクリックして学習を開始。学習のご予算を選ぶここで 2 つの選択肢があって、1 compute hour：15分〜数時間で終わるお手軽モード。精度はそこそこ。毎月 10 回まで無償24 compute hour：24 時間かけて学習する Google の本気モード。学習データにぴったりな ML モデルを ML が自動作成する仕組みで、ML エキスパートがじっくり時間をかけてカスタマイズしたような精度が出る。1 回あたりおよそ 5 万円かかる（2019 年 1 月現在。料金表はここ）とりあえずは前者の 1 compute hour を試してみる。小一時間で学習が終わるので、EVALUATE タブでモデルの精度を確認できる。こんな結果が出た。AutoML Vision による音の予測結果（predicted label）と正解（true label）を比べた表これは混同行列っていう表で、AutoML Vision による音の認識テスト結果（predicted label）と正解（true label）を比べたもの。青いところがラベルごとの精度を表していて、お手軽モードにしてはわるくない。スペクトログラム画像で音の認識ってほんとにできるんだな、と思った。cleaner や bell なんかは高精度が出てるけど、cough と microwave はサンプルの少なさもあってあんまりいい精度が出ていない。ちなみにモデル全体の precision（適合率：認識が正しい割合）は 95.2%、recall（再現率：認識できた割合）は 76.4% であった。つまり、音が鳴ったとき検知できるのは 7〜8 割くらい。もうちょっと精度がほしい。で、Google の本気モードである 24 compute hours を試してみた。こんな精度。高すぎる……。precision/recall も見てみよう。precision は 99.8%で recall は 95.6%、ROC カーブはほぼ直角。これ過学習（モデルが答えを丸暗記してしまう現象）になってそう。この結果はこのまま鵜呑みにできないので、実際に音認識を試したときの精度を追って検証する。AutoML Vision の REST API を呼び出すモデルの学習が終わったら、AutoML Vision の REST API を呼び出すだけで、このモデルを使った画像認識（prediction）をすぐに行える。自分でサーバを立てて ML ライブラリやモデルを入れたりする必要はない。サーバレスって便利だ。PREDICT タブを開くと、こんなふうに呼び出せば使えるよという例が表示される。つまり、画像を base64 にエンコードして JSON に埋め込み、curl で POST すればよい。といっても、これを実行するまえに認証の設定を済ませておく必要がある。詳しい説明がこのドキュメントにあるけど、やるべきことは、Cloud SDK をインストールAPI を呼び出すサービスアカウントを作成、キーファイルを RasPi に入れておくサービスアカウントに AutoML の edit 権限を設定である。この準備ができたら、画像を base64 にしてリクエスト用の JSON ファイルに埋め込み、curl -s -X POST -H "Content-Type: application/json" \  -H "Authorization: Bearer `cat access_token`" \  https://automl.googleapis.com/v1beta1/projects/＜プロジェクトID＞/locations/us-central1/models/＜モデルID＞:predict \  -d @api_request.json &gt; api_response.jsonとして curl で API を呼び出す。今回はお手軽に済ませたのですべて bash スクリプトで 書いたけど、Python から API を呼び出すライブラリもある。REST API を呼び出して 3 秒ほどすると、認識結果のラベルとスコアが返ってくる。{  "payload": [    {      "classification": {        "score": 0.8718641      },      "displayName": "kitchen"    }  ]}これらを見て、RasPi のディスプレイに絵を描くなり、IFTTT で家電連携するなりすればよい。スコアが低い場合（0.5 とか 0.6 とか）は認識結果があまり当てにならないのでスキップしといた方がよい。これで音認識ガジェットのできあがり。音認識の精度とレイテンシとコストできあがったガジェットで、実際の音認識の性能をいろいろ検証してみた。実際の認識精度：実際にマイクを通してそれぞれの音を 10 回ずつ聞かせてみたところ、こんな結果になった。声：100%ドア：80%せき：70%来客ベル：100%電子レンジ：100%鼻をかむ：50%キッチン：70%掃除機：100%決まった音しか鳴らない電子レンジや掃除機、来客ベルは誤認識がほとんど起きない。これらの用途なら、今回の仕組みでそのまま実用になりそうだ。一方、ひとつひとつの音が毎回異なるもの、例えば鼻をかむ、せき、キッチンの音は、7 割程度に落ちる。とくに鼻をかむ音は、集めた音の数が少ない（あんまり何度も意味なく鼻をかむ気になれない）せいか半分しか当たらなかった。AutoML Vision の画面上で精度がとても高くなったのは、おそらく過学習のせいだろう。これらの認識精度を上げるには、もっともっと音のサンプルを集める必要がある。もうひとつ気づいたのは、複数の音が混ざると誤認識が多い点。これは data augmentation を工夫すれば改善するかもしれない。もとの音に対して、別の音や環境雑音をランダムに選んで小さな音でミックスする等。認識のレイテンシ：上述のとおり、AutoML Vision の画像認識は、高精度な一方でレイテンシが高い。us-central1 リージョンで動いているせいもあるけど、curl で呼んでから答えが返るまでに 3 秒くらいかかるので、即時性が要求される用途にはあまり向かない。来客ベルを検知してお知らせしてる間にお客さん返っちゃう認識のコスト：AutoML Vision の画像認識は毎月 1,000 回までは無償で、その後は 1,000 回の認識あたり $3 かかる。一日 1,000 個の音が検出されるとして、一日あたりおよそ 300 円くらい。お手軽さと高精度を考えればこんなものだろうけど、たくさん音が検出される用途ではコストが気になりそうまとめ冒頭で書いたように、この音認識ガジェットは正月休みの 4日間くらいで作れた。音の加工や data augmentation などの前処理は自分で工夫する必要があったけど、ML モデル開発でいちばんハードルの高い作業はスキップできる。つまり、いまどきのイケてるモデルを見つけて理解して TensorFlow で書いたり、 TF 学習環境を用意したり、ハイパラチューンしたり……これらはすべて AutoML まかせ。なので、ML のディープな知識と経験がなくても、precision/recall 等の精度の見方、検証方法や過学習の防ぎ方といった基本知識があれば扱える。このお手軽 ML で、身の回りのいろんな問題を解いてみたいなと思った。音認識だけでなく、センサーデータや時系列データ、ビジネスデータ等々、いろいろなデータの特徴を画像として取り出し、 AutoML Vision に入れるとどうなるか。人がぱっと見てわからないような特徴もばっちり捉えられるのだから、へーこんなことも分かるのか！ って使い方がたくさん出てくるはず。Disclaimer: この記事は個人的なものです｡ここで述べられていることは私の個人的な意見に基づくものであり､私の雇用者には関係はありません｡

== Article 2
* Title: 'obniz でつくる指紋認証ドアロック'
* Author: 'Kaz Sato'
* URL: 'https://medium.com/@kazunori279/obniz-%E3%81%A7%E3%81%A4%E3%81%8F%E3%82%8B%E6%8C%87%E7%B4%8B%E8%AA%8D%E8%A8%BC%E3%83%89%E3%82%A2%E3%83%AD%E3%83%83%E3%82%AF-d5f14ab2297d?source=rss-4b21e207ea2c------2'
* PublicationDate: 'Thu, 03 Jan 2019 08:00:05 GMT'
* Categories: iot

#obniz #おうちIoT #MLなしスマートロック Qrio Lock を使っているのだけど、ドアの開け閉めのためにスマホを取り出してアプリ起動するのは面倒である。Qrio Lock にはお出かけから帰ると GPS と BLE で帰宅を検知してドアを自動解錠する機能があるけど、これ調子がよくない時はドアが開くまでしばらく待たされることがあって、いまいち使えない。そこで、obniz で指紋認証部分を作り、Qrio Lock を解錠する仕組みを作ってみた。こんなふうに動く。https://medium.com/media/3302ddd0f4e5952c95f817a0cbfe466c/hrefobniz で指紋認証 → IFTTT の Webhook 呼び出し → IFTTT から Switchbot 呼び出し → Switchbot で Qrio Key を押す → 解錠 という流れである。Switchbot で Qrio Key を押すってあたりにむりやり感あるし、なんだか全体的にコストかかり過ぎな気もするが、そこは気にしないのである。コードはここ。指紋スキャナをつなぐまずは指紋スキャナを探した。いい感じの製品がスイッチサイエンスで売ってた。https://medium.com/media/d3b50bd98374d3d0c95211c9a9174c33/href4500 円くらい。200 個の指紋を登録できる。sparkfun のドキュメントを見るとシリアル（UART）で制御できるようなので、obniz の UART API でつながるはず。で、シリアルにどんなプロトコルを流せばよいか。メーカーが出してるプログラミングガイドを見た。こんなバイト列を送ればよいらしい。この Command code と Input parameter ってところに、指紋登録とか認識とかのコマンドやパラメータを入れればよい。なんだか難しくなさそうなので、まずはこのコマンド送信のところを書いてみた。最初にUART API を初期化して、uart = obniz.getFreeUart();uart.start({tx: UART_TX, rx: UART_RX, baud:9600, drive:'3v'});続いて送信するバイト列を little endian で組み立てる。  // start code and device id  const mask = 0xff;  var packet = [0x55, 0xaa, 0x01, 0x00];     // send the param  packet.push(param &amp; mask);  packet.push((param &gt;&gt; 8) &amp; mask);  packet.push((param &gt;&gt; 16) &amp; mask);  packet.push((param &gt;&gt; 24) &amp; mask);  // send the command  packet.push(command &amp; mask);  packet.push((command &gt;&gt; 8) &amp; mask);    // calc check sum  var sum = getChecksum(packet);  packet.push(sum &amp; mask);  packet.push((sum &gt;&gt; 8) &amp; mask);最後に、このバイト列を UART で送る。uart.send(packet);すると、UART API のコールバック関数に、以下のフォーマットでレスポンスが返ってくる。ここも、仕様どおりにデコードするコードを書く。  // receiving a response  if (data[0] == 0x55 &amp;&amp; data[1] == 0xaa) {        // check sum    var checksum = data[10] + (data[11] &lt;&lt; 8);    if (checksum != getChecksum(data)) {      console.log('Checksum error: ' + toHexString(data));      return;    }    // decode param and comm    respParam = data[4] + (data[5] &lt;&lt; 8) + (data[6] &lt;&lt; 16) + (data[7] &lt;&lt; 24);  }このコマンド送信とレスポンス受信のコードを使って、ガイドにあるコマンドを送る。例えば指紋認証を行うときは、まず CmosLed を送ってスキャナの LED を点灯させ、 CaptureFinger で指紋画像を保存、 Identify で指紋認証して指紋 ID を取得する、という流れだ。バイト列をエンコードしたりデコードしたりするの楽しい。指が押されたか検知するこの指紋認証のフローを開始する前に、まず指が押されたかを検知する必要があるけど、指紋スキャナにある IsPressFinger の反応がいまいち良くない。冒頭のビデオのように、LED がつけっぱなしになるし、指を置いてから 1 秒くらい間が空いてしまう。もっとさくっと検知したいのだ。obniz の対応ハードウェアリストに感圧センサがあったので、これを指紋スキャナの裏側に張って、圧力がかかったら LED を点けて指紋認証を開始するようにした。これで、認識開始までの遅延が 0.3 秒くらいに短縮できた。よしよし。管理画面をつくる指紋の登録や削除、認証テストのための UI を obniz の HTML で作る。obniz って、こういうちょっとした UI が簡単につくれるのがいいね。IFTTT の webhook で Switchbot を動かすQrio Lock には API がないので、カギの解錠と施錠はどうしようかと悩んだ末に、困った時の IFTTT 頼み。obniz から webhook をコールして、Switchbot で Qrio Key のボタンを押す。これでできあがり。使ってみた感想指紋認証してから実際にカギが開くまで 3〜4 秒かかる指紋スキャナの反応がいまいちで、3 回中 2 回くらい認証に失敗する ← バグを直したら認証に失敗しなくなった全体的にコストが高いobniz のブラウザ画面をつねに foreground にしとかないと JS のイベントループが 1 秒ごとの呼び出しになってしまい使いものにならない（Node.js に載せれば解決するだろうけど、UI 作るの面倒だ）obniz の電源を取るために 長い USB ケーブルを引き回して窓に挟めてるまあでも 手ぶらでカギ開けたり締めたりできるの便利（マンションロビーのオートロック問題があるけど）Disclaimer: この記事は個人的なものです｡ここで述べられていることは私の個人的な意見に基づくものであり､私の雇用者には関係はありません｡

== Article 3
* Title: 'obniz でつくるウェザークロック'
* Author: 'Kaz Sato'
* URL: 'https://medium.com/@kazunori279/obniz-%E3%81%A7%E3%81%A4%E3%81%8F%E3%82%8B%E3%82%A6%E3%82%A7%E3%82%B6%E3%83%BC%E3%82%AF%E3%83%AD%E3%83%83%E3%82%AF-dcc54c6b41d0?source=rss-4b21e207ea2c------2'
* PublicationDate: 'Thu, 27 Dec 2018 08:45:46 GMT'
* Categories: iot

最近、obniz をいじるのが好きで、2 種類のウェザークロックを作ったのでメモしておく（機械学習は出てこない）。シンプルなウェザークロック市販のウェザークロックって気圧変化で天気を予測してるから、ネットの天気予報のようなきめ細かな予測って難しいし、かといってネットにつながるウェザークロックは数万円する。ネットの天気予報を分かりやすく表示するクロックがほしいなと思ってたので、obniz の練習として最初に作ってみた。コードはここ。無料の天気予報の API をいろいろ探したところ、Dark Sky ってサービスがよさげだった。この API を jQuery の ajax で呼び出す。https://api.darksky.net/forecast/[key]/[latitude],[longitude]すると、指定した場所の天気予報が帰ってくる。GET https://api.darksky.net/forecast/0123456789abcdef9876543210fedcba/42.3601,-71.0589            {          "latitude": 42.3601,          "longitude": -71.0589,          "timezone": "America/New_York",          "currently": {              "time": 1509993277,              "summary": "Drizzle",              "icon": "rain",              "nearestStormDistance": 0,              "precipIntensity": 0.0089,              "precipIntensityError": 0.0046,              "precipProbability": 0.9,              "precipType": "rain",              "temperature": 66.1,              "apparentTemperature": 66.31,              "dewPoint": 60.77,              "humidity": 0.83,              "pressure": 1010.34,              "windSpeed": 5.59,              "windGust": 12.03,              "windBearing": 246,              "cloudCover": 0.7,              "uvIndex": 1,              "visibility": 9.84,              "ozone": 267.44          },...現在の天気のほか、毎時および毎日単位で向こう 7 日間の予報を JSON で返してくれるので、これをパースしてラベルやアイコンを表示するだけ。簡単だ。15 分に 1 回呼び出す設定にしたので、個人利用なら無料枠でずっと使える。obniz には OLED ディスプレイに文字を表示する API が用意されてるので、それを使って時刻と天気を表示する。これ、JS コードが動いてるブラウザの Canvas でビットマップを描いて送信する仕組みのようで、ブラウザが変わるとフォントも変化したりする。LED リングで作るウェザークロック次に、Adafruit の LED リングで天気予報を表示する時計を作った。コードはここ。Dark Sky API で得られる降水確率（precipIntencity）が高いと青、曇り確率（cloudCover）が低いと緑、それ以外は白でお天気を表示する。赤は時針だ。https://medium.com/media/3e2847d8966fdbaca59bed50dfc079dc/hrefLED リングは 1/4 ずつバラバラで販売されてるので、Adafruit のドキュメントを参考に、はんだ付けしてリングを作る。このdin を obniz の開いてる IO ポートにつなぐ。 5V と GND は obniz の USB ポートそばにある 5V と GND にはんだ付けする（LED の電源を IO ポートから取ると容量が足りなくなる）。個々の LED の制御ってどうやんのかな？ と思ってたけど、この LED リングは obniz が標準サポートする LED ドライバー WS2812 を使ってたので簡単に書けた。led = obniz.wired("WS2812", {din: 0});こんなふうにして初期化したら、led.hsvs([  [180, 0.5, 1],  [0, 1, 1],  ... ])ってふうに、HSV （色相、彩度、明度）の配列を LED の個数だけ送ると、その色と明るさで光ってくれる。簡単。分針や秒針 のトランジション表示面倒だったのは、天気予報を色で表示しつつ、時針、分針、そして秒針を重ねて表示するところ。パカパカ点滅するのはかっこわるいので、60 個ある LED のベースの色を天気予報に基づいて決めるその上に、時針、分針、秒針を重ねる個々の LED について、前の色から次の色への移行を 1/10 秒単位で更新するってコードを書いた。これで、秒針や分針が移動するときはふわーっと動く。環境の明るさに応じて輝度制御誰も見てない真夜中に煌々と光らせとくのもいまいちだし、日中の明るさに合わせて輝度を設定すると夜まぶしいので、明るさセンサを付けて輝度制御した。obniz の AD変換 で電圧を読み取って LED の輝度を調整するのだけど、室内の明るさと電圧変化の比率が昼と夜とでは全然変わる（というか人間が感じる明るさの度合いが非線形なのだろう）。昼の明るさは log を挟むようにした。また、安いセンサーなので夜間の微妙な明るさの違いを検出するのが難しかったけど、AD の電圧を積分するといい感じ。フォトフレームに入れる近所のケーヨーデイツーにぴったりな大きさのフォトフレームがあった。そのままでは LED が眩しすぎるので、紙を何枚か重ねて輝度を調節する。これでできあがり。Disclaimer: この記事は個人的なものです｡ここで述べられていることは私の個人的な意見に基づくものであり､私の雇用者には関係はありません｡

== Article 4
* Title: '寝かしつけについて'
* Author: 'Kaz Sato'
* URL: 'https://medium.com/@kazunori279/%E5%AF%9D%E3%81%8B%E3%81%97%E3%81%A4%E3%81%91%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6-252852c6e0a3?source=rss-4b21e207ea2c------2'
* PublicationDate: 'Thu, 20 Dec 2018 09:02:57 GMT'
* Categories: tensorflow, iot

この記事は TensorFlow Advent Calendar 2018 の 20 日目の記事です。最近、会社の同僚に子供が生まれた。深夜の夜泣きに目が覚めてしまうそうで、昼間も眠そうだ。苦労をお察しする。俺も息子が赤ん坊のころは夜中の 2 時に起こされて寝かしつけしてたのを思い出した。赤ん坊は生まれた時点で洗練された認知能力をすでに備えており、親のズルを許さない。ソファに座ったままだっこして揺らす、という行為は手抜きとして認識され、いつまでも泣き止まない。そこで、強い眠気でふらふらになりながらも赤ん坊を抱えて立ち、母親の胎内にいるかのようにゆ〜らゆ〜らと優しくゆりうごかしてあげると、こちらが意識を失うあたりで向こうも寝付くのだ。しかしここでゆっくりソファに座ると、たちまち赤ん坊の加速度センサーが手抜きを検知し、ふたたび泣き始めるのである。本稿では、この寝かしつけの苦労を obniz と TensorFlow.js で再現する手順を解説する。obniz はよくできた IoT デバイスだ。アマゾンで 6000 円ほど。箱を開けて USB 電源につないだら WiFi のパスワードを入れ、ブラウザ上で JavaScript コードを書いて helloworld するまで 5 分もかからない。書いたコードはブラウザで動いてる。要するにこれはいろいろな電子部品にブラウザ JS から WiFi 経由で読み書きするためのデバイスだ。これに、秋月で買った加速度センサーをつなぐ。セロテープで留めた。雑でいいのだ。obniz のドキュメントにあるコードをコピペして数行書けば、このセンサーが検知した x、y、z 方向の加速度を JS で読み出せる。// init sensorvar sensor = obniz.wired("KXR94-2050", { vcc:5, enable:6, gnd:7, z:8, y:9, x:10, self_test:11 });// get sensor valueswhile (true) {  let values = sensor.get();  console.log("x:" + values.x);  console.log("y:" + values.y);  console.log("z:" + values.z);  await obniz.wait(30);}生の数字を眺めても意味がわからない。three.js を使ってセンサーの値を 3D 表示してみた。下の黒い板はモバイルバッテリーである。https://medium.com/media/7df452e6af98fd58c96bd8834b5be1fb/href毎秒 10 回くらいで読める。時間をかけて 3D にしてみたが、かえって見づらいので後悔した。D3.js にしておけばよかった。昨晩、俺はこのデバイスを腕に抱え、12 年前の辛い時期のことを思い出しながら、こんなふうにしたっけな、と、寝かしつけをしていた。まあ今回は、そんなに長い時間をかける必要はない。1 分間で十分だ。ついでに、赤ちゃん大好き「高い高い」、いちばん嫌いな「床に放置」、そして「はげしく揺さぶる」も 1 分間ずつ記録した。CSV にして localStorage に入れておいた。TensorFlow.js は、JavaScript で動く ML ライブラリである。加速度センサーやら、なんちゃらセンサーやら、たくさんの数字を集めたって、人が書く if 文でできることはたかが知れているのである。だから、obniz を使い始めた当初から、TF.js と使うことを考えていた。x、y、z の 3 つの数字が時系列で流れてくるとき、そのパターンをどのようにして知ればよいか。x、y、z を画素の色、時間軸を画素の並びと捉えれば、この直線の模様を画像認識できればよい。画像認識といえば CNN である。直線だから 1 次元の CNN でよい。RNN や LSTM を使ってもいいけど、なんかめんどくさそうなのでやらない。ここに 1 次元 CNN のいい図があった。この赤い枠がフィルタというもので、太いシマシマや細いシマシマの模様のついた色眼鏡のようなものだ。この色眼鏡を少しずつずらしながら数字を見ていくと、模様にぴったり合うところで数字がばっちり見える。ここには太いシマシマがあるぞ、とわかる。そして、いろんな模様のフィルタをたくさん作り、それら全部使って数字を眺めていけば、どんな模様がどこにあるかがわかる。TensorFlow.js では、こういう 1 次元の CNN を作るための conv1d 関数があるので使ってみる。const model = tf.sequential();model.add(  tf.layers.conv1d({    inputShape: [20, 3],    kernelSize: 10,    filters: 50,    strides: 1,    activation: “relu”,    kernelInitializer: “randomNormal”  }));ここでは、センサーの値を 20 個（つまりおよそ 2 秒分）ごとに区切り、x、y、z の値を持つ [20, 3] の tensor として入力する。この直線の模様を見分ける長さ 10 のフィルタを 50 個用意する。この 10 とか 50 とかの設定の調整を本気でやると面倒だが、仕事じゃないので適当に済ませておく。学習がうまく行けば、TF.js のオプティマイザが、 x、y、z の変化のパターンをうまく見分けられるいい塩梅のフィルタを 50 種類作ってくれる。この conv1d 層から出てくる数字をざっくりまとめる maxPooling1d 層をつなげる。さらに、数字をわざとランダムに消すことで融通の利くモデルに育てるdropout 層、そして最後に、こんな模様の数字の並びはいったいどんな寝かしつけなのかを判断する dense 層をつなげる。conv1d_Conv1D1 (Conv1D) [null,11,50]max_pooling1d_MaxPooling1D1 [null,5,50]flatten_Flatten1 (Flatten) [null,250]dropout_Dropout1 (Dropout) [null,250]dense_Dense1 (Dense) [null,4]ちなみに、1 次元 CNN をもっときちんと理解してがっちり作るには、yuta さんが教えてくれたこのページが大変参考になる。Yuta Kashino on Twitter@kazunori_279 おお、kerasのlayerのConv1Dですか。これはどうでしょうか？https://t.co/vP3aJaalMBあと、学習をはじめたところ、どうやっても loss が NaN にしかならず、頭を抱えた。Kazunori Sato on TwitterNaNでだろ〜NaNでだろ〜♪（赤と青のジャージでそこで TF.js のオーソリティ、飯塚先生に見てもらったら、環境依存問題だったことがすぐに分かり助かった。ありがとうございました。4 種類の動作をそれぞれ 1 分間記録すると、 x、y、z が 20 個ずつ入った tensor が 128 個できる。このうち 100 個を学習用、28 個をテスト用に分け、学習を始めると、おおよそ 1 分くらいで 96% 精度を超える。07:40:57 Start training…07:41:05 loss: 1.24, acc: 0.5407:41:11 loss: 1.03, acc: 0.5407:41:18 loss: 0.88, acc: 0.5407:41:24 loss: 0.76, acc: 0.6107:41:30 loss: 0.67, acc: 0.6407:41:36 loss: 0.59, acc: 0.7507:41:43 loss: 0.52, acc: 0.8207:41:49 loss: 0.46, acc: 0.8607:41:55 loss: 0.41, acc: 0.9307:42:01 loss: 0.37, acc: 0.9607:42:08 loss: 0.34, acc: 0.9607:42:14 loss: 0.30, acc: 1.0007:42:14 Training finished.データが少ないから過学習してるかもだが、試してみた分にはそれっぽく動いてるのでよしとする。TF.js ではモデルの save 関数で local storage にモデルを入れておけて便利である。ここまでのコードはここに置いてある。ではこれを寝かしつけてみる。obniz そのままでは無骨なので、もふもふ動物ポーチに入れた。obniz の画面があるところを切り抜き、動きに応じた表情を見せることで、より寝かしつけ感を出してみた。こんな感じになった。https://medium.com/media/c1fc430e518da1ef75660584133e76c2/href動物は、机に放置されてるときは悲しいので、泣き顔である。しかし人が抱えてくれると泣き止む。好奇心まんまんである。はげしく揺さぶって見る。目を回した。高い高いすると喜ぶ。ゆっくりと、やさしく寝かしつける。やがてすやすやと眠りに落ちる。寝たかな、と思って、机にそっと戻すと、すぐ起きてしまうのだ。Disclaimer: この記事は個人的なものです｡ここで述べられていることは私の個人的な意見に基づくものであり､私の雇用者には関係はありません｡

== Article 5
* Title: 'Build the world’s largest IoT with RasPi and Google BigQuery'
* Author: 'Kaz Sato'
* URL: 'https://medium.com/google-cloud/build-the-world-s-largest-iot-with-raspi-and-google-bigquery-169b332d02b1?source=rss-4b21e207ea2c------2'
* PublicationDate: 'Fri, 17 Jul 2015 22:37:40 GMT'
* Categories: 

This is my weather station built with RasPi. It took only several hours in my weekend to build this, but it’s already capable of deploying the world’s largest IoT platform. Why? Because it directly sends the metrics to Google BigQuery, the massively parallel query engine from Google Cloud Platform which is able to collect one million rows of metrics every second and execute query on terabytes of data in 10 seconds.So, you can start deploying millions of this box to collect temperature, humidity and atmospheric pressure (or any metrics if you add sensors) from everywhere in the world, right now. I don’t have to do anything more to build a large distributed frond ends, load balancers, app servers and super-fast database cluster with scaling out, fail-over, replication and etc — the complications required to build a large production IoT platform. BigQuery has them all in Google’s largest Datacenter with Google scale and quality.The RasPi box runs a simple Python code to send the metrics to BigQuery that will be aggregated and shown as graphs on Google Spreadsheet.The total cost for building this is less than $100. You only need to buy the RasPi box and buy some sensors and that’s it. Google Spreadsheet is free. Google BigQuery is outrageously inexpensive: the storage cost is 20 cents / GB / month and the query cost is a few cents for querying on 100M rows each time. Most importantly, it’s a fully managed service. You don’t have to hire tens of senior engineers to build and operate the world’s largest big data cluster.Let’s take a look at how you can build this box in your weekend.Connecting Sensors to RasPiThe followings are the parts you need to buy:RasPi, NOOBS SD Card, USB WiFiUSB Power for RasPiDHT22, temperature and humidity sensorResistor for DHT 22 (5K to 10K ohm)LPS331, atmospheric pressure sensorBreadboard and jumper wiresBy using the NOOBS SD card, it’s so easy to setup RasPi. The USB WiFi should also be easy to set up with the OS’ config tool.For connecting the sensors to RasPi, you may refer to the following page:DHT Humidity Sensing on Raspberry PiFrom DHT Humidity Sensing on Raspberry PiNo special circuitry is required. You can connect GPIO pins of RasPi to the sensors with a breadboard. One caveat is that you have to keep DHT22 away from RasPi box to avoid the heat from CPU. For LPS331, you may use any other pressure sensors that can be attached to RasPi.Installing Drivers with AnsibleIt’s not so easy to write drivers for the sensors. Especially, DHT22 takes a little effort. But the cool thing about using RasPi is that you can find them on GitHub.Another cool thing is that RasPi is a Linux. You can use Ansible Playbook like the following to install drivers on GitHub for the sensors.    # Adafruit DHT drivers    - git: repo=git@github.com:adafruit/Adafruit_Python_DHT.git           dest={{ dht_dir }} accept_hostkey=yes      sudo: no    - command: python setup.py install chdir={{ dht_dir }}Although you may need to wait a while until finishing the Playbook execution :)Decode Sensor Values by PythonThe LPS331 pressure sensor uses I2C bus protocol for communication. With Python, you can use i2ctools command to read the values from the sensor.def cmd_exec(cmd):    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)    stdout, stderr = p.communicate()    if stderr != None and len(stderr.strip()) &gt; 0:        raise IOError("Error on executing cmd: " + stderr)    return stdout.strip()def i2cget(reg):    return cmd_exec("i2cget -y 1 " + LPS331_ADRS + " " + reg)The code let you execute i2cget command as a sub process. So that you can read values from I2C registors to calculate pressure values.def read_lps():    # reading from LPS    out0 = i2cget("0x28")    out1 = i2cget("0x29")    out2 = i2cget("0x2a")    # decoding the value    return (int(out0, 16) + (int(out1, 16) * 0x100) + (int(out2, 16) * 0x10000)) / 4096.0DHT22 temperature and humidity sensor could be read easily by using the Adafruit Python driver.# read humidity and temp from DHT humidity, temp = Adafruit_DHT.read_retry(Adafruit_DHT.DHT22, DHT22_GPIO)Use Fluentd to Send to BigQueryNow it’s ready to send the metrics to BigQuery. You can use Fluentd, the popular open source log collector, to do this. The following is the Ansible Playbook for installing Fluentd and its BigQuery plugin:    # Fluentd    - command: aptitude install ruby-dev    - command: gem install fluentd    # pip, fluent-logger-python, fluent-plugin-bigquery    - command: aptitude install python-pip    - command: pip install fluent-logger    - command: fluent-gem install fluent-plugin-bigqueryOn fluentd.conf, you can add the following config to receive the event log from fluent-logger-python and forward it to fluent-plugin-bigquery.&lt;source&gt;  type forward   port 24224&lt;/source&gt;&lt;match weather.**&gt;  type bigquery  method insert  auth_method private_key  email YOUR_SERVICE_ACCOUNT_EMAIL   private_key_path YOUR_PRIVATE_KEY_FILE_PATH   project YOUR_PROJECT_ID   dataset YOUR_DATASET   table YOUR_TABLE_NAME   time_format %s  time_field time  fetch_schema true  field_integer time&lt;/match&gt;In Python code, use fluent-logger-python API to send the metrics to Fluentd.    # write metrics to local fluentd    event.Event("metrics", {        "atmos": atmos,        "hum": humidity,        "temp": temp    })Create a Table and Key on BigQueryNext, set up BigQuery to receive the event log from Fluentd. If this is the first time for you to start using the query service, please take a look at the getting started guide to prepare a Google Cloud Platform project and BigQuery dataset.For this demo, I’ve created a BigQuery table to receive the metrics. You may need to add a column to hold IDs of each device if you want to collect metrics from multiple devices.[  {    "name": "time",    "type": "INTEGER"  },  {    "name": "hum",    "type": "FLOAT"  },  {    "name": "temp",    "type": "FLOAT"  },  {    "name": "atmos",    "type": "FLOAT"  }]By using bq command, execute the following line to create a table with the schema above.&gt; bq mk -t &lt;your-project-id&gt;:&lt;your_dataset&gt;.weather_report wr_bqschema.jsonThen, create a private key to connect from the RasPi box to BigQuery. At Google Developers Console, open APIs and auth menu and create a new Client ID for a service account.This will start downloading of the private key. Copy the key to RasPi box, and edit fluentd.conf to set private_key_path field to the path of the key file. Also, set email field to the email address of the service account.Execute a Query on BigQueryNow it’s ready to try. On RasPi, execute the following to initiate Fluentd.&gt; fluentd -c fluentd.confAnd open another shell to execute the python code.&gt; sudo python weather_report.pyIf it runs successfully, you would see anything on the console. Go to BigQuery Console and execute the following SQL.SELECT   LEFT(STRING(SEC_TO_TIMESTAMP(time)), 15) + '0:00' as time,     AVG(temp) as temp,  AVG(hum) as hum,   AVG(atmos) as atmos FROM [YOUR_PROJ:YOUR_DATASET.weather_report] GROUP BY time ORDER BY time DESCSounds like it’s working! This query aggregates the 10 min average metrics and sort them with the timestamp.Because it’s running on BigQuery, you could get the result of this query in 10 seconds even when you collect 100 billion rows of metrics. See this white paper to learn why it could be possible. In short, the service runs thousands of servers in parallel for every single query.Now you are ready to send any metrics of any number of IoT devices — sensors in appliances, cars, mobile devices, factory machinery and etc — to BigQuery without any concerns about the scalability and availability.Draw a Graph with Google SpreadsheetBy using Google Spreadsheet and its Google Apps Script, you can execute the BigQuery query from it and draw a graph from the result. Please look at Real-time log analysis using Fluentd and BigQuery to learn how to do this. No need to write your own script. You can just copy the sheet and script in the document and use it to draw a graph like this.As you have seen on this article, it’s so fun to play with RasPi, especially when you are connecting it with the powerful cloud toolings such as BigQuery and Fluentd. Even though it’s just a hobby in weekend, you could already get the fully managed, highly scalable and available IoT platform without spending much money.The sample code for this demo is available on my GitHub repo.Originally published at qiita.com.Build the world’s largest IoT with RasPi and Google BigQuery was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.
