
== Article 1
* Title: 'How I write the Google Cloud Platform Technology Nuggets newsletter'
* Author: 'Romin Irani'
* URL: 'https://iromin.medium.com/how-i-write-google-cloud-platform-technology-nuggets-newsletter-4c144ca47151?source=rss-802a4d428d95------2'
* PublicationDate: 'Tue, 02 Jul 2024 06:21:42 GMT'
* Categories: newsletter, google-cloud-platform, process

I write a fortnightly newsletter on the latest news in Google Cloud Platform. The news is strictly restricted to announcements from the official Google Cloud Platform Blog.Here is some data from this journey:I started writing this letter 3+ years back and initially it was a monthly newsletter that I thought of sending out to Google Cloud Platform Partners in India. I was part of the Partner Engineering team then in Google Cloud. This was sent out as an email.The frequency was then bumped to twice a month since the amount of news coming up on the official blog was increasing.I started publishing this newsletter via the TinyLetter newsletter platform and that served me well till 2nd half of 2023, when MailChimp decided to take the service down.I moved on to Substack and the newsletter continues to be sent to subscribers from there and I do publish it on Medium too.There are over 1000+ subscribers of the newsletter today. I have never actively tried to promote it though at times I do put it in a link or two to subscribe.The open rate for the newsletter is around 45–50%.On an average, I get 1 new subscriber a day.If you would like to subscribe to the newsletter, please read the latest edition of the newsletter here and subscribe to it too. Thank you in advance.I thought of documenting my process on how I write this newsletter and one of the motivations for writing this down has been questions being asked on how I write this newsletter in the age of Generative AI. A few even remarked if I am generating an auto-summary of these articles and simply passing it on to the reader. I am delighted to document the process here.Statutory warning: Each step is manual and intentionally kept manual. No Automation tools are harmed in the process.Step 0 : I know very little of Google Cloud PlatformThis is not necessarily a step in the process but nevertheless it is important to call it out. Some folks have assumed that I know most of the services, if not all in Google Cloud and more. Nothing can be further than the truth.As any practitioner in Google Cloud will tell you, the number of services have grown exponentially and there is no way, any human can know all of them or even claim to know them.What I try is to understand whats coming out in each of the services to the best extent I can. In fact, one of my selfish reasons for writing this newsletter is that I learn as much as you about these services and announcements. If not for this process, I would be poorer in my information about whats going on in Google Cloud.It is a humbling experience at times to read a blog post and find that I struggled to understand a majority of it. The process sometimes takes me through an adventure, where I do not know much about the service and then I read a bit more via its documentation, so that I get the gist of things. Nothing more and nothing less. There is nothing to prove to anyone here.Step 1 : Collect the blog posts for the specific periodSince I create a newsletter every 15 days, I have kept it simple and if I need to cover June 1–15, 2024 , then I simply visit the official Google Cloud Platform Blog and then go through each section manually to identify the posts for that period. What do I mean by each section? If you visit the blog, you will find the following navigation menu at the top (in the sample image below, you can see the top navigation title Solutions and technology and under that several areas). These areas form the different sections in the newsletter and I classify the news items as much as possible under those sections. Not all sections make it to every newsletter.My understanding of some of the services is very minimal and so I admit that some of my bias comes in when I recommend some article over the article, when it comes to getting included or not in the newsletter.Step 2 : Challenges in collecting the blog postsGoogle Cloud Blog has evolved over the last few years. What used to simply be a listing of blog posts by date is no longer the case. So when you visit the home page, there is no immediate way of knowing if the article was published in a specific time period (e.g. June 1–15, 2024). What this means is that I end up clicking each of the blog posts (opening them up in a tab) and then observing the dates.Step 3 : Filtering out some blog postsThere are reasons for doing this manually. I can easily automate some of this stuff but I have held off on that. Reason being that by the time I start this step of filtering it out or reading the articles, I have mentally gone through the titles of the articles a few times, once in the listing, then in the tab in which it is open. That kicks off a mental process of aggregating the different articles that will need to go into the newsletter. It also provides me with an overall sense on which could be big announcements, that I should not miss. For e.g. any specific training promotions, events coming up (Cloud Next, I/O) and more.There is no specific number of blog posts that I cover or length of the newsletter in words. I have never let myself be influenced by that. I usually will eliminate most customer stories from the newsletter, unless I find them interesting enough to let you spend some time on it. Partner, Google Maps, Productivity — these topics do get a less priority from me.So at this point , I probably have 20–40 tabs open with the individual blog posts and the next part is the most interesting part of the newsletter.Step 4 : Start the writing processI take each blog post i.e. every open tab now and read the post. I am privileged to bring you this newsletter and my promise to you is that I read each and every blog post, understand it to the best extent I can and then summarize it. There is no shortcut to this process and I wouldn’t want it to. I want to be authentic and this is the best that I can do i.e. read the article, summarize it for you.There are chances that for some reason, it doesn’t really strike a chord with me and I eliminate that blog post from making it to the newsletter. It happens and there is simply no way that I will mention each and every blog post that has been published during that period.My summarization typically would range from a couple of lines to a paragraph at times. I let my writing flow as if I am trying to talk to a person and telling them what is there in the blog post. I also pick an interesting image to go with the article. I do pick sentences at times as is from the blog post since its written well. I try to eliminate as many buzzwords as possible and keep my language simple. My expertise in English language is not too sophisticated in its use of words, so that helps me indirectly to keep things simple.At the end of it all, I read the newsletter. I am not known to be very picky at times with typos and grammar, so you will see that in the newsletter. I do try to see if there is a typo or two.For those who are curious about where I write this article. I used to write it in the initial days in a Google Doc. But since the last 2 years or so, I simply type it out as a story in Medium, which is then directly published.Step 5: Send out the newsletterI copy the story over from Medium to Substack, which is what I use for the newsletter. I immediately send it out to the subscribers and there is nothing specific that I do around which day of the week or time, would be the best to send out the newsletter. I haven’t paid those things much attention.I usually target to send the newsletter within 2–3 days of the specific period. So if the period is June 1–15, 2024, then I will try to send it out anywhere from June 16–18/19, 2024.On an average, if I have to this activity end to end for a specific newsletter, it takes me a total time of 3–4 hours.Hope you have enjoyed reading about the process. Let me know in the comments if you have any more questions on the same.In SummaryI am happy to let everyone know that while I am delighted to see the advent of Generative AI and how it can help all of us, I do not use any of that to auto-summarize or aggregate into the newsletter that you receive today.The joy of learning about a new announcement on Google Cloud, reading it, trying to understand where it fits in and summarizing it in a few lines to inform you on that, is a process that I enjoy bringing to my readers and no Generative AI tool comes even close to giving me that experience.Thanks for reading the newsletter and supporting it. If you’ve got some ideas on where it could go, please do let me know.

== Article 2
* Title: 'Google Cloud Platform Technology Nuggets — June 16–30, 2024 Edition'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-june-16-30-2024-edition-c5fb52d93f08?source=rss-802a4d428d95------2'
* PublicationDate: 'Mon, 01 Jul 2024 08:49:36 GMT'
* Categories: gcp-weekly, gcp-tech-nuggets, tech-nuggets, google-cloud-platform

Google Cloud Platform Technology Nuggets — June 16–30, 2024 EditionWelcome to the June 16–30, 2024 edition of Google Cloud Technology Nuggets.Please feel free to give feedback on this issue and share the subscription form with your peers.Containers and KubernetesIn GKE, the Cluster Autoscaler (CA) is the component responsible for automatically adjusting the size of your node pools based on demand. Not that you have to deal with it directly but the team behind the component works on continuously looking at improvements to make it better. Some of the recent improvements include improved memory usage, less CPU waste, fast homogenous scale up and more. Check out the post for details. Some of the becnhmarking results and improvements are highlighted below:Identity and SecurityThe first Cloud CISO perspectives for June 2024, three of the more promising use cases for AI in cybersecurity are discussed:AI for malware analysisUsing AI to boost SecOps teamsUsing AI to scale security solutionsGoogle Cloud has launched of Cloud KMS Autokey in preview. Cloud KMS Autokey automates key control operations for CMEK while at the same time, incorporating best practices to reduce toil while managing your own encryption key. Check out a detailed article that provides information on how these features work in detail and how you can get started with it.API ManagementIf Dialogflow CX is your choice of chatbot development platform, it is often recommended not to expose your bots directly and instead opt for an API Proxy layer that can manage and route your calls to your Dialogflow CX Agent. Apigee is a good solution that can help you do that. Check out the blog post on how you can configure a Reverse Proxy in Apigee to secure your Dialogflow CX Agents today.Continuing with Apigee, Custom Reports in Apigee, allow you to get insights into your API. Check out this blog post that highlights several custom reports that you can consider to troubleshoot production deployments, optimize performance, and enhance your overall API strategy.Machine LearningThere was a mega announcement around Vertex AI with significant advancements in models and enterprise platform capabilities with Vertex AI. Key announcements included:Gemini 1.5 Flash is now in GA with 1 million-token context window and extremely competitive pricing and performance.Gemini 1.5 Pro with 2 million-token context capabilities.Imagen3, Google’s latest image generation model is now available in preview for Vertex AI customers with multi-language support, built-in safety features like Google SynthID digital watermarking, and support for multiple aspect ratios.Claude 3.5 Sonnet model is available on Google Cloud.Context Caching feature is now available for both Gemini 1.5 Flash and 1.5 Pro.More factual responses via improved and expanded grounding facilities. More on that in the next section.Data ResidencyCheck all the details here.Enterprises have clearly marked out grounding of the models as a key requirement to ensure that the experience that their customers/users see contains results that are factual. Towards this, there are key improvements/expansions been down to the grounding capabilities in Vertex AI. These include:Grounding with Google Search feature will introduce dynamic retrieval, that will intelligently select when to use Google Search results and when to use the model’s training data.Vector Search will offer hybrid search, now available in Preview.Upcoming Partnerships with data companies to provide further grounding in terms of factual data.A high-fidelity mode that is designed to further reduce hallucinations.Check out the blog post for details.Anthropic’s newly released model, Claude 3.5 Sonnet is making waves across the Generative AI world with its capabilities and new benchmarks for reasoning, undergraduate-level knowledge, math, and coding. The model is available on Vertex AI via the Model Garden repository. Check out the blog post on its capabilities,which customers are using it and how you can use it today via Vertex AI.Gartner® has named Google as a Leader in the Magic Quadrant™ for Data Science and Machine Learning Platforms. Check out the blog post and download the complimentary 2024 Gartner Magic Quadrant™ for Data Science and Machine Learning Platforms.DatabasesIf you’d like to get a summary on various services in Google Cloud Databases in the month of June, you can check out this great summary blog post titled “What’s new with Google Cloud databases — June 2024 edition”. If you are working with Google Cloud Databases, this is a must read.There are several new features for Cloud SQL for MySQL, available in Preview. These include:Support for storage and similarity search of vector embeddings. K-nearest-neighbor (KNN) and approximate-nearest-neighbor (ANN) search between embeddings is supported.Gemini in Cloud Databases is available throughout now. You can use Index Advisor, address performance issues with Active Queries and more.Check out the blog post for details.Log-based Metrics is a neat feature available in Cloud Logging, where you can observe your logs, identify patterns in the log message and turn them into metrics. You can then use these metrics to configure alerts that can get triggered based on certain threshold values that you would like to define. The logs could be from various Google Cloud Platform services and specifically it would be useful to create log-based metrics based for your PostgreSQL and AlloyDB services. Check out this blog post that provides an example of how you can do that. If you are into monitoring for certain exceptions vis-a-vis usage of your database services, this would be a must-add feature.NetworkingWith the rise of Generative AI applications, organizations are looking at ways to address how best to host and serve these workloads to optimize of costs and cloud resources like CPU, memory, etc. If you are looking to assume that these are still web applications that are front-ending a model inferencing endpoint, that might not be accurate. Here is a good article that starts off with highlighting the differences between traditional web traffic and AI traffic and then talks about various innovations available across Google Cloud Networking and baked into Vertex AI, that you can look at today.Google Cloud’s IPv6 Hybrid Connectivity portfolio has seen some key announcements:IPv6 BGP sessionsPartner Interconnect IPv6IPv6-only HA-VPNCheck out the blog post for more details.Struggling with having your services up and available behind Google Cloud Load Balancing? How about some some troubleshooting tips for that? The article takes you into understanding health checks, the tools available at your disposal and tips. Check it out.Moving on from troubleshooting to benchmarking. In what is likely to be a series of whitepapers on best practices for network performance and benchmarking, the first article cover best practices for benchmarking Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) bulk flows. Check out the blog post for a series of white papers on this topic.Data AnalyticsVector Search in BigQuery enables semantic similarity search in BigQuery. A fantastic use case that gets enabled via this feature in BigQuery is to analyze logs and asset metadata stored in BigQuery tables. So if you are funneling your logs into BigQuery, you are all set to analyse a log entry, identify if it is an anomaly, compare it across previous instances and more.Check out this blog post that does a great job at highlighting differences between a text-based and semantic search, understanding the Vector Search feature in BigQuery and then looking at real-world cases in how you can use them.Here is an interesting use case. What if you had to finding and understanding reviews in a customer’s preferred language across multiple languages. Continuing with the previous news item of having Vector Search supported in BigQuery, the solution involves usage of BigQuery multilingual embeddings, vector index and vector search, to let customers search for products or business reviews in their preferred language and receive results in that same language. Check it out.Gartner® has named Google a Leader in the 2024 Magic Quadrant™ for Analytics and Business Intelligence. Check out the blog post and download the complimentary 2024 Gartner Magic Quadrant for Analytics and Business Intelligence Platforms.Cost ManagementAll cloud costs should be visible and allocated, spend should be efficient with no waste, and there are of course no surprise costs. That has been the philosophy of Google Cloud FinOps. At FinOps X 2024, several interesting announcements. Some of them included an update to the BigQuery view to match the latest FOCUS v1.0 Specification GA release, integrates carbon footprint reporting and Scenario Modeling for CUDs. Check out the blog post for more details.The previous paragraph talked about FOCUS. What is FOCUS? It stands for FinOps Open Cost &amp; Usage and is a technical specification that normalizes cost and usage billing data across cloud vendors. Think of it as a unifying specification for cloud billing data. Google Cloud has introduced:A new Looker Template for Focus v1.0 GAAn updated BigQuery view for Focus v1.0 GACheck out the blog post for more details.Developers and PractitionersGoogle Cloud supports six deployment archetypes: zonal, regional, multi-regional, global, hybrid, and multicloud. What is a deployment archetype? It is an architectural model that you use as the foundation on which to build cloud topologies that meet the business and technical requirements of your application. It influences your availability, latency, performance, and cost requirements.Check out this blog post that dives deeper and provides reference architecture material for each of them.DevOps and SREIf you are into Operations, you would be grappling with the task of understanding what Generative AI is and how it could apply to your daily tasks with Google Cloud or any other environment. Here is a great list of resources curated for a SRE role, to come up to speed with fundamentals of Generative AI and several codelabs that help you experience the power of Gemini. Take a look.Learn Google CloudHave you joined the Cloud Innovators program? The program provides you 35 credits every month to use towards courses and hands-on labs.Join the Innovators program at no cost today!Stay in TouchHave questions, comments, or other feedback on this newsletter? Please send Feedback.If any of your peers are interested in receiving this newsletter, send them the Subscribe link.Want to keep tabs on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.Google Cloud Platform Technology Nuggets — June 16–30, 2024 Edition was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'Google Cloud Platform Technology Nuggets — June 1–15, 2024 Edition'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-june-1-15-2024-edition-36c5d2628984?source=rss-802a4d428d95------2'
* PublicationDate: 'Mon, 17 Jun 2024 10:46:19 GMT'
* Categories: gcp-weekly, tech-nuggets, google-cloud-platform, gcp-tech-nuggets

Google Cloud Platform Technology Nuggets — June 1–15, 2024 EditionWelcome to the June 1–15, 2024 edition of Google Cloud Technology Nuggets.Please feel free to give feedback on this issue and share the subscription form with your peers.Get Learning today !Have you heard of Google Cloud Innovators? It’s a community of Google Cloud enthusiasts, where you get access to news and latest events on Google Cloud. This program is free to join and open to everyone.The program is now doubly attractive to anyone who is invested in the Google Cloud ecosystem via the announcement that all Google Cloud Innovators will get 35 no-cost learning credits every month! Yes, that’s correct every month that you can use to practice hands-on labs, access on-demand courses and more. These credits get renewed every month.Join the Innovators program at no cost today!InfrastructureIn a previous installment of this newsletter, we covered 5 ways that you could save costs while using Compute Engine in Google Cloud. The follow up to that highlights another 5 ways that you could save costs. Some of the recommendations include using Spot VMs, automate the process of turning machines on and off, letting go of external IP Addresses and more. Check out Part 2 of the series.The N4 and C4 machine series in the Compute Engine VM portfolio were announced at Cloud Next 2024. The N4 machine series is generally available today and the C4 machine series is available now in preview for Compute Engine and Google Kubernetes Engine (GKE). Check out the blog post that provides more details on these series.PartnersDeveloper Experience has a direct impact on creating modern applications, that in turn drives positive business outcomes. The SDLC (Software Development Life Cycle) is complex, filled with multiple tools, areas of possible attacks, plus we have regular context switching and disruption of the developer workflow. The recently announced Gitlab integration, a partnership between Gitlab and Google, highlights what a comprehensive solution looks like that addresses development efficiency, enhances security and overall improve software delivery. Check it out.Identity and SecurityEncryption and decryption between BigQuery and Sensitive Data Protection is now available. This makes it easy to protect and share sensitive data inside of BigQuery. Check out the blog post for more details.Security Talks 2024 is scheduled to happen on June 26, 2024. You can RSVP here at the official event site and check out the agenda too. It comes as no surprise that there are several sessions that discuss the intersection of AI and Security.A recent critical security vulnerability (CVE-2024–3094) discovered in the XZ Utils compression software affected sshd, a tool for secure remote access. This vulnerability could allow attackers to execute remote code on servers without requiring authentication. The blog post states that “While Google Cloud Compute Engine customers who rely on our supported public images are not affected by this vulnerability, those using custom images should refer to our security bulletin for remediation guidance.” The blog post also goes into recommending Identity Aware Proxy (IAP) and Security Command Center (SCC) , two tools that can help to strengthen your security posture and implement best practices for internet-exposed cloud resources.Containers and KubernetesWhen I heard the word GKE Compliance, I was not sure what it meant? Is it is set of standards defined and accepted by the community? Given the type of workloads, does it apply to some of them? From the blog post, it is a streamlined way to assess your GKE clusters and workloads against industry-standards, benchmarks and control frameworks, including:CIS Benchmark for GKE: The gold standard for secure GKE configurationsPod Security Standards (PSS), which offer both baseline and restricted profiles to protect your workloadsThe good news is that GKE Compliance is built into GKE and fully managed by Google. The GKE Compliance dashboard gives you centralized compliance reporting that is updated every 30 minutes, giving you a clear view of your compliance posture for your fleet of clusters.Machine LearningGoogle has been announced a Leader in The 2024 Forrester Wave™: AI Foundation Models for Language, Q2 2024, receiving the highest scores of all vendors evaluated in the Current Offering and Strategy categories. The full report can be accessed here: The Forrester Wave™: AI Foundation Models for Language, Q2 2024.Gemini Integration with Google Workspaces and Google Cloud is likely to see some innovation solutions/integrations being built out. How do you get started on the same with foundational code that you could use to potentially implement some ideas that you might have for integrating the two services. Check out this blog post that show how to integrate Gemini and Google Sheets with BigQuery.How do you monitor models deployed on any serving infrastructure (even outside of Vertex AI, e.g. Google Kubernetes Engine, Cloud Run, Google Compute Engine and more)? As the blog post states, that’s what the new Vertex AI Model Monitoring is for.Finally, looking to implement RAG using BigQuery and LangChain? This practical guide gives you a step by step process to implementing it.Storage and DatabasesCloud SQL for PostgreSQL now supports PostgreSQL 16. This new version brings in improvements in observability, performance, replication and more. Check out the blog post for more details.With the recently announced LangChain packages for Cloud SQL for PostgreSQL, developing your AI app can receive a significant efficiency boost. Check out a blog post that highlights these LangChain integrations for using Cloud SQL for PostgreSQL as a vector database and if you’d like, there is an accompanying notebook too.While on the topic of Cloud SQL, the rapid pace of integrating AI features into the databases continues. Several new features for Cloud SQL for MySQL, available in Preview, have been announced that help companies power their database and applications with AI. You can now use Vector Search to build generative AI applications and integrate with MySQL with the support for Vector Embeddings in Cloud SQL for MySQL. Additionally, with Gemini integration you can get advise on Indexing, debug performance issues in queries and more. Check out the blog post for these interesting features.Running your enterprise workloads seamlessly across Google Cloud and Oracle Cloud Infrastructure (OCI) would have been something that one could not have imagined a while back. Well, this is going to be a reality with the recent announcement. Central to this concept is what is termed as Oracle Database@Google Cloud, where Oracle will directly host, operate and manage Oracle database services natively within and from the Google Cloud data centers, beginning with regional footprints in North America and Europe, and plans to rapidly expand globally. Read the blog post for more details and the upcoming integration.Data AnalyticsWe are increasingly seeing the integration of Generative AI features across various Google Cloud services and BigQuery has definitely seen tons of features that make using Gen AI models inside of BigQuery a breeze. A popular use case has been that of Sentiment Analysis and this is an excellent blog post that dives into showing you how you can use Gemini models in BigQuery for that. Right from creating the data to extracting the themes and visualizing it, the blog post has it all.Looking to generate synthetic data for facilitating the training of machine learning (ML) models or the evaluation of mathematical models? BigQuery DataFrames allows you to generate artificial data at scale. It is open-source python package providing pandas-like DataFrame and scikit-learn-like ML library for big data. It utilizes BigQuery and the rest of Google Cloud as the storage and compute platform under the hood. Check out the blog post for a step by step execution on how you can do that. There is accompanying notebook too.Continuing with BigQuery DataFrames, here is another blog post that demonstrates how you can combine BigQuery DataFrames with the visualization capabilities of CARTO to build out complex mapping applications. The solution utilizes pydeck-CARTO, a Python library that renders deck.gl maps in a Jupyter notebook.The name DataSageGen could be interpreted in multiple ways but as per the blog post, DataSageGen is a chatbot designed to be a personal guide to access and process information from a vast array of sources, including:Data and AI product documentationBlog posts and white papersCommunity knowledgeProduct and event announcementsHow do you build such a bot? The blog post gives a step by step guide to doing so. There is a Github repository too.BigQuery news has been heavy in this edition of the newsletter and to round it off, BigQuery User Defined Functions (UDFs) from the bigquery-utils repo are now available in all BigQuery regions.Developers and PractitionersCloud Run, in my opinion is one of those services on Google Cloud that is so well suited to developer requirements of having a runtime that can cost applications easily and which can scale via its serverless nature. With the increase in the number of developers trying out AI services, Cloud Run is still a service that is possibly their best bet to get their AI applications out there to the world. The usual questions once you move your application to production includes whether the application will scale? Will we have enough monitoring features? How about revisions to A/B test out some scenarios? Check out this blog post that explains these points beautiful and a quick run through of taking a sample code from the Vertex AI studio and packaging it as a Flask application to deploy on Cloud Run. Probably the fastest way, eh ?Earlier in this newsletter, we mentioned about a new post that highlights 5 more ways to save on Compute Engine costs. It looks like that “5 more” series seems to have caught on with everyone and we have a post titled “5 more myths about Platform Engineering”. This series is an excellent way to understand Platform Engineering and bust the myths one at a time. This part grounds everyone in their expectations on what Platform Engineering can solve, reduce dramatically and more.DevOps and SRELooking to understand and learn more about building reliable systems but feeling a bit lost with the amount of material out there. Google Cloud has curated a list of resources to keep the focus and help you in this journey. Check out the blog post that provides research papers, blogs and books that you could look up to get up to speed on systems engineering.Learn Google CloudInstead of a specific course or material to read up in this edition, this is a reminder again to join the Cloud Innovators program, that will provide you 35 credits every month to use towards courses and hands-on labs.Join the Innovators program at no cost today!Stay in TouchHave questions, comments, or other feedback on this newsletter? Please send Feedback.If any of your peers are interested in receiving this newsletter, send them the Subscribe link.Want to keep tabs on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.Google Cloud Platform Technology Nuggets — June 1–15, 2024 Edition was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 4
* Title: 'Google Cloud Platform Technology Nuggets — May 16–31, 2024 Edition'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-may-16-31-2024-edition-8c5dfe87fcf5?source=rss-802a4d428d95------2'
* PublicationDate: 'Mon, 03 Jun 2024 06:32:16 GMT'
* Categories: tech-nuggets, google-cloud-platform, gcp-weekly

Google Cloud Platform Technology Nuggets — May 16–31, 2024 EditionWelcome to the May 16–31, 2024 edition of Google Cloud Technology Nuggets.Please feel free to give feedback on this issue and share the subscription form with your peers.InfrastructureA Google Cloud incident involving UniSuper in Australia, gained widespread attention last month. Both CEOs made a statement about the incident and there were lot of speculations on what happened, and customers were rightly concerned if this would occur to them to. Google Cloud has published a detailed blog post on what happened, how the recovery was done and remediation.Are you running your workloads on Compute Engine and have understood various ways to save money on Compute Engine costs? It helps to take a look at a post that highlights 5 ways you can save money on Compute Engine. It includes looking at Auto Recommendations, Committed Use Discounts and more.Google Cloud continues to invest in infrastructure across Africa. This includes Umoja, the first ever fiber optic route to directly connect Africa with Australia. Check out the post for more details.Containers and KubernetesRunning highly scalable services on GKE and finding it tough to allocate network space in a single VPC, and rapidly running out of IP addresses? Check out a design to completely reuse the IP space across your GKE clusters.Identity and SecurityGKE Security Posture Dashboard has been enhanced with software supply chain security insights for your Google Kubernetes Engine workloads. It includes insights into vulnerabilities and workload configuration checks. The dashboard also clearly points out which workloads are affected by a security concern and provides actionable guidance to address it. Check out the blog post for more details.There are a couple of CISO bulletins for this month. The first CISO bulletin for May 2024 contains a summary of the discussion on a range of topics with the Mandiant CEO, at the RSA Conference fireside chat. The second one takes a look back at the year and offers a glimpse into cybersecurity trends for the year.Your CI/CD pipelines in respective tools may not be the best places to store your secrets. It is essential to use a dedicated secret manager for the same. Check out this Mandiant analysis of how attackers got a hold of secrets inside of CI/CD pipelines in Bitbucket.NetworkingGoogle Cloud global front-end released last year had got new updated that were announced at Cloud Next ‘24. As a refresh, the global front end solution consists of the Cloud External Global Application Load Balancer, Cloud CDN and Cloud Armor. The blog post goes into the details of these new features and those include Service Extension Callouts, which allows for more programmability at the Load Balancer layer, custom error responses and more.What is FQDN (Fully Qualified Domain Name)? How does it fit in within the Cloud Next Generation Firewall (NGFW)? How does it help in ease of management with Firewall Rules configuration? Check out this blog post to know more.Image Reference: https://en.wikipedia.org/wiki/Fully_qualified_domain_name#/media/File:DNS_schema.svgMachine LearningTo tune or not to tune LLMs is the question? What are the different methods available to us today to tune, ground the LLM responses to our data. Check out this post and the useful decision tree below.Are you a gaming developer focused on building out your games and facing a crunch when it comes to designing game assets. Generative AI to the rescue again. Check out how gaming developers can utilize a range of models available on Model Garden on Vertex AI and explore the potential for scaling game-asset creation.Are you using Google’s foundation models as is and not happy with it being able to return results that are probably just a Google Search away, especially with recent events, etc. You can now ground your model results with Google Search and the process is dead simple as this blog post demonstrates.Ray, a powerful distributed Python framework, with Google Cloud’s Vertex AI is now generally available (GA). Check out this blog post that highlights the why Ray on Vertex AI, Getting Started with Ray on Vertex AI and ton of other resources.CloudSQL for PostgreSQL is pushing the envelope when it comes to making it easier for developers to build out Generative AI applications without the complexities. Two things are making this possible. First was the The pgvector extension adds support for vector types along with various Approximate Nearest Neighbour (ANN) index types such as IVFFLAT and HNSW. And now with google_ml_integration extension , that removes the need for external pipelines to integrate your database with LLMs for embedding generation during indexing and search, all with the familiarity and transactional guarantees of SQL. Check out the blog post that highlights the case with an example.DatabasesCloud SQL has introduced extended support for MySQL and PostgreSQL end-of-life versions. This means that even if you are using a version of the databases that have reached end-of-life, you can opt-in for extended support (a paid service) that will give you the flexibility to migrate at your pace while still giving you security and bug fixes, SLAs and more. Check out the blog post for versions supported and note that pricing is scheduled to be announced in August 2024 as per the post.Data AnalyticsLooking to launch campaigns quickly, efficiently, and with a level of personalization that was previously impossible, leading to increased engagement, conversions, and customer satisfaction? That’s what the blog post states and provides you with a deep dive on how you can use multimodal large language models (LLMs) in BigQuery to create and launch a marketing campaign. Check out the blog post for more details.We have been reading about how RAGs (Retrieval Augmented Generation) can help to overcome some of the limitations of LLMs. How about an article that gives not just another overview of LLMs, how RAGs help but to create a RAG solution that combines with vector search and BigQuery, thereby tapping into domain-specific knowledge, real-time information and more. Check out the solution.BigQuery has announced the public preview of numeric search indexes, which enables optimized lookups on INT64 and TIMESTAMP data types. As the blog post states, “ the EQUAL(=) and IN operations on these data types can utilize search indexes to reduce byte scans for improved performance. So now your lookups for account IDs or transactions IDs or log timestamps can get faster and cheaper.” Check out the blog post for more details and a demonstration of the gains on real data, showcasing index creation and queries on a 100TB log table.Dataflow continues to see significant enhancements to ensure that it remains a solid platform for your streaming applications that need to now make that data available to AI systems for analysis and action. New features have been added to Dataflow ML to allow for support of common machine learning use cases. This includes Dataflow’s new right fitting allows users to mix-and-match compute types to only use GPUs when necessary, reducing cost. At Next ’24, there was a preview announced of continuous queries in BigQuery, that allows for users to now directly create stream processing jobs to create real-time change streams based on the latest data coming into BigQuery. Check out the blog post for more details.Did you know that BigQuery includes native CDC(Change Data Capture) support? Check out a blog post that highlights BigQuery’s new CDC capability in Dataflow along with the new Dataflow at-least-once streaming mode to drastically simplify your CDC pipeline and reduce costs.Developers and PractitionersI am assuming that you are now firmly in Artifact Registry land when it comes to storing, managing, and securing your build artifacts. What if you have multiple projects across an organization and are looking to share the Artifact Registry artifacts outside of the project? Check out this relevant solution which might be a good thing to implement across your organization.DevOps and SREIf you have been using SLOs feature within Google Cloud Monitoring, you would have used to seeing a list of services that are auto-discovered and provided to you to monitor on SLO Metrics. The services auto-discovered were from GKE, App Engine and Cloud Run. This has changed, you will need to add these services manually from now on. Check out the post for more details.Learn Google CloudPlatform Engineering has been garnering a lot of mindshare of late. We all have our own definition of Platform Engineering and have assumed a few things about Platform Engineering that are likely to be busted in this blog post. The post does a great job to explain Platform engineering by focusing on a few myths but make no mistake, the post explains key points via Google Cloud Platform products and that really helps to map these concepts to actual products. Highly recommended reading.Stay in TouchHave questions, comments, or other feedback on this newsletter? Please send Feedback.If any of your peers are interested in receiving this newsletter, send them the Subscribe link.Want to keep tabs on new Google Cloud product announcements? We have a handy page that you should bookmark → What’s new with Google Cloud.Google Cloud Platform Technology Nuggets — May 16–31, 2024 Edition was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 5
* Title: 'A Tale of Two Functions : Function calling in Gemini'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/a-tale-of-two-functions-function-calling-in-gemini-9b7fd8ae031b?source=rss-802a4d428d95------2'
* PublicationDate: 'Tue, 28 May 2024 05:15:07 GMT'
* Categories: llm-function-calling, google-cloud-platform, gemini, generative-ai, machine-learning

A Tale of Two Functions : Function calling in GeminiBuilding a Generative AI application comes with its own set of interesting challenges. This is especially true when you are trying to use the foundation models in combination with your own data. I am sure that you would agree if you could use the power of natural language understanding and generation of a foundation model and use that to infuse its responses with your own data served via your applications APIs, it would make for a very powerful combination.Function calling, as they call it, is the ability to tell the model to determine which of the functions should be called to meet the request that you have sent to it, via a prompt. I probably made a terrible attempt to describe the feature in my own words but let me augment that with what some of the vendors say:Connect large language models to external tools. [Reference]Define custom functions and provide these to a generative AI model. [Reference]Define the set of tools you want Claude to have access to, including their names, descriptions, and input schemas. [Reference]While there are several articles and tutorial on Function calling, my goal in this blog post is the following:Describe the problem that we are trying to address and fit that within the Function calling feature.Explore how it works with Google Cloud Vertex AI’s Function calling feature.This by no means is a definitive tutorial on this subject and my goal is to highlight how it works step by step and convince myself that it works too. All the code samples are in Java language (not Python). Its not about a language preference here but I’d like to bump up more samples in Java language. If you choose any other language for which the Vertex AI client library is available, you should ideally be able to port the code accordingly.If you prefer to jump to the code, you can hop over to the Github repository on the same.GitHub - rominirani/gemini-function-calling: Java sample code to demonstrate Gemini Function CallingWhat are we trying to do?I have found it a bit difficult to wrap my head around Function calling, when articles define the feature instead of using an example straight up (which I have been guilty of doing in this article too :-)). So let’s understand what we are trying to do first.Inventory Application APIConsider that I have an inventory API that has been designed, implemented and deployed for my distribution company. The inventory API provides me information on the following two high level entities:Warehouse InformationCurrent Inventory quantity in a specific WarehouseFor simplicity, let us consider two magical functions (the implementation is not important here), that have the following signatures:String getInventoryCount(String productId, String location)String getWarehouseDetails(String location)Given this, we would like to create a Chatbot application for the organization, that allows anyone to use any of the following prompts to get the information on either the warehouse or specific inventory available at a specific warehouse location.Sample prompts that one can provide are given below:How much of P101 do we have in warehouse w101?Where is warehouse w1 located?”Where are warehouse w1 and w2 located?How much of P1 and P2 do we have in warehouse w10?What is the inventory P1, P2 and P3 in warehouse w101?Where is warehouse w10 located and how many unit of p1 are there?You would agree that if we simply give these prompts to a foundation model, it is going to respond saying that it is not able to answer this question or worse, it might even end up giving hallucinating.What we want instead is a way for the model to interpret the prompt and determine that our functions/tools/APIs (pick your terminology) need to be invoked and then the response needs to be formatted back and given back as the prompt response.Two points arise over here:How does the LLM know which function/tool to invoke?Will the LLM invoke it for us automatically or ask us to invoke it?More questions follow and for that let us consider the scenarios via the sample prompt.Where is warehouse w1 located?”This prompt clearly indicates that the LLM needs to consider calling the getWarehouseDetails function with w1 as the value for the input parameter location.How much of P1 and P2 do we have in warehouse w10?This would require two invocations of getInventoryCount function, once with productid of P1 and location of w10 and next with productid of P2 and location of w10.Where is warehouse w10 located and how many unit of p1 are there?This is an interesting scenario. This would require three function calls. 2 times for getInventoryCount and 1 time for getWarehouseDetails.Still more questions arise.When there are 2 or three function calls to be made, would it ask us to invoke them one by one ?Is there a possibility of asking us to invoke all the functions in parallel. We sure can make those calls to our functions or APIs .. isn’t it ?Is there a way to tell the LLM to only consider functions and not try to answer it otherwise? There could be a lot more to those prompts than the one that we have provided. Can we provide some hint , just like how we do for Temperature, tokens, etc to control whether the LLM can consider only functions, or any or none.Interesting possibilities, isn’t it? My colleague Guillaume Laforge taught me a beautiful thing about these prompts. If you look at these prompts, they are very specific and map one-to-one with the functions that we have available. For e.g. how much inventory of P1 maps to getInventoryCount and where is warehouse w1 located maps to getWarehouseDetails.What if you could give a prompt that says:What is the total inventory of P1 and P2 in warehouse w1?Do we have more of P1 or P2 in warehouse w1 ?This is the real power of LLMs to do this kind of logical reasoning once it has received the responses from our API.Now that we are clear on what we are trying to do and the different combinations, let us revisit what Function Calling is.What is Function calling?I will take the liberty of borrowing information from a couple of documentation sources here. First up is the essence of what Function calling is and I really like the step by step description that Anthropic provides and I reproduce it here. You can replace the word tools with functions or Claude with your favorite foundation LLM that supports function calling. I have modified the steps a bit to keep them simple.Using tools with Claude involves the following steps:1. Provide Claude with tools and a user prompt: (API request)Define the set of tools you want Claude to have access to, including their names, descriptions, and input schemas.Provide a user prompt that may require the use of one or more of these tools to answer, such as “What is inventory of P1 in warehouse W1?“.2. Claude uses a tool: (API response)Claude assesses the user prompt and decides whether any of the available tools would help with the user’s query or task. If so, it also decides which tool(s) to use and with what inputs.Claude constructs a properly formatted tool use request.3. Extract tool input, run code, and return results: (API request)On the client side, you should extract the tool name and input from Claude’s tool use request.Run the actual tool code on the client side.Return the results to Claude by continuing the conversation with a new user message containing a tool_result content block.4. Claude uses tool result to formulate a response: (API response)After receiving the tool results, Claude will use that information to formulate its final response to the original user prompt.Steps (3) and (4) are optional — for some workflows, Claude using the tool is all the information you need, and you might not need to return tool results back to Claude.If you are a visual person, I reproduce the diagram from the official Google Cloud Vertex AI Function calling documentation.Let’s try to answer some of the questions that we had earlier.How does the LLM know which tools or functions to use? We provide a list of tools or functions that the LLM can use. The information we provide about the tools needs to be clear and precise in what the function definition is, the parameters, the data types and clear descriptions for what the function does and what each parameter is. [Step 2].This is one of the best practices and I recommend that you read this.Will the LLM invoke the tools or function for us? No. The LLM does not invoke the tool or the function for us. It provides us the function and parameters to use and it is upto us to invoke our own functions/methods/APIs and return the results back to the LLM. [Steps 3,4,5,6]Source code repositoryThe source code for these experiments is all available here.GitHub - rominirani/gemini-function-calling: Java sample code to demonstrate Gemini Function CallingThis is a standard Maven project with a pom.xml in the root folder, so you should be able to take the dependencies from there, should you want to recreate it in the different way.There are two Java main programs available:1.src/main/java/com/geminidemo/AutomateFunctionCalling.java and2. src/main/java/com/geminidemo/ParallelFunctionCalling.javaTo run the programs, you will need to do the following in the respective Java files:Replace YOUR_GOOGLE_CLOUD_PROJECT_ID with your Google Project IdReplace us-central1 with another Google Cloud location should you want to change that.In the main() method, you will find the different prompts all commented out. Uncomment any of the prompts before running the program.We will come to each of the above programs and understand them in the following respective two sections.Sequential Function callingThe source code file for this is :src/main/java/com/geminidemo/AutomateFunctionCalling.javaWhen I started off with my experiments on Function calling, I was using the Gemini Pro 1.0 model. We will focus on this model in this section.We will then look at what Gemini Pro 1.5 model enabled (Parallel Function calling) in the next section. This will help us understand some additional questions that we had raised in an earlier section and which I reproduce here:When there are 2 or three function calls to be made, would it ask us to invoke them one by one ?Is there a possibility of asking us to invoke all the functions in parallel. We sure can make those calls to our functions or APIs .. isn’t it ?https://medium.com/media/6a715aa9952f154b1075cc91eb906b89/hrefLet’s understand the source code first:The main method provides a list of prompts that we have discussed. You can uncomment any of them before you run the application.Look at line 182–192. It contains the two sample methods getWarehouseDetails and getInventoryCount. These just return dummy data for now but in the real world, these could be invoking your actual APIs or implementation code.Lines 49–53 , we are just building a map of function names and the methods that we need to invoke. This will be useful once Gemini tells us which function name to invoke.Lines 57–90 defines the Functions. So we define that as per the OpenAPI Spec. We give the function name, description, each of the parameters (name, type, description).Line 92–96 is critical. Here we are building the Tools i.e. adding these existing functions and we will be passing this to the LLM along with our prompt, so that the LLM can determine if the function needs to be invoked.Line 98–110 is the standard Model configuration settings for a few categories and to set the temperature which we have set to 0.Line 112–117, we instantiate the model passing the tools, configuration, etc and start a chat conversation session with the model.Line 121 we send the prompt the model and in Line 122, we get out first response from the model.Line 126–175 is nothing but going through the response received and if there is a function call specified, then we invoke it. We pass the response received from the function back to the model. If there is another function that it asks to be invoked, then we do so. This continues until the model does not ask us to invoke any more functions.Sequential Function calling in actionSample Output #1For the prompt "How much of P1 and P2 do we have in warehouse w10?", we get the following output. You can see that we are being asked by the model to invoke the getInventoryCount method twice. Once for P1 and the other for P2:User provided Prompt: How much of P1 and P2 do we have in warehouse w10?Initial response: role: "model"parts {  function_call {    name: "getInventoryCount"    args {      fields {        key: "location"        value {          string_value: "w10"        }      }      fields {        key: "productid"        value {          string_value: "P1"        }      }    }  }}Need to invoke function: getInventoryCountExecuting function with parameters: P1 w10Response: role: "model"parts {  text: "We have 50 units of P1 in w10. \n\n"}parts {  function_call {    name: "getInventoryCount"    args {      fields {        key: "location"        value {          string_value: "w10"        }      }      fields {        key: "productid"        value {          string_value: "P2"        }      }    }  }}Need to invoke function: getInventoryCountExecuting function with parameters: P2 w10Response: role: "model"parts {  text: "We also have 50 units of P2 in w10."}No more function calls found in responseSample Output #2For the prompt "Where is warehouse w10 located and how many unit of p1 are there??", we get the following output. You can see that we are being asked by the model to invoke the getWarehouseDetails method first with the warehouse location as w10 and then we are asked to invoke the getInventoryCount method with the location as w10 and the productId as p1:User provided Prompt: Where is warehouse w10 located and how many unit of p1 are there?Initial response: role: "model"parts {  function_call {    name: "getWarehouseDetails"    args {      fields {        key: "location"        value {          string_value: "w10"        }      }    }  }}Need to invoke function: getWarehouseDetailsExecuting function with parameters: w10Response: role: "model"parts {  text: "w10 is located at 123 Main Street. \n\n"}parts {  function_call {    name: "getInventoryCount"    args {      fields {        key: "location"        value {          string_value: "w10"        }      }      fields {        key: "productid"        value {          string_value: "p1"        }      }    }  }}Need to invoke function: getInventoryCountExecuting function with parameters: p1 w10Response: role: "model"parts {  text: "There are 50 units of p1 in w10."}No more function calls found in responseSo as you can see over here, the Gemini 1.0 Pro model is doing great. However it asks us to invoke the Functions one after the other.Enter Parallel Function calling.Parallel Function callingYou would have noticed that the model response gives us a sequence of function calls to make one after the other. But if you look at it, it could have just determined that the functions can be invoked in parallel and could have given us a list of functions to invoke in the first original response itself. For e.g. if we had provided the following prompt: Where are warehouse w1 and w2 located?, it should have given us two function calls that we need to make to getWarehouseLocation, one with warehouse location as w1 and the other with warehouse location as w2. In that case, we could have made both the API calls ourselves, collected the response and given it back to the model in one shot to form the final response.From Gemini 1.5 Pro and Gemini 1.5 Flash models, the model can propose several parallel function calls. This means that we will need to modify our code to expect not just one function call or multiple ones that we will then need to make before handing the API results from those function calls back to the model. The documentation does highlight a parallel function call sample.I have provided another Java program in src/main/java/com/gem

== Article 6
* Title: 'Google Cloud Platform — Technology Nuggets : May 1–15, 2024 Edition'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-may-1-15-2024-edition-1e7d78445afb?source=rss-802a4d428d95------2'
* PublicationDate: 'Fri, 17 May 2024 07:55:02 GMT'
* Categories: tech-nuggets, gcp-weekly, gcp-tech-nuggets, google-cloud-platform

Google Cloud Platform — Technology Nuggets : May 1–15, 2024 EditionWelcome to the May 1–15, 2024 edition of Google Cloud Platform Technology Nuggets.Please feel free to give feedback on this issue and share the subscription form with your peers.Google I/O 2024Google I/O 2024 was held on May 14 and there were interesting announcements across the Google developer ecosystem in not just Cloud but other key services too. You can check out the Cloud specific announcements from here.We will also cover I/O announcements specific to the Cloud Area in the next few sections as applicable.InfrastructureGoogle’s custom AI-specific hardware, Tensor Processing Units, or TPUs have helped push the frontiers, especially in the development of key foundational models that have powered the Generative AI space. Google announced the sixth-generation TPU, the most performant and most energy-efficient TPU to date, which achieves a 4.7X increase in peak compute performance per chip compared to TPU v5e. Check out the blog post for more details.Containers and KubernetesConfig Sync is a GitOps service that lets cluster operators and platform administrators deploy configurations from a source of truth. A new auto-upgrades feature in now available in preview, that let’s you automatically upgrade Config Sync versions. This is an opt-in feature for existing configurations and helps you reduce the maintence overhead.Check out the blog post for more details and a demonstration of how fleet-level default settings enables Config Sync auto-upgrades across fleet clusters from a single, centralized location, rather than requiring modifications to the settings of each cluster individually.Continuing on Config Sync, here is an interesting blog post that details how platform administrators can streamline resource management for their teams across the fleet, in conjunction with Config Sync, specifically scenarios of maintaining specific resources requirements across different teams.As per the Kubernetes documentation, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand. Horizontal scaling means that the response to increased load is to deploy more Pods. Achieving the best price-performance with HPA is not an easy task and requires you to have a good understanding of your CPU utilization targets. Should 50% HPA CPU target be the right one or should it be 70% vis-a-vis cost, performance and other requirements. Check out this blog post that intends to spring a surprise.NetworkingWhile you may be familiar with Google Cloud Application Load Balancers, have you heard about Service Extension Callouts? They allow programmability and extensibility at the edge and on load balancing data paths. There are two types of extensions: Plugin (insert custom code inline in the networking data path) and Callout (use Cloud Load Balancing to make gRPC calls to user-managed services during data processing). Check out this blog post that dives into 5 ways that you can use Service Extension Callouts to improve your Cloud Load Balancing environment.Looking to implement Vertex AI workloads for your enterprises? Chances are high that you will have private connections options for your Vertex AI workloads as a key requirement. What are the options available to you in Google Cloud for private connectivity and which ones may apply in your scenario. Check out this blog post.Machine LearningAt Google I/O ’24, there were several Vertex AI updates:Gemini 1.5 Flash model, in public preview, while offering a context window of 1 million tokens, is lighter-weight than 1.5 Pro and designed to efficiently serve with speed and scale for tasks like chat applications.PaliGemma, available in Vertex AI Model Garden, is the first vision-language model in the Gemma family of open models, and is well-suited for tasks like image captioning and visual question-answering.In addition to the above models which are now available, several other models : Imagen 3, Gemma 2 and Gemini 1.5 Pro with 2 million context window will be available soon.It was not just about the new models but also available of Vertex AI integration that are now driving other services. For e.g. Genkit from Firebase, an open-source Typescript/JavaScript framework designed to simplify the development, deployment, and monitoring of production-ready AI agents is facilitated through the Vertex AI plugin. As a result, Firebase developers can now take advantage of Google models like Gemini and Imagen 2, as well as text embeddings. Also announced were LlamaIndex on Vertex AI and Grounding with Google Search has gone GA. Check out the blog post for more details. You can also check out another similar blog post that provides more information on updates to Gemini, Imagen, Gemma and MLOps on Vertex AI.Gartner® has named Google as a Leader in the Magic Quadrant™ for Cloud AI Developer Services (CAIDS). Check out the blog post that highlights key Vertex AI services that help you build, host models and write applications.Looking to implement RAG using the best in breed Open Source libraries and models, to retain the flexibility and yet hosting it on performant cloud services, check out this blog post that demonstrates how to deploy a complete RAG application on Google Kubernetes Engine (GKE), and Cloud SQL for PostgreSQL and pgvector, using Ray, LangChain, and Hugging Face.If you building out Generative AI solutions using media content on Google Cloud, the need is to handle different kinds of unstructured data, ingest them and then possibly look at analytics and integration with Google Cloud AI services. Check out a step by step solution that:Allows you to interact with structured data through BigQuery.Using BigQuery object tables to analyze unstructured data in Cloud Storage, you can perform analyses using generative AI models via remote functions, cloud APIs via Vertex AI, or perform inference by using BigQuery MLFinally, join the results of these operations with the rest of your structured data in BigQuery.Finally, Translation API customers can now choose between the traditional machine translation model (a.k.a. NMT) or the new Translation LLM. Translation LLM is well-suited to longer context, so it should be considered for use in translating paragraphs and articles. You can test out the Specialized Translation LLM, Gemini, or Google Traditional translation models, all in AI Studio.Check out the blog post for more details on the updates to Translation models and which ones to use depending on your requirements.Storage and Data TransferCloud Storage has introduced the “soft delete” feature, which protects against accidental and malicious data deletion by providing you with a way to retain and restore recently deleted data at enterprise scale. Check out a blog post on how the feature is implemented, how you can configure it across for your organizations and monitor, take actions based on it.Management ToolsThe Active Assist service has got several new features:Improved Recommendation hub, where you can view all of your recommendations in one place across multiple categories: cost, security, performance, reliability, manageability, sustainability and more.4 new recommendations: Breaking changes, IAM for BigQuery, Advisory Notifications, Recent change.Check out the blog post for more details.Running to control costs due to a large number of metrics ingestion into Cloud Monitoring. If you are using Managed Service for Prometheus, there are ways to manage this by controlling the number of samples ingested, since the charges are based on that. The two techniques discussed in the blog post to manage the samples ingested are filtering input or adjusting the length of the sampling period. Check it out.DatabasesThe Firebase suite of services had a series of announcements at Google I/O. This blog post summarizes the key announcements and provides details on each of them with possible preview signups for some of the features announced. The integration of some of these new Firebase services with Google Cloud services is interesting to note. Key among them (as reproduced from the blog) are:Firebase Data Connect: You can now connect your app directly to a Postgres database hosted on Cloud SQL with Firebase. With Data Connect, you define your data model, the queries you want to allow on that model, and the mutations you want to allow–all in a concise language based on GraphQL.Firebase Genkit is an AI integration framework to help make building sophisticated AI features feel easy and familiar. With Genkit, now in beta, you can build faster with AI libraries and plugins, with access to powerful AI models, vector stores, evaluators, tools, and more to compose flows that use patterns like RAG. Genkit also includes developer tooling that enables you to run, evaluate, and debug your AI features locally.Firebase App Hosting, now in preview, is a framework-aware, serverless web hosting solution that manages everything from the CDN to server-side rendering. It has built in support for modern frameworks such as Angular and NextJS. It’s also built on enterprise-grade Google Cloud services, like Cloud Run, making it scalable, flexible, and secure. App Hosting automatically sets up the infrastructure needed to build and run your full-stack web app based on what it detects in your source code.Looking to make a decision between AlloyDB vs. self-managed PostgreSQL? Consider this detailed guide on a price-performance comparison.Data AnalyticsGoogle Cloud has been named a leader in The Forrester Wave™: Data Lakehouses Q2 2024 report. Forrester rated Google Cloud a score of 5 out of 5 across 15 different criteria. Check out the blog post that highlights BigQuery as a unified AI platform and key announcements from Google Next 2024 in this space.How good is support for multi-modal LLMS in BigQuery? The use case that we could potentially look at is to generate database schema? This blog post covers the following fictional scenario: Take examples of data definition languages (DDLs) and entity relationship (ER) diagrams to create a database schema. The steps are simple:Create a ER diagramCreate a prompt with the ER diagramSend the multi-model prompt to the Gemini 1.0 Pro Vision modelData mesh is an architectural framework that promotes the idea of treating data as a product and decentralizes data ownership and infrastructure. How do we bring that to reality? This blog post looks at Dataplex, a data fabric capability within the BigQuery suite, which can be used to realize the benefits of this decentralized data architecture.Managed disaster recovery for BigQuery is now available in Preview, in the BigQuery Enterprise Plus edition, offers automated failover of compute and storage and a new cross-regional SLA tailored for business-critical workloads. The goal is to ensure business continuity in the unlikely event of a total regional infrastructure outage. It also provides failover configurations for capacity reservations, so you can manage query and storage failover behavior. Check out the blog post for more details.Developers and PractitionersWhat is common between the following scenarios: placing a customer order in a shipping queue, withdrawing funds from a bank account, or processing a payment? Ideally, we would like that requests made in these scenarios are processed exactly once and not more. Check out this solution involving Google Cloud Workflows, that will invoke Workflows exactly once when using HTTP callbacks, Pub/Sub messages, or Cloud Tasks.Continuing with Workflows, you should look at exploring more of them as you work with several large language models and if you have requirements to co-ordinate calls across them, execute some calls in parallel, handle retries and more. Check out this detailed blog post that describes one of the most popular use cases in Generative AIs: summarizing long documents. This would require breaking the document into chunks, getting a summary of the individual chunks and then combining it again. Check out this elegant solution with Cloud Workflows.Learn Google CloudChaos engineering, as the definition states is the discipline of experimenting on a system in order to build confidence in the system’s capability to withstand turbulent conditions in production. Its all about deliberately injecting faults into a system. Check out how Spanner, which is based on fault-tolerant design still significantly benefits from Chaos Testing. Understand the kind of faults that the team introduces: Server crash, file faults, RPC faults, Memory/quota faults, Cloud faults and regional outages.How does a data cache work for a database? What are the internal mechanisms, and the types of workloads that will benefit the most from it. In this post, which is reproduced here, “Cloud SQL for PostgreSQL Enterprise Plus edition includes an innovative data cache feature, which significantly improves read performance. The data cache is a read cache that uses a server-side SSD to cache data. Because it is co-located with compute in the server, data accesses have low latencies and high throughput.”Check out the post to learn more about how it is implemented?Stay in TouchHave questions, comments, or other feedback on this newsletter? Please send Feedback.If any of your peers are interested in receiving this newsletter, send them the Subscribe link.Google Cloud Platform — Technology Nuggets : May 1–15, 2024 Edition was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 7
* Title: 'Codelab : Writing Synthetic Monitoring Tests for your services using Gemini'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/codelab-writing-synthetic-monitoring-tests-for-your-services-using-gemini-cae542b12547?source=rss-802a4d428d95------2'
* PublicationDate: 'Mon, 06 May 2024 09:45:18 GMT'
* Categories: google-cloud-platform, codelab, cloud-monitoring, synthetic-monitoring, gcp-security-operations

Codelab : Writing Synthetic Monitoring Tests for your services using GeminiRecently I wrote a blog post that covered the newly released “help me code” feature in Google Cloud Monitoring, that used Gemini to help generate Synthetic Monitoring tests for your application.Help me code feature in writing Synthetic Monitors in Google Cloud MonitoringYou can read the blog post here:Using Gemini to help write Synthetic Monitoring tests in Google CloudWhile the blog post covers the details on Synthetic Monitoring and a step by step guide to authoring them, you might prefer to do a focused codelab on the same.Check out the codelab here:Writing Synthetic Monitoring Tests for your services using Gemini | Google CodelabsThe codelab helps you do the following:You will deploy an API to Google Cloud Run, which will act as a basis for our service that we shall test.You will then write a Synthetic Monitor, which is a feature of Cloud Monitoring.You will use the Help Me Write feature to write the Synthetic Monitor.Hope you enjoy the codelab. Do give me feedback, if possible.Codelab : Writing Synthetic Monitoring Tests for your services using Gemini was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Codelab: Using Gemini Code Assist to explore and enhance Generative AI Document Summarization Jump…'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/codelab-using-gemini-code-assist-to-explore-and-enhance-generative-ai-document-summarization-jump-28c410042544?source=rss-802a4d428d95------2'
* PublicationDate: 'Thu, 02 May 2024 12:23:36 GMT'
* Categories: gemini-code-assist, google-cloud-platform, generative-ai, jump-start-solutions, gcp-app-dev

Codelab: Using Gemini Code Assist to explore and enhance Generative AI Document Summarization Jump Start SolutionGoogle Cloud Jump Start solutions are pre-built sample applications and infrastructure best-practices that you can deploy within a few clicks in your own Google Cloud account.If you would like an introduction to Jump Start Solutions, check out my blog post titled Super charge your Google Cloud Learning with Jump Start Solutions.This post is focused on a codelab that you can do for the following:Understand one of the Jump Start Solutions — Generative AI Document SummarizationUse Gemini Code Assist to help you understand the code and develop a new feature in the Jump Start Solution.Generative AI Documentation Summarization — The solutionAs the Jump Start solution documentation states, “This solution leverages Vertex AI Generative AI Large Language Models (LLM) to process and summarize documents on demand.”This solution deploys a pipeline that is triggered when you add a new PDF document to your Cloud Storage bucket. The pipeline extracts text from your document (using Vision API OCR feature), creates a summary (via a Vertex AI LLM model) from the extracted text, and stores the summary in a database (BigQuery) for you to view and search.You can read more on the official documentation here but first the architecture diagram, which will make things more clear. Focus on the blocks outlined in red color, since the codelab is focused on that.You can see from the flow that trigger and summarization of the document is a manual process. You have to trigger it by uploading a PDF into the specific Cloud Storage bucket and then the Cloud Function gets triggered, which will extract the Text from the document, summarize and write the results to both Cloud Storage and BigQuery.The codelab that you can execute to understand this existing solution, do a sample run and then develop a new functionality is available here:Using Gemini Code Assist to explore and enhance AI Summarization Jump Start Solution | Google CodelabsAs part of the codelab, you will setup Gemini Code Assist, an AI assistant available in Google Cloud, that will allow you to inspect the source code for the solution, take assistance to understand some parts of the code via the “Explain this” prompt and more. (Step 6 in the codelab)The codelab also contains steps to do a manual execution of the flow via a sample PDF (Research paper). (Step 7 in the codelab)Use Gemini Code Assist to develop new functionalityAs mentioned, the Jump Start Solution presents a manual process i.e. you have to upload the sample PDF to a Google Cloud Storage bucket. Our goal is to take this further and develop new functionality i.e. build out a web front-end that allows you to upload the file that needs to be summarized.To build out this functionality, you will need to do the following at a high level:Develop the Web assets (HTML, Javascript, CSS) for the upload form.Build out a server side functionality that will accept the uploaded file, extract out the contents and the use Google Cloud Storage client APIs to write the object blob to the Google Cloud Storage bucket.How about using Gemini Code Assist to help you out with source code that can speed up the development of the above functionality? That is exactly what is demonstrated in the codelab. (Step 8 in the codelab)BonusThe codelab also utilizes Code Assist to demonstrate how you can work with it to get information on:Deploying the application you developed to Cloud RunEnhance the Web application by throwing in some CSS StylingHope you enjoy the codelab. Use this codelab to understand the process of taking any existing solution and using Code Assist to understand the source code and incrementally enhance the application.Codelab: Using Gemini Code Assist to explore and enhance Generative AI Document Summarization Jump… was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'Google Cloud Platform — Technology Nuggets : April 16–30, 2024 Edition'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/google-cloud-platform-technology-nuggets-april-16-30-2024-edition-d5a39332288f?source=rss-802a4d428d95------2'
* PublicationDate: 'Thu, 02 May 2024 07:54:27 GMT'
* Categories: google-cloud-platform, gcp-tech-nuggets, tech-nuggets, gcp-weekly

Google Cloud Platform — Technology Nuggets : April 16–30, 2024 EditionWelcome to the April 16–30, 2024 edition of Google Cloud Platform Technology Nuggets.Please feel free to give feedback on this issue and share the subscription form with your peers.Google Cloud Next 2024A final reminder that sessions from Google Cloud NEXT 2024 are all available on demand at the Cloud NEXT ’24 site. The only requirement is that you will need to register to view the sessions. Check out the entire session library from the conference.InfrastructureConnectivity is key to long term economic and productivity gains. Check out this post that captures that details on the $1 billion investment in digital connectivity to Japan, that includes expansion of the Pacific Connect Initiative and two new subsea cables, Proa and Taihei.Regional Persistent Disk is a storage option that lets you implement high availability (HA) services in Compute Engine. Regional Persistent Disk synchronously replicate data between two zones in the same region and ensure HA for disk data for up to one zonal failure. Its is best designed for workloads that require a lower RTO and RPO. Check out this blog post that helps you to learn more about Regional Persistent Disk, including its features, setting it up and more.Containers and KubernetesIf you are in the Telecom domain and looking to adopt Kubernetes, one of the key challenges/requirements has been addressing multi-networking requirements within the domain, especially around strict network isolation and maximizing performance to achieve the high throughput and low latency requirements. This blog post is an essential read that highlights critical telecom use cases enabled by multi-networking.Each Pod in Kubernetes has its own network address. As the blog post states, “With every Pod in every cluster being allocated IPs on the same VPC, and allowing space in those ranges for expansion, IPs get used very fast.” How do you solve this problem with IPv4 ranges today? Check out the solution.Hyperdisk is Google Cloud’s next-generation block storage service that allows you to modify your capacity, throughput, and IOPS-related performance and tailor it to your workloads, without having to re-deploy your entire stack. Hyperdisk Balanced has just been announced, which sort of lies in the middle with Hyperdisk Throughput and Hyperdisk Extreme. Hyperdisk Balance is “General-purpose volume type that is the best fit for most workloads, with up to 2.4GBps of throughput and 160k IOPS. Ideal for line-of-business applications, web applications, databases, or boot disks.”Check out the blog post on read more on this and getting started with Hyperdisk on GKE.NetworkingDirect Peering enables you to establish a direct peering connection between your business network and Google’s edge network and exchange high-throughput cloud traffic. Direct Peering requires complex routing designs and as a result, not all customers are interesting in managing that. At the same time, the need to achieve low-latency and highly-available connectivity is a given. Enter Verified Peering Provider (VPP) program, that simplifies connectivity to Google’s network.As the blog post states, “The Verified Peering Provider program identifies ISPs that offer enterprise-grade internet services and have met multiple technical requirements, including diverse peering connectivity to Google. Information about each Verified Peering Provider, such as their Google network connectivity locations, redundancy information and markets where they offer internet services, are listed on Google’s website.”Read the blog post for more details and check out a list of Verified Peering Providers here.Direct VPC egress is now in general availability (GA) for Cloud Run. This allows your Cloud Run resources to send traffic directly to a VPC network without proxying it through Serverless VPC Access connectors. As the blog post states “It delivers approximately twice the throughput compared to both VPC connectors and the default Cloud Run internet egress path, offering up to 1 GB per second per instance.”Identity and SecurityGemini 1.5 Pro, with its processing capability upto 1 million tokens is an excellent tool to help with Automated Malware Analysis. In a detailed blog post, that is an essential read for Security professionals, the authors provided code and a simple prompt to Gemini 1.5 Pro, to determine if the file was malicious, and to provide a list of activities and indicators of compromise. The result: Gemini 1.5 Pro was accurate to check for malware, generate summary reports and determination of code. The post goes into detail about Malware Analysis, how Gemini 1.5 Pro performed and most importantly, the fact that it is able to handle the large context window played a key role in its accuracy v/s other tools that required chunking and assembling.Regulatory, Compliance and Sovereignity needs are important for customers as they run various types of workloads in the Cloud. To help simplify and manage these requirements, several announcements were made at Cloud NEXT 2024, which included new Regional Control and Google Cloud Partners that are providing Sovereign controls to address regional requirements. Check out the blog post for more details.Machine LearningMeta Llama 3 is available now on Vertex AI Model Garden. Acessing Llama 3 through Vertex AI, allows access to tuning options made available through Colab Enterprise. These include preconfigured notebooks for supervised tuning (LoRA), reinforcement learning through human feedback (RLHF), and distillation. Check out the blog post for more details.For ML Practitioners using PyTorch/XLA, a new release 2.3 is out, which brings productivity, performance and usability improvements. Official PyTorch/XLA repository is available at https://github.com/pytorch/xla. Check out the post for more details.Storage and Data TransferContinuing with Hyperdisk Storage, a Hyperdisk Storage Pool is a pre-purchased collection of capacity, throughput, and IOPS which you can then provision to your applications as needed. A Hyperdisk Storage Pool helps you lower your block storage TCO in two ways: It leverages a thin-provisioned capacity pooling to drive up to 80% capacity utilization. What this means is that you can provision a single Storage Pool which will only be consumed by the data you write. Second, it leverages data reduction technologies to further lower the capacity you need to purchase up front. Check out this blog post that covers how they work and using them in your environment.Storage Pools are now available in Google Cloud console.DevOps and SREThe 2024 DORA Survey is open. DORA has been a leading effort for over a decade to understand how organizations can continue to deliver key outcomes and what capabilities are essential to meet them. In this years survet, you will find key new themes that our industry has been discussing in detail: Artificial Intelligence, Platform Engineering and Developer Experience. Check out this post to know more about filling out the survey, access to other DORA material and more.DatabasesWhen it comes to Cloud Migration, the words “migration” and “modernization” often come up. Do you just move your applications and data to the cloud, as is or should you look at modernizing your applications and databases by using some of the cloud providers services, thereby possibly looking at a new architecture too? This post takes a high level look at migration v/s modernization, the key factors to consider in the process along with best practices.Data AnalyticsBigQuery has support for analyzing your data using a range of LLMs hosted in Vertex AI (Gemini 1.0, text-bison, etc). But what if the models do not meet your expectations for some scenarios. What if model fine-tuning is needed for your specific case. Support for supervised fine-tuning and evaluation is now available in BigQuery. At a high level, “Supervised fine-tuning via BigQuery uses a dataset which has examples of input text (the prompt) and the expected ideal output text (the label), and fine-tunes the model to mimic the behavior or task implied from these examples.” Check out the blog post for more details via a walkthrough.In more BigQuery news, a new set of functions that enable model monitoring directly within BigQuery have been introduced. You can use these functions to describe data through profiling training or inference data, monitor skew between training and serving and monitor drift. Check out the blog post for details.It is no surprise that BigQuery is fast becoming a single place for integrating ML models, APIs and tools. Consider a scenario, where you need to analyze customer reviews that are available via voice recordings. This can all be done inside of BigQuery by utilizing the speech-to-text functionality and then using BigQuery ML to leverage Gemini 1.0 Pro to gain additional isnights and data formatting such as entity extraction and sentiment analysis. Check out the blog post that highlights how to do that.Developers and PractitionersDeveloper Productivity is a key topic of discussion across organizations today. With the proliferation of AI Assistants, organizations are trying to answer queries like “Do these tools help us code faster?”, “Did these tools help us save money?” and more. At the same time, it is important that we also look at it from the developers and their workflow. Take some time and read this paper that explore what it means to measure developer productivity and how generative AI will make a difference.Learn Google CloudIf I may be allowed to say so, the best time to learn more about Google Cloud is today. There are new Generative AI courses announced on YouTube and Google Cloud Skills Boost. The courses range across beginner to advanced types.Two new Google Cloud certificates have been announced too, Data Analytics and Cybersecurity.A new interview prep tool has been launched too. A great way to not just prep for an interview but even check out your own understanding of key topics.In addition to the above announcements, the blog post has more vis-a-vis no-cost access for higher education institutions and more. Check it out.Stay in TouchHave questions, comments, or other feedback on this newsletter? Please send Feedback.If any of your peers are interested in receiving this newsletter, send them the Subscribe link.Google Cloud Platform — Technology Nuggets : April 16–30, 2024 Edition was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Using Gemini to help write Synthetic Monitoring tests in Google Cloud'
* Author: 'Romin Irani'
* URL: 'https://medium.com/google-cloud/using-gemini-to-help-write-synthetic-monitoring-tests-in-google-cloud-359561a23230?source=rss-802a4d428d95------2'
* PublicationDate: 'Wed, 24 Apr 2024 12:33:56 GMT'
* Categories: google-cloud-platform, gemini, gcp-security-operations, monitoring

Last year, Google Cloud Monitoring had introduced Synthetic Monitoring and I covered in detail how you could write a Synthetic Monitoring Test Suite that could regularly perform tests on your deployed services.The original blog post is here:Google Cloud Synthetic Monitoring TutorialA lot has happened in the last 8–10 months since the article and with the integration of Gemini in Google Cloud and especially within Synthetic Monitoring, it was time to revisit the article and see if I could ask Gemini to generate the Test Suite for me completely. The results have been fascinating and I would like to demonstrate that.First up, for those of you, who are new to Synthetic Monitoring, I repeat sections of the original article over here:What is Synthetic Monitoring?As the documentation states and allow me to break it down:Test the availability, consistency, and performance of your services, applications, web pages, and APIs.It works by periodically issue simulated requests and then records whether those requests were successful, and they record additional data about the request such as the latency.You can be notified when a test fails by creating an alerting policy to monitor the test results.The original preview release of this feature required that we write the test cases in supported JavaScript Test Frameworks (Mocha) or with no frameworks too. These test cases would be wrapped inside of Google Cloud Functions (2nd generation powered by Cloud Run) and these functions would be invoked for you on a regular basis, depending on the recurring interval that you configure (every minute, 2 minutes, etc).Sample Inventory API (from the original article)To test out Synthetic Monitoring, we need something to test against. While we could have used some existing API endpoints and validated the results, its nice to see it all integrated within Google Cloud itself.So the first thing that I am going to do is deploy a Sample Inventory API to my favourite service, Cloud Run.GitHub - rominirani/cloud-code-sample-repository: Repository for Cloud Code sample applicationsThe repository has a Python Flask API that I am going to Cloud Run. You can follow the instructions in the README, where it gives you the choice to use either VS Code with Cloud Code extension or if you prefer, directly via a gcloud command.To keep things simple, just go to the python-flask-api folder and execute the following gcloud command to deploy the service to Google Cloud Run:gcloud run deploy --source .This will ask you multiple questions. Go ahead with them and within a couple of minutes, you should have the Inventory API deployed in Googele Cloud Run and you have a URL via which you can access the same. The Service URL will be of the format:https://&lt;SERVICE_NAME&gt;.xxxxx.a.run.appLet us refer to the above url as SERVICE_URL for the rest of the article.Test out the Inventory APIBefore we start writing the Tests and deploying the Synthetic Monitors, lets understand the basic API.The Inventory API has 3 methods and it has pre-populated data of 3 Inventory Items (I-1, I-2 and I-3) along with their associated on-hand quantity. We can try out:SERVICE_URL/inventory : This will give a list of all inventory items.[ { “I-1”: 10, “I-2”: 20, “I-3”: 30 }]SERVICE_URL/inventory/I-1: This will give a specific Inventory record with its quantity.{ “productid”: “I-1”, “qty”: 10}SERVICE_URL/inventory/I-100: Since this record is not present, it is an incorrect product id and hence the qty returned is -1.{ “productid”: I-200, “qty”: -1}Asking Gemini to help us write the TestsLogin to Google Cloud Console and visit the Cloud Monitoring → Synthetic Monitoring page as shown below:This will bring up the Synthetic monitoring page as shown below:Click on the CREATE SYNTHETIC MONITOR button. This will bring up a form as shown below. I have given a name s1 to my monitor and then I click on the HELP ME CODE button as shown below.This brings up a nice popup, where you are asked to describe the test scripts that you want in natural language (prompt). It also provides you a helpful Example text that you can modify/enhance and use.I provide the following prompt:Help me write a Node.js based synthetic test script that performs the following tests:1. It does a GET operation on https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory. This should return a 200 HTTP status code.2. It does a GET operations on https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-1. This should return a 200 HTTP status code. It should contain a JSON reponse as an object and the object should have an attribute "productid", whose value should be "I-1".3. It does a GET operations on https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-123. This should return a 200 HTTP status code. It should contain a JSON reponse as an object and the object should have an attribute "qty", whose value should be -1.I provide the above prompt and click on GENERATE.This provides me a good snippet of test code as shown below:const assert = require('assert');const fetch = require('node-fetch');async function run(logger, executionId) {  try {    logger.log('Starting test');    // Test 1: GET https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory    const response1 = await fetch('https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory');    assert.strictEqual(response1.status, 200);    // Test 2: GET https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-1    const response2 = await fetch('https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-1');    assert.strictEqual(response2.status, 200);    const data2 = await response2.json();    assert.strictEqual(data2.productid, 'I-1');    // Test 3: GET https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-123    const response3 = await fetch('https://python-flask-api-ido3ocn3pq-uc.a.run.app/inventory/I-123');    assert.strictEqual(response3.status, 200);    const data3 = await response3.json();    assert.strictEqual(data3.qty, -1);    logger.log('Test passed');  } catch (err) {    logger.error(err);    throw err;  }}exports.run = run;As you can see, its nicely generated the 3 test cases and I am happy with the code generated and can put it to test. What is very interesting is the attention given to reducing the friction further by allowing you to insert and package this code immediately into a Cloud Function. That is very helpful indeed.Go ahead and click on the INSERT INTO CLOUD FUNCTION button. This brings up the Create Google Cloud Function dialog and the required fields and data is automatically populated for you. Notice the Cloud Function name, its trigger and the necessary code files (JS and package.json) that is generated for you:Simply click on APPLY FUNCTION button. This will bring you back to the Create Synthetic check page, where you can further setup the Alert Name, Notification Channels and so on, should the check fail and you need to get notified. Click on CREATE button there to move forward.This kicks off the process to deploy the Synthetic monitor and you are presented with the status as shown below:Give it a few minutes, since this deploys the Google Cloud Function (2nd Generation) and once its applied, then the Monitoring tests will be executed automatically for you based on the interval that you specified.I observed that the tests ran fine for me and here are a few executions of the same.The details for each of the runs can be seen on clicking the Monitor, the details of which are shown below:ConclusionIt is interesting to note that a few months back, I had to wrangle with Mocha Test code to get the test suite working well but now Gemini, as the Assistant gave me code that was able to run. It gives me a good basis on which I can further enhance the suite. Definitely, a great way on how to introduce Generative AI suggestions (in this case, Code Generation) into existing applications.Using Gemini to help write Synthetic Monitoring tests in Google Cloud was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.
