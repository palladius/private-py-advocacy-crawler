
== Article 1
* Title: 'Grounding LLMs with your own data using Vertex AI Search'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/grounding-llms-with-your-own-data-using-vertex-ai-search-eb7a008b025f?source=rss-6444b44155fa------2'
* PublicationDate: 'Mon, 01 Jul 2024 12:48:53 GMT'
* Categories: google-cloud-platform, generative-ai-tools, software-development

IntroductionIn my previous Vertex AI’s Grounding with Google Search: how to use it and why post, I explained why you need grounding with large language models (LLMs) and how Vertex AI’s grounding with Google Search can help to ground LLMs with public up-to-date data.That’s great but you sometimes need to ground LLMs with your own private data. How can you do that? There are many ways but Vertex AI Search is the easiest way and that’s what I want to talk about today with a simple use case.Cymbal Starlight 2024For the use case, imagine you own the 2024 model of a fictitious vehicle called Cymbal Starlight. It has a user’s manual in PDF format ( cymbal-starlight-2024.pdf) and you want to ask LLM questions about this vehicle from the manual.Since LLMs are not trained with this user manual, they’ll likely not be able to answer any questions about the vehicle but let’s see.Without GroundingFirst, let’s ask a question to the LLM about the vehicle without any grounding. You can do this in the Vertex AI section of Google Cloud Console against any model.Prompt: What is the cargo capacity of Cymbal Starlight?Response: Unfortunately, I do not have access to real-time information,including specific details about ships like "Cymbal Starlight" ...As you can see, the model thinks we’re talking about a ship and cannot really answer the question.There’s also a main.py sample code that you can run:python main.py --project_id your-project-idYou get a similar response:Prompt: What is the cargo capacity of Cymbal Starlight?Response text: I do not have access to real-time information,including specific details about ships like the "Cymbal Starlight."This is not surprising. Let’s now ground the LLM with this private PDF.Create datastore with PDFTo use a PDF for grounding, you need to upload the PDF to a Cloud Storage bucket and set up a datastore to import from that bucket.Create a Cloud Storage bucket with uniform bucket-level access and upload the PDF file:gsutil mb -b on gs://cymbal-starlight-bucket gsutil cp cymbal-starlight-2024.pdf gs://cymbal-starlight-bucketGo to the Agent Builder section of the Google Cloud Console. Click on Data Stores on the left and Create Data Store button. Select Cloud Storage.Point to the PDF file in the bucket and continue:Give your datastore a name and click Create:You need to wait until the import is complete which can take up to 10–15 minutes:Create a search appBefore you can use grounding, we need to create a search app and point to the datastore you just created.Go to Agent Builder section of the Google Cloud Console and click on Create App button:Select Search app type.For Content, select Generic.Make sure Enterprise edition features is enabledGive your app a name and enter company info.In the next page, choose the data store you created earlier and click Create:Setup grounding with Vertex AI SearchCaution: Make sure the datastore import is complete before you continue. If you get an error message: “Cannot use enterprise edition features” or something similar, you might need to wait a little before trying again.Let’s set up grounding with Vertex AI Search now.Go back to Vertex AI section of Google Cloud Console and in the Advanced section, select Enable grounding:Customize grounding and point to the datastore you just created:This is the format of the datastore string:projects/{}/locations/{}/collections/{}/dataStores/{}In my case, it’s as follows:projects/genai-atamel/locations/global/collections/default_collection/dataStores/cymbal-datastore_1718710076267With groundingFinally, we’re ready to ask questions about the vehicle with grounding enabled.Let’s start with the previous question:Prompt: What is the cargo capacity of Cymbal Starlight?Response: The Cymbal Starlight 2024 has a cargo capacity of 13.5 cubic feet.Let’s ask another question:Prompt: What's the emergency roadside assistance phone number?Response: 1-800-555-1212Also run main.py Python sample with grounding:python main.py --project_id your-project-id \  --data_store_path projects/your-project-id/locations/global/collections/default_collection/dataStores/your-datastore-idYou get a similar response:Prompt: What is the cargo capacity of Cymbal Starlight? Response text: The Cymbal Starlight 2024 has a cargo capacity of 13.5 cubic feet.It works!ConclusionIn this blog post, I explored Vertex AI’s grounding with Vertex AI Search. It’s the easiest way of grounding LLMs with your own private data. If you want to learn more, here are some more resources:Get started with generic search docsGrounding decision flowchartGetting Started with Grounding with Gemini in Vertex AI notebookGrounding for Gemini with Vertex AI Search and DIY RAG talkFix My Car sample appOriginally published at https://atamel.dev.Grounding LLMs with your own data using Vertex AI Search was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 2
* Title: 'Give your LLM a quick lie detector test'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/give-your-llm-a-quick-lie-detector-test-97922d247c81?source=rss-6444b44155fa------2'
* PublicationDate: 'Thu, 06 Jun 2024 14:03:20 GMT'
* Categories: generative-ai-tools, google-cloud-platform, software-development

IntroductionIt’s no secret that LLMs sometimes lie and they do so in a very confident kind of way. This might be OK for some applications but it can be a real problem if your application requires high levels of accuracy.I remember when the first LLMs emerged back in early 2023. I tried some of the early models and it felt like they were hallucinating half of the time. More recently, it started feeling like LLMs are getting better at giving more factual answers. But it’s just a feeling and you can’t base application decisions (or any decision?) on feelings, can you?How can I tell if my LLM is lying to me and how much?The rigorous answer probably requires using something like Vertex AI’s Generative AI evaluation service which lets you evaluate model performance across specific use cases. It’s something I’ll look into and report back on in a future article.For now, I’m interested in a quick lie detector test for my LLM and for that, you need:A high quality, labeled dataset that you can trustA good test question that you can ask the LLM about the datasetA way to measure the accuracy of the LLM responseLet’s talk about these in more detail.DatasetFor a dataset, you have a lot of options and you probably want to use something that’s close to your application domain to make it a realistic test. However, in this quick &amp; dirty lie detector test, we can use any dataset that you trust to be correct.I was introduced to the Open Trivia database by a coworker last year. It is a free to use, user-contributed trivia question database with 4100+ verified questions and answers.It also has an API where you can request random questions in different categories and answers with a URL like this: https://opentdb.com/api.php?amount=10And you get a JSON response like this:{  response_code: 0,  results: [    {      type: "multiple",      difficulty: "medium",      category: "Sports",      question: "How many games did Arsenal FC go unbeaten during the 2003-2004        season of the English Premier League",      correct_answer: "38",      incorrect_answers: [        "51",        "49",        "22"      ]    },    ...}TestWe have the dataset but what should the test question be for the LLM?Again, there are different approaches you can take here but the simplest approach is to ask the LLM: Given this question and answers, find the correct answer. This is a good test because it’s a very specific task, the LLM has all the info it needs, and the lie is easy to detect.We need to be careful about not biasing LLM with the correct answer. We also don’t want to burden the LLM with more info then it needs. This means, the previous JSON file can be transformed into this simplified JSON with all correct and incorrect answers shuffled in the same list:[  {    question: "How many games did Arsenal FC go unbeaten during the 2003-2004        season of the English Premier League",    answers: [      "51",      "49",      "38",      "22"    ]  },  ...]Now, we can ask the LLM: Given these questions and answers, find the correct answer for each question.MeasureMeasuring accuracy is basically comparing the correct answers in the original dataset and the correct answers given by the LLM. If they all match, you get 100% accuracy but we know this won’t be the case. The question is: How close will we get to 100% accuracy with different models?CodeThe full code is in my opentrivia-llm-testing repo. You can check out for details but to give you an overview:get_questions retrieves questions from Open Trivia and filters out the unnecessary fields.transform_questions combines correct and incorrect answers into a single field.ask_llm asks the LLM to find the correct answer and return a JSON back with the same format as Open Trivia.compare_question_lists compares the lists and keeps track of how many correct answers the LLM returned.run_tests runs multiple tests in an iteration, keeping track of accuracy and also time taken.A few notes about the code:When prompting the LLM, I set the temperature to 0 for more consistency. Temperature can be thought of as a measure of how random or creative you want the response to be. Using a value of zero means we want the most repeatable results possible.I turned off safety settings to avoid censored output.Despite specifying the desired JSON output in our prompt, I occasionally still got malformed JSON so I had to add some post-processing to catch and correct that case.I observed quite a bit of variance in the results when running these tests, so I run multiple iterations and averaged the results.I tried adding the Google Search grounding option to see if it helps improve accuracy.You can run the tests as follows with 4 test iterations and 25 questions in each iteration and no Google Search grounding:python main.py your-project-id model-idIf you wanted to customize the test runs, you could do so as follows:python main.py your-project-id model-id \   --num_iterations= 4 --no_questions= 25 --google_search_groundingAs a side note, I used Gemini Code Assist to help me write the Python code and it was amazing! It really sped up my development process and saved me quite a bit of time as a non-expert Python developer.ResultsDisclaimer: My tests are hardly scientific. For each model, I only ran 4 iterations with 25 questions in each iteration. 100 questions is not enough. I need to run against more questions, more iterations, and use more sophisticated evaluation methods beyond accuracy and time. Also, I don’t know if correctness against Open Trivia generalizes to other use cases.Let’s look at the results with the disclaimer in mind. You can see the detailed test runs in the runs folder with a results table.Surprisingly, Google Search grounding did not help in my tests. It made the execution time longer with slightly less correctness percentage. My guess is that the dataset is already public and possibly models are already trained on it and that’s why grounding with Google Search does not help.Here are the results without grounding and with average correctness and execution seconds:Average results without groundingMost models achieved more than 90% correctness and that was good to see. There was a big variation in execution times, ranging from 14 seconds to almost 45 seconds. If speed is not important, I’d probably go with gemini-1.5-pro with its best accuracy. Otherwise, I'd probably choose gemini-1.0-pro as it seems to be quick but also has a good accuracy at the same time. Another consideration is probably cost but it's not something I looked into.ConclusionIn this blog post, I tried to see if I can figure out how much LLMs lie to me and came up with a simple test. As I said, my tests are hardly scientific and not sure if they generalize beyond the Open Trivia dataset. The main point is that if accuracy is important for your use case, you can also come up with a test. It was nice to see accuracy above 90% with most models in my case. It’s not 100% but a good start and with other techniques like RAG and grounding, it can probably be improved depending on the dataset and questions asked.In a future post, I want to look into Vertex AI’s Generative AI evaluation service and see if I can do a more rigorous, scientific evaluation.Originally published at https://atamel.dev.Give your LLM a quick lie detector test was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'AsyncAPI gets a new version 3.0 and new operations'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/asyncapi-gets-a-new-version-3-0-and-new-operations-013dd1d6265b?source=rss-6444b44155fa------2'
* PublicationDate: 'Mon, 13 May 2024 08:32:53 GMT'
* Categories: software-development, software-engineering, open-source, event-driven-architecture

Almost one year ago, I talked about AsyncAPI 2.6 and how confusing its publish and subscribe operations can be in my Understanding AsyncAPI's publish &amp; subscribe semantics with an example post.Since then, a new 3.0 version of AsyncAPI has been released with breaking changes and a totally new send and receive operations.In this blog post, I want to revisit the example from last year and show how to rewrite it for AsyncAPI 3.0 with the new send and receive operations.AsyncAPI 3.0AsyncAPI 3.0 was released in December 2023. Since it's a major version, it has some breaking changes. These two pages does a good job explaining the changes and the rationale behind them:AsyncAPI 3.0.0 Release Notes.Migrating to v3.I won’t go through all the changes. For me, the biggest change is the separation of operations and channels, and changing publish and subscribe operations to send and receive.Recap: Publish and subscribe operations in AsyncAPI 2.6As a recap, AsyncAPI 2.6 has the following publish and subscribe operations with these semantics:In 2.6, publish and subscribe operations are from user's perspective.New: Send and receive operations in AsyncAPI 3.0In 3.0, the publish and subscribe operations are replaced with send and receive operations. In Migrating to v3 page, the rationale is given as follows:In v2, the publish and subscribe operations consistently caused confusion, even among those familiar with the intricacies.When you specified publish, it implied that others could publish to this channel since your application subscribed to it. Conversely, subscribe meant that others could subscribe because your application was the one publishing.In v3, these operations have been entirely replaced with an action property that clearly indicates what your application does. That eliminates ambiguities related to other parties or differing perspectives.While I agree that publish and subscribe were confusing, I'm not sure if send and receive are less confusing. You still need to talk about whose perspective when defining these operations. AsyncAPI docs talk about application but that's not clear either. Does application refer to the code sending the message (user) or the code receiving the message (server)?In Async 3.0, send and receive operations are from server's perspective. An example will clarify.Account and Email ServicesLet’s revisit our example from last year. You have two microservices: Account Service emits an userSignedUp event and Email Service receives that event:How do you define such an architecture in AsyncAPI 2.6 vs 3.0?Account ServiceFor Account Service, in 2.6, you had to define a channel with subscribe operation because the user has to subscribe to receive messages:asyncapi: 2.6.0channels:  user/signedup:    subscribe:      operationId: publishUserSignedUp      message:        $ref: '#/components/messages/userSignedUp'account-service-2.6.yamlIn 3.0, the channel does not have an operation anymore. Instead, there's a new publishUserSignedUp operation with send action that refers to the channel. This is because the server sends the message:asyncapi: 3.0.0channels:  user/signedup:    address: user/signedup    messages:      publishUserSignedUp.message:        $ref: '#/components/messages/userSignedUp'operations:  publishUserSignedUp:    action: send    channel:      $ref: '#/channels/user~1signedup'    messages:      - $ref: '#/channels/user~1signedup/messages/publishUserSignedUp.message'account-service-3.0.yamlEmail ServiceSimilarly, in Email Service, in 2.6, you had to define a publish operation because the user had to publish a message to the server:asyncapi: 2.6.0channels:  user/signedup:    publish:      operationId: receiveUserSignedUp      message:        $ref : '#/components/messages/userSignedUp'email-service-2.6.yamlHowever, in 3.0, the server receives a message, so the operation has receive action:asyncapi: 3.0.0channels:  user/signedup:    address: user/signedup    messages:      receiveUserSignedUp.message:        $ref: '#/components/messages/userSignedUp'operations:  receiveUserSignedUp:    action: receive    channel:      $ref: '#/channels/user~1signedup'    messages:      - $ref: '#/channels/user~1signedup/messages/receiveUserSignedUp.message'In this blog post, I explored a small part of AsyncAPI and explained the differences between 2.6 operations publish and subscribe and 3.0 operations send and receive. In a nutshell, in 2.6, you need to think from user's perspective and in 3.0 in server's perspective when you define these operations.If you’re interested in learning more, I have a talk on CloudEvents and AsyncAPI and a repo with some samples:Open standards for building event-driven applications in the cloud.asyncapi-basicsOriginally published at https://atamel.dev.AsyncAPI gets a new version 3.0 and new operations was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 4
* Title: 'A tour of Gemini 1.5 Pro samples'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/a-tour-of-gemini-1-5-pro-samples-da48e3c907b3?source=rss-6444b44155fa------2'
* PublicationDate: 'Tue, 07 May 2024 10:37:56 GMT'
* Categories: software-engineering, google-cloud-platform, gemini, genai, software-development

IntroductionBack in February, Google announced Gemini 1.5 Pro with its impressive 1 million token context window.Larger context size means that Gemini 1.5 Pro can process vast amounts of information in one go — 1 hour of video, 11 hours of audio, 30,000 lines of code or over 700,000 words and the good news is that there’s good language support.In this blog post, I will point out some samples utilizing Gemini 1.5 Pro in Google Cloud’s Vertex AI in different use cases and languages (Python, Node.js, Java, C#, Go).AudioGemini 1.5 Pro can understand audio. For example, listen this audio file:It’s 10:28 long but maybe you don’t have time or patience to listen it fully.You can use use Gemini to summarize it with Python in gemini_audio.py:def summarize_audio(project_id: str) -&gt; str:    import vertexai    from vertexai.generative_models import GenerativeModel, Part    vertexai.init(project=project_id, location="us-central1")    model = GenerativeModel(model_name="gemini-1.5-pro-preview-0409")    prompt = """    Please provide a summary for the audio.    Provide chapter titles with timestamps, be concise and short, no need to provide chapter summaries.    Do not make up any information that is not part of the audio and do not be verbose.    """    audio_file_uri = "gs://cloud-samples-data/generative-ai/audio/pixel.mp3"    audio_file = Part.from_uri(audio_file_uri, mime_type="audio/mpeg")    contents = [audio_file, prompt]    response = model.generate_content(contents)    print(response.text)    return response.textYou get a nice summary from Gemini:This episode of the Made by Google podcast discusses the Pixel feature drops,which are software updates that bring new features and improvements to Pixeldevices. The hosts, Aisha Sharif and DeCarlos Love, who are both productmanagers for various Pixel devices, talk about the importance of feature dropsin keeping Pixel devices up-to-date and how they use user feedback to decidewhich features to include in each drop. They also highlight some of theirfavorite features from past feature drops, such as call screening, direct mycall, and clear calling.Chapter Titles with Timestamps:00:00 Intro00:14 Made by Google Podcast Intro00:35 Transformative Pixel Features01:49 Why Feature Drops Are Important02:28 January Feature Drop Highlights02:58 March Feature Drop: Pixel Watch03:41 March Feature Drop: Pixel Phone 05:34 More Portfolio Updates06:09 Pixel Superfans Question07:32 Importance of User Feedback08:07 Feature Drop Release Date08:23 Favorite Feature Drop Features10:17 Outro10:18 Podcast OutroMaybe you want to transcribe the whole audio file instead. Here’s how you can do it with Node.js in gemini-audio-transcription.js:async function transcript_audio(projectId = 'PROJECT_ID') {  const vertexAI = new VertexAI({project: projectId, location: 'us-central1'});  const generativeModel = vertexAI.getGenerativeModel({    model: 'gemini-1.5-pro-preview-0409',  });  const filePart = {    file_data: {      file_uri: 'gs://cloud-samples-data/generative-ai/audio/pixel.mp3',      mime_type: 'audio/mpeg',    },  };  const textPart = {    text: `    Can you transcribe this interview, in the format of timecode, speaker, caption?    Use speaker A, speaker B, etc. to identify speakers.`,  };  const request = {    contents: [{role: 'user', parts: [filePart, textPart]}],  };  const resp = await generativeModel.generateContent(request);  const contentResponse = await resp.response;  console.log(JSON.stringify(contentResponse));}transcript_audio(...process.argv.slice(2)).catch(err =&gt; {  console.error(err.message);  process.exitCode = 1;});You get a full transcription (cropped here to keep it short):## Interview Transcription**00:00** Speaker A: Your devices are getting better over time, and so we thinkabout it across the entire portfolio, from phones to watch, to buds, to tablet.We get really excited about how we can tell a joint narrative across everything. **00:14** Speaker B: Welcome to the Made by Google Podcast, where we meet thepeople who work on the Google products you love. Here's your host, RasheedFinch....**10:19** Speaker C: Don’t miss out on new episodes. Subscribe now wherever youget your podcasts to be the first to listen. Video with audioSo far so good, but how about videos?Take this 57 seconds long video for example:You can describe the video and everything people said in the video in Java with VideoInputWithAudio.java:  public static String videoAudioInput(String projectId, String location, String modelName)      throws IOException {    try (VertexAI vertexAI = new VertexAI(projectId, location)) {      String videoUri = "gs://cloud-samples-data/generative-ai/video/pixel8.mp4";      GenerativeModel model = new GenerativeModel(modelName, vertexAI);      GenerateContentResponse response = model.generateContent(          ContentMaker.fromMultiModalData(              "Provide a description of the video.\n The description should also "                  + "contain anything important which people say in the video.",              PartMaker.fromMimeTypeAndData("video/mp4", videoUri)          ));      String output = ResponseHandler.getText(response);      System.out.println(output);      return output;    }  }You get a pretty impressive output:The video is an advertisement for the new Google Pixel phone. It features aphotographer in Tokyo who is using the phone to take pictures and  videos of thecity at night. The video highlights the phone's "Night Sight" feature, whichallows users to take clear and bright pictures and videos in low-lightconditions. The photographer also mentions that the phone's "Video Boost"feature helps to improve the quality of videos taken in low light. The videoshows the photographer taking pictures and videos of various scenes in Tokyo,including the city streets, a bar, and a puddle. The video ends with thephotographer saying that the new Pixel phone is "amazing" and that she "lovesit."All modalitiesYou can go even further and process images, video, audio, and text at the same time. Here’s how to do it in C# with MultimodalAllInput.cs: public async Task&lt;string&gt; AnswerFromMultimodalInput(        string projectId = "your-project-id",        string location = "us-central1",        string publisher = "google",        string model = "gemini-1.5-pro-preview-0409")    {        var predictionServiceClient = new PredictionServiceClientBuilder        {            Endpoint = $"{location}-aiplatform.googleapis.com"        }.Build();        string prompt = "Watch each frame in the video carefully and answer the questions.\n"                  + "Only base your answers strictly on what information is available in "                  + "the video attached. Do not make up any information that is not part "                  + "of the video and do not be too verbose, be to the point.\n\n"                  + "Questions:\n"                  + "- When is the moment in the image happening in the video? "                  + "Provide a timestamp.\n"                  + "- What is the context of the moment and what does the narrator say about it?";        var content = new Content        {            Role = "USER"        };        content.Parts.AddRange(new List&lt;Part&gt;()        {            new() {                Text = prompt            },            new() {                FileData = new() {                    MimeType = "video/mp4",                    FileUri = "gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4"                }            },            new() {                FileData = new() {                    MimeType = "image/png",                    FileUri = "gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png"                }            }        });        var generateContentRequest = new GenerateContentRequest        {            Model = $"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}"        };        generateContentRequest.Contents.Add(content);        GenerateContentResponse response = await predictionServiceClient.GenerateContentAsync(generateContentRequest);        string responseText = response.Candidates[0].Content.Parts[0].Text;        Console.WriteLine(responseText);        return responseText;    }The output:- The timestamp of the image is 00:49.- The context is that the narrator, a blind filmmaker, is talking about the  story of his film. The story is about a blind man and his girlfriend, and the  film follows them on their journey together.Pdf filesGemini 1.5 can even handle Pdf files. Here’s a Go example in pdf.go that summarizes a given PDF with the help of Gemini:type pdfPrompt struct { pdfPath string question string}func generateContentFromPDF(w io.Writer, prompt pdfPrompt, projectID, location, modelName string) error { // prompt := pdfPrompt{ //  pdfPath: "gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf", //  question: ` //   You are a very professional document summarization specialist. //   Please summarize the given document. //  `, // } // location := "us-central1" // modelName := "gemini-1.5-pro-preview-0409" ctx := context.Background() client, err := genai.NewClient(ctx, projectID, location) if err != nil {  return fmt.Errorf("unable to create client: %w", err) } defer client.Close() model := client.GenerativeModel(modelName) part := genai.FileData{  MIMEType: "application/pdf",  FileURI:  prompt.pdfPath, } res, err := model.GenerateContent(ctx, part, genai.Text(prompt.question)) if err != nil {  return fmt.Errorf("unable to generate contents: %w", err) } if len(res.Candidates) == 0 ||  len(res.Candidates[0].Content.Parts) == 0 {  return errors.New("empty response from model") } fmt.Fprintf(w, "generated response: %s\n", res.Candidates[0].Content.Parts[0]) return nil}You get the summary back (cropped here to keep it short):## Gemini 1.5 Pro: A Summary of its Multimodal, Long-Context Capabilities**Gemini 1.5 Pro** is a cutting-edge multimodal, large language model (LLM)developed by Google DeepMind. Its most significant advancement lies in itsability to process and understand extremely long contexts of information,spanning millions of tokens across various modalities like text, audio, andvideo. This represents a substantial leap forward from previous LLMs, which weretypically limited to processing hundreds of thousands of tokens.Here are the key takeaways from the document:...System instructionsLast but not least, Gemini 1.5 supports system instructions. System instructions enable users to direct the behavior of the model based on their specific needs and use cases. It’s an additional context to understand the task over the full user interaction with the model.For example, here’s a Python example in gemini_system_instruction.py on how to set system instructions:def set_system_instruction(project_id: str) -&gt; str:    import vertexai    from vertexai.generative_models import GenerativeModel    vertexai.init(project=project_id, location="us-central1")    model = GenerativeModel(        model_name="gemini-1.5-pro-preview-0409",        system_instruction=[            "You are a helpful language translator.",            "Your mission is to translate text in English to French.",        ],    )    prompt = """    User input: I like bagels.    Answer:    """    contents = [prompt]    response = model.generate_content(contents)    print(response.text)    return response.textAnd with that instruction, the model answers in French:J'aime les bagels.ConclusionGemini 1.5 Pro is quite impressive with its multimodal nature and large context size. In this blog post, I provided you pointers to samples for different use cases in different languages. If you want to learn more, here’s a list of further resources:Overview of multimodal models documentation page.Getting started with the Vertex AI Gemini 1.5 Pro notebookPython Gemini samples in python-docs-samplesNode.js Gemini samples in nodejs-docs-samplesJava Gemini samples in java-docs-samplesGo Gemini samples in golang-samplesC# Gemini samples in dotnet-docs-samplesAs always, for any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.A tour of Gemini 1.5 Pro samples was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 5
* Title: 'C# and Vertex AI Gemini streaming API bug and workaround'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/c-and-vertex-ai-gemini-streaming-api-bug-and-workaround-dd5eb61f1f8e?source=rss-6444b44155fa------2'
* PublicationDate: 'Wed, 01 May 2024 10:55:28 GMT'
* Categories: software-development, gemini, dotnet, grpc, google-cloud-platform

A user recently reported an intermittent error with C# and Gemini 1.5 model on Vertex AI’s streaming API. In this blog post, I want to outline what the error is, what causes it, and how to avoid it with the hopes of saving some frustration for someone out there.ErrorThe user reported using Google.Cloud.AIPlatform.V1 library with version 2.27.0 to use Gemini 1.5 via Vertex AI's streaming API and running into an intermittent System.IO.IOException.As a test, I took our GeminiQuickstart.cs, change the model from gemini-1.0-pro-vision to gemini-1.5-pro-preview-0409 and ran into the problem after running the sample a few times:[xUnit.net 00:00:08.11]     GeminiQuickstartTest.TestGenerateContentAsync [FAIL]  Failed GeminiQuickstartTest.TestGenerateContentAsync [7 s]  Error Message:   Grpc.Core.RpcException : Status(StatusCode="Unavailable", Detail="Error reading next message. IOException: The request was aborted. IOException: The response ended prematurely while waiting for the next frame from the server.", DebugException="System.IO.IOException: The request was aborted.")---- System.IO.IOException : The request was aborted.-------- System.IO.IOException : The response ended prematurely while waiting for the next frame from the server.  Stack Trace:     at Grpc.Net.Client.Internal.HttpContentClientStreamReader`2.MoveNextCore(CancellationToken cancellationToken)   at Google.Api.Gax.Grpc.AsyncResponseStream`1.MoveNextAsync(CancellationToken cancellationToken)   at GeminiQuickstart.GenerateContent(String projectId, String location, String publisher, String model) in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs:line 82   at GeminiQuickstart.GenerateContent(String projectId, String location, String publisher, String model) in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs:line 82   at GeminiQuickstartTest.TestGenerateContentAsync() in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples.Tests/GeminiQuickstartTest.cs:line 35--- End of stack trace from previous location ---Root causeI wasn’t sure what was causing the issue but thankfully, we have the awesome Jon Skeet in our team and after some debugging, he pointed out issues 2358 and 2361 from grpc-dotnet project. Basically, there’s a bug in the interaction between .NET gRPC client + the Google L7 load balancer that causes the failure.To summarize:The issue happens only when the streaming API is used.The issue manifests itself intermittently in Gemini 1.5 but it could technically happen in other Gemini versions too.Fix and workaroundsThe permanent fix is on the way on the .NET side: dotnet/runtime#9788 and it looks like it’ll be available in .NET 9, .NET 8, and backported to previous versions .NET 7, and .NET 6.That’s great but what do you do in the meantime? There are a couple of options.First, if you don’t require streaming, you can use the non-streaming API. In the GeminiQuickstart.cs sample, instead of streaming responses like this:using PredictionServiceClient.StreamGenerateContentStream response = predictionServiceClient.StreamGenerateContent(generateContentRequest);StringBuilder fullText = new();AsyncResponseStream&lt;GenerateContentResponse&gt; responseStream = response.GetResponseStream();await foreach (GenerateContentResponse responseItem in responseStream){    fullText.Append(responseItem.Candidates[0].Content.Parts[0].Text);}return fullText.ToString();You can do a non-streaming call like this:GenerateContentResponse response = await _predictionServiceClient.GenerateContentAsync(generateContentRequest);Of course, this might not be feasible. If you require streaming, thankfully, there are a couple more workarounds.You can specify an app switch to disable dynamic window sizing:AppContext.SetSwitch("System.Net.SocketsHttpHandler.Http2FlowControl.DisableDynamicWindowSizing", true);2. You can use Grpc.Core instead of Grpc.Net.Client:Add a dependency to Grpc.Core version 2.46.6Add a using directive for Google.Api.Gax.GrpcIn the PredictionServiceClientBuilder object initializer, add GrpcAdapter = GrpcCoreAdapter.InstanceSince the first option is much easier, I tried that and it works great.Hopefully this blog post saved some frustration for someone out there and in the worst case, it’ll serve me as a reminder to remove the AppContext workaround once the permanent fix makes it to the .NET runtime 😀As always, for any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.C# and Vertex AI Gemini streaming API bug and workaround was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 6
* Title: 'A Tour of Gemini Code Assist — Slides and Demos'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/a-tour-of-gemini-code-assist-slides-and-demos-dbe00fc109c7?source=rss-6444b44155fa------2'
* PublicationDate: 'Wed, 24 Apr 2024 08:23:17 GMT'
* Categories: google-cloud-platform, software-development, genai

A Tour of Gemini Code Assist — Slides and DemosThis week, I’m speaking at 3 meetups on Gemini Code Assist. My talk has a little introduction to GenAI and Gemini, followed by a series of hands-on demos that showcase different features of Gemini Code Assist.In the demos, I setup Gemini Code Assist in Cloud Code IDE plugin in Visual Studio Code. Then, I show how to design and create an application, explain, run, generate, test, transform code, and finish with understanding logs with the help of Gemini.Here is the presentation:Here’s the gemini-code-assist-tour repository that contains a guided tour of demos.As always, for any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.A Tour of Gemini Code Assist — Slides and Demos was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 7
* Title: 'Vertex AI Gemini generateContent (non-streaming) API'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/vertex-ai-gemini-generatecontent-non-streaming-api-3c4965f33cf8?source=rss-6444b44155fa------2'
* PublicationDate: 'Mon, 26 Feb 2024 16:58:57 GMT'
* Categories: google-cloud-platform, generative-ai-tools, vertex-ai, gemini

IntroductionIn my recent blog post, I’ve been exploring Vertex AI’s Gemini REST API and mainly talked about the streamGenerateContent method which is a streaming API.Recently, a new method appeared in Vertex AI docs: generateContent which is the non-streaming (unary) version of the API.In this short blog post, I take a closer look at the new non-streaming generateContent API and explain why it makes sense to use as a simpler API when the latency is not super critical.Recap: streamGenerateContent methodAs a recap, this is how you can use the streamGenerateContent method:PROJECT_ID="genai-atamel"LOCATION="us-central1"API_ENDPOINT=${LOCATION}-aiplatform.googleapis.comMODEL_ID="gemini-pro"curl -X POST -H "Authorization: Bearer $(gcloud auth print-access-token)" \    --no-buffer -H "Content-Type: application/json"  \    https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent -d \    $'{      "contents": {        "role": "USER",        "parts": { "text": "Why is the sky blue?" }      },      "generation_config":{        "temperature": 0.4,        "top_p": 1,        "top_k": 32,        "max_output_tokens": 2048      }  }'And this is the sort of response you’d get:[{    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": "The sky appears blue due to a phenomenon called Rayleigh scattering. This occurs when sunlight, which is composed of all colors of the visible spectrum, passes through the Earth"            }          ]        },        "safetyRatings": [          ...      }    ]  }  ,  {    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": "'s atmosphere. The atmosphere is made up of tiny particles, such as molecules and dust, that are much smaller than the wavelength of visible light.\n\nAs"            }          ]        },        "safetyRatings": [          ...        ]      }    ]  }  ,  {    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": " sunlight passes through the atmosphere, these particles scatter the light in all directions. However, the amount of scattering depends on the wavelength of the light. Shorter wavelengths, such as blue light, are scattered more than longer wavelengths, such as red light. This is because the shorter wavelengths have a higher frequency and therefore interact more with"            }          ]        },        "safetyRatings": [          ...        ]      }    ]  }  ,  {    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": " the particles in the atmosphere.\n\nAs a result, more blue light is scattered in all directions, which means that when we look up at the sky, we see more blue light than any other color. This is why the sky appears blue during the day.\n\nAt sunset and sunrise, the sunlight has to travel through"            }          ]        },        "safetyRatings": [          ...        ]      }    ]  }  ,  {    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": " more of the atmosphere to reach our eyes. This means that more of the blue light is scattered away, and we see more of the longer wavelengths, such as red and orange. This is why the sky appears red or orange at these times of day."            }          ]        },        "finishReason": "STOP",        "safetyRatings": [          ...        ]      }    ],    "usageMetadata": {      ...    }  }  ]Notice how the text is split in multiple chunks and the last chunk has finishReason: STOP to indicate that it's the last one. Since this is a streaming API, you'd receive these chunks as they become available.This is useful if you have a latency sensitive application such as a chat application. But it makes processing the response more complicated, as you need to combine the text in each chunk into a final text.New: generateContent methodIf you have an application where latency is not that important and you’d rather wait for the whole response before displaying anything to the user, then generateContent method is more appropriate. It is the non-streaming (unary) version of the API.Usage of the generateContent is very similar to before:PROJECT_ID="genai-atamel"LOCATION="us-central1"API_ENDPOINT=${LOCATION}-aiplatform.googleapis.comMODEL_ID="gemini-pro"curl -X POST -H "Authorization: Bearer $(gcloud auth print-access-token)" \    -H "Content-Type: application/json"  \    https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent -d \    $'{      "contents": {        "role": "USER",        "parts": { "text": "Why is the sky blue?" }      },      "generation_config":{        "temperature": 0.4,        "top_p": 1,        "top_k": 32,        "max_output_tokens": 2048      }  }'But the response is different:{    "candidates": [      {        "content": {          "role": "model",          "parts": [            {              "text": "The sky appears blue because of a phenomenon called Rayleigh scattering. This occurs when sunlight passes through the Earth's atmosphere and interacts with molecules of nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light, so they scatter the light in all directions. However, blue light is scattered more than other colors because it has a shorter wavelength. This means that more blue light reaches our eyes from all directions, making the sky appear blue."            }          ]        },        "finishReason": "STOP",        "safetyRatings": [          ...        ],        "citationMetadata": {          ...        }      }    ],    "usageMetadata": {      ...    }  }As you can see, we got back a single chunk with the full text and finishReason: STOP. This is certainly much simpler to process and a better choice for applications where you don't need to display responses right away.SummaryIn this short blog post, I showed you how to use the non-streaming generateContent API and explain why it makes sense to use as a simpler API when the latency is not super critical.If you want to run these samples yourself, you can check out my GenAI repo on GitHub:https://github.com/meteatamel/genai-samples/tree/main/vertexai/gemini/console/curlAs always, for any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.Vertex AI Gemini generateContent (non-streaming) API was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Using Vertex AI Gemini from GAPIC libraries (C#)'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/using-vertex-ai-gemini-from-gapic-libraries-c-390ee4066002?source=rss-6444b44155fa------2'
* PublicationDate: 'Wed, 14 Feb 2024 12:11:12 GMT'
* Categories: google-cloud-platform, generative-ai-tools, software-development, software-architecture

IntroductionIn my previous Using Vertex AI Gemini REST API post, I showed how to use the Gemini REST API from languages without SDK support yet such as C# and Rust.There’s actually another way to use Gemini from languages without SDK support: GAPIC libraries. In this post, I show you how to use Vertex AI Gemini from GAPIC libraries, using C# as an example.What is GAPIC?At this point, you might be wondering: What’s GAPIC? GAPIC stands for Google API CodeGen. In Google Cloud, all services have auto-generated libraries from Google’s service proto files. Since these libraries are auto-generated, they’re not the easiest and most intuitive way of calling a service. Because of that, some services also have hand-written SDKs/libraries on top of GAPIC libraries.Specifically for Vertex AI and Gemini, there are hand-written SDKs for Python, Java, Node.js, and Go. You’d prefer these hand-written SDKs over GAPIC versions.For other languages, you can either rely on the REST API or you can use the GAPIC library for that language, if it exists.REST vs. GAPICAs an example, in C#, there is a GAPIC library for Vertex AI. It’s mostly through the PredictionServiceClient ofGoogle.Cloud.AIPlatform.V1 library. For other languages, you'd search for similarly named classes and libraries to see if there's GAPIC support.When you have the choice to use the REST API or the GAPIC library, which one should you use? It’s really up to you but keep in mind:The REST API is probably more intuitive but you need to handle authentication and you need to parse the returned JSON to strongly typed objects on your own.The GAPIC library provides authentication and strongly typed classes but you need to figure out how to construct the right protobuf classes, so it can be a little unintuitive.Gemini REST API from C#As a reminder, in my Using Vertex AI Gemini REST API post, I showed how to use the Gemini REST API from C# to describe an image of a cat. The full sample is in my GitHub repo in GenerateTextFromImageGcs.cs.Gemini GAPIC library from C#Now, let’s build the same sample but this time using the GAPIC library.Define prompt and imageDefine the prompt and the image stored on Cloud Storage:string text = "Describe this image in detail in a few paragraphs";string imageUrl = "gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg";Console.WriteLine($"Text: {text}");Console.WriteLine($"ImageUrl: {imageUrl}");It’s an image of a cat 🙂While we’re at it, also define some constants we’ll need later:const string ProjectId = "genai-atamel";const string Location = "us-central1";const string Publisher = "google";const string Model = "gemini-pro-vision";Create a prediction service clientCreate a PredictionServiceClient that you'll use to make requests to Vertex AI:// Create clientvar predictionServiceClient = new PredictionServiceClientBuilder{    Endpoint = $"{Location}-aiplatform.googleapis.com"}.Build();Construct the request payloadNext, construct the request payload with the right parameters. This can be tricky to get right:// Initialize request argument(s)var content = new Content{    Role = "USER"};content.Parts.AddRange([    new() {        Text = prompt    },    new() {        FileData = new() {            MimeType = "image/png",            FileUri = imageUrl        }    }]);var generateContentRequest = new GenerateContentRequest{    Model = $"projects/{ProjectId}/locations/{Location}/publishers/{Publisher}/models/{Model}",    GenerationConfig = new GenerationConfig    {        Temperature = 0.4f,        TopP = 1,        TopK = 32,        MaxOutputTokens = 2048    }};generateContentRequest.Contents.Add(content);Send the request and parse the streaming responseSending the request is a simple call:// Make the request, returning a streaming responseusing PredictionServiceClient.StreamGenerateContentStream response =     predictionServiceClient.StreamGenerateContent(generateContentRequest);Note that the authentication is handled for us, so we don’t need to do anything special to pass the auth token or anything like that.The response is streaming, you can print it out as it comes back:Console.Write($"Response: ");// Read streaming responses from server until completeAsyncResponseStream&lt;GenerateContentResponse&gt; responseStream = response.GetResponseStream();while (await responseStream.MoveNextAsync()){    GenerateContentResponse responseItem = responseStream.Current;    Console.WriteLine(responseItem.Candidates[0].Content.Parts[0].Text);}You can see the full sample in my GitHub repo in GenerateTextFromImageGcs.cs.Run the sampleRun the sample:dotnet runPrompt: Describe this image in detail in a few paragraphsImageUrl: gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpgResponse:  This is an image of a cat in the snow. The cat is standing on asnow-covered field, and it appears to be looking off to the side. It is ashort-haired tabby cat with a white belly and paws. The cat's fur is brown andblack, with some white patches on its chest and belly. The cat's eyes areyellow, and its nose is pink. The cat's tail is long and fluffy, and it iscurled up at the end. The cat's body is slightly hunched, and its ears areperked up. The cat's fur is matted and dirty, and it appears to be in poorcondition. The cat's surroundings are covered in snow, and there are no otheranimals or people visible in the image.You’ll realize that the response will stream as it’s available. That’s nice.SummaryUltimately, the language of your choice determines what SDK/library to use to talk to Gemini on Vertex AI:Hand-written SDKs for Python, Java, Node.js, Go.GAPIC libraries for other Google Cloud supported languages like C#.REST API for everything else.As always, if you have any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.Using Vertex AI Gemini from GAPIC libraries (C#) was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'Using Vertex AI Gemini REST API (C# and Rust)'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/using-vertex-ai-gemini-rest-api-c-and-rust-cf6a488e19b1?source=rss-6444b44155fa------2'
* PublicationDate: 'Mon, 05 Feb 2024 09:31:04 GMT'
* Categories: software-architecture, google-cloud-platform, generative-ai-tools, software-development

IntroductionBack in December, Google announced Gemini, its most capable and general model so far available from Google AI Studio and Google Cloud Vertex AI.The Try the Vertex AI Gemini API documentation page shows instructions on how to use the Gemini API from Python, Node.js, Java, and Go.That’s great but what about other languages?Even though there are no official SDKs/libraries for other languages yet, you can use the Gemini REST API to access the same functionality with a little bit more work on your part.In this blog post, I want to take a look at an example on how to use the Gemini REST API from languages without SDK support yet: C# and Rust in this case.Gemini REST APIThere are currently two models available in the Gemini API:Gemini Pro model (gemini-pro): Fine-tuned model to handle natural language tasks such as classification, summarization, extraction, and writing.Gemini Pro Vision model (gemini-pro-vision): Multimodal model that supports adding image and video prompts for a text response.Gemini API page is a great resource on learning how to make the right HTTP requests with the right parameters.For example, to send a multi-modal (text + image) request to Gemini, you’d make an HTTP POST to gemini-pro-vision model with the right parameters in the request body:POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-pro-vision:streamGenerateContent{  "contents": {    "role": "user",    "parts": [      {        "fileData": {          "mimeType": "image/png",          "fileUri": "gs://cloud-samples-data/ai-platform/flowers/daisy/10559679065_50d2b16f6d.jpg"        }      },      {        "text": "Describe this picture."      }    ]  },  "safety_settings": {    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",    "threshold": "BLOCK_LOW_AND_ABOVE"  },  "generation_config": {    "temperature": 0.4,    "topP": 1.0,    "topK": 32,    "maxOutputTokens": 2048  }}A couple of things to watch out for:You need to get and set an authentication token with your request.The responses come in batches (see sample responses), so you need to extracttext from each batch and combine them to get the full text.Now, let’s take a look at how to make these requests from actual code.Gemini REST API from C#Let’s say we want Gemini to describe an image in Cloud Storage for us from my default language: C#.Define prompt and imageLet’s define the prompt and the image stored on Cloud Storage:string text = "Describe this image";string imageUrl = "gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg";Console.WriteLine($"Text: {text}");Console.WriteLine($"ImageUrl: {imageUrl}");It’s an image of a cat 🙂Construct the request payloadConstruct the right request payload with prompt and image url:private static string GeneratePayload(string text, string imageUrl){    var payload = new    {        contents = new        {            role = "USER",            parts = new object[] {                new {text = text},                new {file_data = new {                        mime_type = "image/png",                        file_uri = imageUrl                    }                }            }        },        generation_config = new        {            temperature = 0.4,            top_p = 1,            top_k = 32,            max_output_tokens = 2048        }    };    return JsonConvert.SerializeObject(payload);}Send the request with auth tokenGet an authentication token and send the HTTP request:private async static Task&lt;string&gt; SendRequest(string payload){    GoogleCredential credential = GoogleCredential.GetApplicationDefault();    var handler = credential.ToDelegatingHandler(new HttpClientHandler());    using HttpClient httpClient = new(handler);    httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));    HttpResponseMessage response = await httpClient.PostAsync(EndpointUrl,        new StringContent(payload, Encoding.UTF8, "application/json"));    response.EnsureSuccessStatusCode();    return await response.Content.ReadAsStringAsync();}Parse the responseReceive the HTTP response from Gemini and deserialize the JSON response body. JSON response body will have a list of candidates to parse through (see response body). It makes sense to create aGeminiResponse.cs class to capture this to make our lives easier in JSON deserialization:string payload = GeneratePayload(text, imageUrl);string response = await SendRequest(payload);var geminiResponses = JsonConvert.DeserializeObject&lt;List&lt;GeminiResponse&gt;&gt;(response);Finally, use some LINQ magic to combine the text in each batch into a final text:string fullText = string.Join("", geminiResponses    .SelectMany(response =&gt; response.Candidates)    .SelectMany(candidates =&gt; candidates.Content.Parts)    .Select(part =&gt; part.Text));Console.WriteLine($"Response: {fullText}");You can see the full sample in my GitHub repo in GenerateTextFromImageGcs.cs.Run the sampleRun the sample:dotnet runText: Describe this imageImageUrl: gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpgResponse:  A cat is walking in the snow. The cat is gray and white, and it has a long tail. The cat is looking at the camera. The snow is white and it is covering the ground.That’s a nice and pretty good description of the image from Gemini!Gemini REST API from RustOf course, you can use the REST API from any language. My colleague from the Chrome DevRel team, André Bandarra, rewrote my sample in Rust. It follows the same pattern of getting an auth token, generating the request with the right parameters and combining the text from the response:let authentication_manager = AuthenticationManager::new().await?;let scopes = &amp;["https://www.googleapis.com/auth/cloud-platform"];let token = authentication_manager.get_token(scopes).await?;let prompt = "Describe this image";let image_url = "gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg";let payload = GenerateContentRequest {    contents: vec![Content {        role: "user".to_string(),        parts: vec![            Part::Text(prompt.to_string()),            Part::FileData {                mime_type: "image/jpeg".to_string(),                file_uri: image_url.to_string(),            },        ],    }],    generation_config: Some(GenerationConfig {        max_output_tokens: Some(2048),        temperature: Some(0.4),        top_p: Some(1.0),        top_k: Some(32),        ..Default::default()    }),    tools: None,};let resp = reqwest::Client::new()    .post(&amp;endpoint_url)    .bearer_auth(token.as_str())    .json(&amp;payload)    .send()    .await?;let response = resp.json::&lt;GenerateContentResponse&gt;().await?;response.0.iter().for_each(|chunk| {    chunk.candidates.iter().for_each(|candidate| {        candidate.content.parts.iter().for_each(|part| {            if let Part::Text(text) = part {                print!("{}", text);            }        });    });});You can check out the full sample in his repo in generate-text-from-image-gcs.rs.SummaryAdmittedly, using the Gemini REST API is not easy as it can be as other languages that have Gemini SDK support. However, with a little bit of work in making the right request and parsing the response, it’s straightforward to talk to Gemini with the REST API from any other language. To see more Gemini samples, you can check out our repos on GitHub:https://github.com/meteatamel/genai-sampleshttps://github.com/andreban/gemini-rustAs always, if you have any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.Using Vertex AI Gemini REST API (C# and Rust) was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Test and change an existing web app with Duet AI'
* Author: 'Mete Atamel'
* URL: 'https://medium.com/google-cloud/test-and-change-an-existing-web-app-with-duet-ai-863380c9a9d6?source=rss-6444b44155fa------2'
* PublicationDate: 'Mon, 29 Jan 2024 14:40:51 GMT'
* Categories: google-cloud-platform, generative-ai-tools, software-engineering, software-development

In the Create and deploy a new web app to Cloud Run with Duet AI post, I created a simple web application and deployed to Cloud Run using Duet AI’s help. Duet AI has been great to get a new and simple app up and running. But does it help for existing apps? Let’s figure it out.In this blog post, I take an existing web app, explore it, test it, add a unit test, add new functionality, and add more unit tests all with the help of Duet AI. Again, I captured some lessons learned along the way to get the most out of Duet AI.Before you startI’m using Duet AI with Cloud Code in Visual Studio Code but you can use Duet AI from other IDEs supported by Cloud Code such as IntelliJ. You can also use it in the Cloud Shell Editor or Cloud Workstations from Google Cloud Console. Check out Duet AI for Developers page for more details on how to get setup with Duet AI.Todo list appFor the application, I went through the Tutorial: Create a web API with ASP.NET Core to create a Todo list app with MVC architecture and a web API.Or, you can start with the finished tutorial app in before folder in my GitHub repo.Check that you can run the application:cd before/TodoApi dotnet runYou should see the app running:Building...info: Microsoft.Hosting.Lifetime[14]      Now listening on: http://localhost:5112info: Microsoft.Hosting.Lifetime[0]      Application started. Press Ctrl+C to shut down.You can also interact with the app with Swagger/OpenAPI endpoint at http://localhost:5112/swagger/:We’re ready to use Duet AI’s help.Explain code with Duet AIFirst, let’s see if Duet AI can help us to understand the code.Open TodoItemsController.cs in your IDE and ask the following to Duet AI using the Duet AI chat functionality:PromptCan you explain what this class does?AnswerThe TodoItemsController class is a RESTful API controller that provides CRUD operations for the TodoItem entity. The controller inherits from the ControllerBase class, which provides a number of common methods for handling HTTP requests.The GetTodoItems() method returns a list of all TodoItems in the database. The GetTodoItem() method returns a single TodoItem by its id. The PutTodoItem() method updates a TodoItem in the database. The PostTodoItem() method creates a new TodoItem in the database. The DeleteTodoItem() method deletes a TodoItem from the database.The controller also implements the IActionFilter interface, which allows it to perform custom actions before and after each request. In this case, the controller uses the IActionFilter interface to validate the input parameters for each request and to ensure that the user is authenticated.The TodoItemsController class is a good example of how to create a RESTful API using ASP.NET Core. The controller is well-organized and easy to use, and it provides a number of features that make it a good choice for building APIs.That was a pretty good explanation and gave me a good overview of the code!Test code with Duet AINext, let’s see if Duet AI can help us to test the code.Highlight the PostTodoItem() method in TodoItemsController.cs:Ask the following:PromptCan you help me test this highlighted code with curl? The server is running on port 5112AnswerSure. To test the highlighted code with curl, you can use the following command:curl -X POST \ -H “Content-Type: application/json” \ -d ‘{ “name”: “Learn ASP.NET Core”, “description”: “Build a web API with ASP.NET Core”, “priority”: 4, “done”: false }’ \ http://localhost:5112/api/TodoItemsThis command will send a POST request to the api/TodoItems endpoint on the server, with the body of the request containing the JSON representation of a new TodoItem object. The server will then create the new TodoItem and return a response with the status code 201 Created.You can then use the curl command to get the newly created TodoItem by using the following command:curl http://localhost:5112/api/TodoItems/1This command will send a GET request to the api/TodoItems/1 endpoint on the server, and the server will return a response with the status code 200 OK and the body of the response containing the JSON representation of the newly created TodoItem.As you can see, the command works but it isn’t correct. ToDoItem.cs contains id, name, and isComplete fields. Duet AI got the name correctly but the rest was not correct.Let’s try a more specific prompt.PromptCan you try generating the curl command, taking ToDoItem.cs into account?AnswerSure. The following command will send a POST request to the api/TodoItems endpoint on the server, with the body of the request containing the JSON representation of a new TodoItem object:curl -X POST \ -H "Content-Type: application/json" \ -d '{ "name": "Learn ASP.NET Core", "isComplete": false }' \ http://localhost:5112/api/TodoItemsThis command will create a new TodoItem with the name “Learn ASP.NET Core” and the value of IsComplete set to false.This time the answer was correct! Duet AI needs good context to provide good answers.Add unit tests with Duet AINext, let’s see if Duet AI can help us to add unit tests.First, let’s refresh our memories about the unit test frameworks in C#.PromptWhat unit testing frameworks exist for ASP.NET apps?AnswerThere are a number of unit testing frameworks available for ASP.NET apps. Some of the most popular include:xUnit: xUnit is a free and open source unit testing framework for .NET. It is based on the xUnit unit testing framework for Java. xUnit is a popular choice for ASP.NET apps because it is easy to use and has a large community of users and contributors.NUnit: NUnit is a free and open source unit testing framework for .NET. It is based on the JUnit unit testing framework for Java. NUnit is a popular choice for ASP.NET apps because it is powerful and has a large community of users and contributors.MSTest: MSTest is a built-in unit testing framework for Visual Studio. It is a good choice for ASP.NET apps because it is easy to use and integrates well with Visual Studio.In addition to these three frameworks, there are a number of other unit testing frameworks available for ASP.NET apps. Some of these include FluentAssertions, Shouldly, and AssertJ.The best unit testing framework for your ASP.NET app will depend on your specific needs and preferences. If you are new to unit testing, xUnit or NUnit are a good place to start. If you are more experienced with unit testing, MSTest or one of the other more powerful frameworks may be a better choice.This is a pretty good answer! I remember using xUnit before, so let’s continue with more specific xUnit questions.How can I create a new xUnit project for unit tests for TodoItemsController.cs from the command line? I want the project in ToDoApiTest folderAnswerTo create a new xUnit project for unit tests for TodoItemsController.cs from the command line, you can use the following steps:Open a terminal window.Navigate to the ToDoApiTest folder.Run the following command:This will create a new xUnit project with the name UnitTest1.Open the UnitTest1.csproj file in a text editor. Add the following code to the file:using TodoApi.Controllers;using Xunit;namespace TodoApi.Tests;public class TodoItemsControllerTests{     [Fact]     public void GetTodoItems_ShouldReturnAllTodoItems()     {         // Arrange         var controller = new TodoItemsController();         // Act         var result = controller.GetTodoItems();         // Assert         Assert.Equal(3, result.Count());     }}This answer is pretty good for generating a new test project but it has some issues:Duet AI is telling us to add code to the project file, UnitTest1.csproj instead of UnitTest1.cs. You an go ahead and rename UnitTest1.cs to ToDoItemsControllerTests.cs and add the suggested code.ToDoItemsController is another project, so we need to refer to the other project from the test project.Here comes another question to Duet AI:PromptHow can I change the project file ToDoApiTest.csproj to refer to the TodoItemsController class in the other project ToDoApi.csproj?AnswerTo change the project file ToDoApiTest.csproj to refer to the TodoItemsController class in the other project ToDoApi.csproj, you can use the following steps: [1]Open the ToDoApiTest.csproj file in a text editor.Find the line that references the TodoItemsController class.Change the path to the TodoItemsController class to the path in the ToDoApi.csproj file.Save the ToDoApiTest.csproj file.Here is an example of how you would change the project file ToDoApiTest.csproj to refer to the TodoItemsController class in the other project ToDoApi.csproj:While the answer is not perfect ( Controller Include is not needed), it gives you what you need, just copy the ProjectReference Include to your testing project (see TodoApiTest.csproj)Now, the generated testing code is still not compiling:We need a mocked DB context for the controller.PromptHow can I have a mocked TodoContext for xunit testing?AnswerTo have a mocked TodoContext for xunit testing, you can use the following steps:Create a new class that inherits from the TodoContext class.In the new class, override the OnConfiguring method and provide a fake connection string.In the new class, override the OnModelCreating method and provide a fake model builder.Create a new instance of the new class and use it in your xunit tests.Here is an example of how you would create a mocked TodoContext for xunit testing:The answer is pretty good and actually works! You can now go back to the TodoItemsControllerTests.cs and change it to have the basic unit test:We got our unit test project setup with our first unit test done! As you observed, we had to nudge Duet AI with the right context to get a good answer. It’s important to not expect perfection from Duet AI or GenAI in general.Add new functionality with Duet AINow that we have basics in place, let’s use Duet AI to add a new endpoint to check in an item exists or not.PromptCan you modify TodoItemsController.cs to add a method to check if a ToDoItem exists? The method should handle the HTTP HEAD method and it should use the id of ToDoItem to check for existenceAnswerusing Microsoft.AspNetCore.Mvc;using Microsoft.EntityFrameworkCore;using TodoApi.Models;namespace TodoApi.Controllers{    [Route("api/[controller]")]    [ApiController]    public class TodoItemsController : ControllerBase    {...        // HEAD: api/TodoItems/5        [HttpHead("{id}")]        public async Task&lt;IActionResult&gt; HeadTodoItem(long id)        {            var todoItem = await _context.TodoItems.FindAsync(id);            if (todoItem == null)            {                return NotFound();            }            return NoContent();        }    }}The answer is very good and compiles out of the box!You can test by adding an item:curl -X POST \    -H "Content-Type: application/json" \    -d '{        "name": "Learn ASP.NET Core",        "isComplete": false    }' \    http://localhost:5112/api/TodoItems{"id":1,"name":"Learn ASP.NET Core","isComplete":false}%Verify that the new endpoint works:curl -I -X HEAD http://localhost:5112/api/ToDoItems/1HTTP/1.1 204 No ContentDate: Mon, 15 Jan 2024 13:42:24 GMTServer: KestrelDuet AI inline code generation seems to work pretty well, presumably, Duet AI has good context when it’s generating code.Add more unit tests to test new functionality with Duet AILet’s see if Duet AI can generate us some unit tests for the new functionality.PromptCan you add a new unit test in TodoItemsControllerTests.cs to test the HeadTodoItem method in TodoItemsController.cs?Answerusing Microsoft.AspNetCore.Mvc;using Microsoft.EntityFrameworkCore;using System.Threading.Tasks;using TodoApi.Controllers;using TodoApi.Models;using Xunit;namespace TodoApi.Tests;public class TodoItemsControllerTests{    [Fact]    public async Task HeadTodoItem_ReturnsNoContent_WhenItemExists()    {        // Arrange        var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()            .UseInMemoryDatabase(databaseName: "TodoItems")            .Options;        var context = new TodoContext(options);        var controller = new TodoItemsController(context);        var id = 1;        // Act        var result = await controller.HeadTodoItem(id);        // Assert        Assert.IsType&lt;NoContentResult&gt;(result);    }    [Fact]    public async Task HeadTodoItem_ReturnsNotFound_WhenItemDoesNotExist()    {        // Arrange        var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()            .UseInMemoryDatabase(databaseName: "TodoItems")            .Options;        var context = new TodoContext(options);        var controller = new TodoItemsController(context);        var id = 1;        // Act        var result = await controller.HeadTodoItem(id);        // Assert        Assert.IsType&lt;NotFoundResult&gt;(result);    }}The good news is that both tests compile fine! The first test actually fails but the second test works. The reason why the first test fails is that the in-memory database does not have any items really, so I asked Duet AI to refine the test.PromptHeadTodoItem_ReturnsNoContent_WhenItemExists method does not work because the item is not in the list. Can you change the code to make sure the item actually exists?AnswerSure. Here is the updated code for the HeadTodoItem_ReturnsNoContent_WhenItemExists method:[Fact]public async Task HeadTodoItem_ReturnsNoContent_WhenItemExists(){    // Arrange    var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()        .UseInMemoryDatabase(databaseName: "TodoItems")        .Options;    var context = new TodoContext(options);    var controller = new TodoItemsController(context);    var todoItem = new TodoItem { Id = 1, Name = "Item 1", IsComplete = false };    context.TodoItems.Add(todoItem);    await context.SaveChangesAsync();    var id = 1;    // Act    var result = await controller.HeadTodoItem(id);    // Assert    Assert.IsType&lt;NoContentResult&gt;(result);}That’s perfect!The only thing you need to change is to make sure the second test refers to a non-existent item. You can check the final tests TodoItemsControllerTests.cs:Duet AI has been very useful in adding new functionality and adding unit tests for it!Lessons learnedLet’s recap some lessons learned as I played with different prompts:Direct Duet AI with your knowledge. You need to direct Duet AI to the right answer with your knowledge of the code. E.g. Don’t say “Test the highlighted code with curl” but instead say “Test the highlighted code takingToDoItem.cs into account".Be specific. The more specific you get, the better will be the answers. E.g. Don’t say “How can I test?” but rather say “How can I create axUnit project to unit testTodoItemsController.cs class from command line"Don’t expect perfection. Duet AI will tell you to generate code in wrong files sometimes or generate unnecessary configuration. You need to know your way around in your project to know what’s needed where, rather than expecting perfect answers that you can copy/paste.The generated tests are good starting points. They won’t be perfect but will save a lot of time. You then need to nudge Duet AI to generate the right tests for what you want to exactly test.Inline code/test generation seems to pretty good in Duet AI, presumably, due to Duet AI having good context with existing code and files in your project.In this blog post, I took an existing web app, explored it, tested it, added unit tests and added new functionality with the help of Duet AI. With specific prompts, right context, and some nudging, it gives good answers and overall, it saved me a lot of time and effort.As always, if you have any questions or feedback, feel free to reach out to me on Twitter @ meteatamel.Originally published at https://atamel.dev.Test and change an existing web app with Duet AI was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.
