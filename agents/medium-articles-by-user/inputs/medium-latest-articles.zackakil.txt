
== Article 1
* Title: 'Hobbies to Hired: 5 tips for making great Portfolio Projects to Land Your First Tech Job'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/hobbies-to-hired-5-tips-for-making-great-portfolio-projects-to-land-your-first-tech-job-8c45aaeafd06?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Sun, 14 Apr 2024 00:07:10 GMT'
* Categories: portfolio, ai, junior-developer, tech, jobs

When you starting off in tech, building strong portfolio projects is an essential step in showcasing your skills and impressing potential employers. However, figuring out what projects to create and how to make them shine can feel overwhelming. This guide will break down 5 actionable tips to help you start building projects that youâ€™ll be excited to showÂ off!1. Go â€œoffÂ trailâ€Especially for those coming from bootcamps, just having projects that were â€œpart of the curriculumâ€ in your portfolio shows to the interviewer that you can follow instructions in a safe environment. However in the wild those conditions are rare. You need projects that show you ability to (independently) IDENTIFY a problem, then (independently) LEARN something new that can be used to solve that problem, and of course successfully (independently) APPLY that learning to solve theÂ problem.When telling the story of these â€œoff trail projectsâ€ a good structure isÂ :I saw thisÂ problem.I learnt about this technology that could be used to solve theÂ problem.I built this thing using what I learnt to solve theÂ problem.When hiring juniors employers main concern is that they will have to provide extensive hand-holding. These kind of projects directly address that concern by showing that you can take ownership of your learning rather than always having to rely on explicit instructions.So back yourself and go â€œoffÂ trailâ€!Key tip about your first â€œoff trail projectsâ€One new thing at a time: Itâ€™s good practice focus on one new skill at a time with each new project. This helps keep the complexity manageable, allowing you to apply your existing knowledge while stretching your abilities with your new learnings. E.g I might want to learn about Tensorflow.js and also d3js, and I already know how to make HTML webpages. So Iâ€™ll make a project were I apply my existing HTML knowledge to my new Tensorflow.js knowledge in some app. Then Iâ€™ll do a separate project were I build something with d3js in a HTML app. Then finally another separate project that combines everything! Thatâ€™s 3 unique projects that can go my portfolio!2. The Enthusiasm hackPeople want to work with enthusiastic people that do exciting work. As a junior developer, enthusiasm should be one of your strengths. But, how do you get interviewers enthusiastic about your portfolio projects?First off, YOU have to be genuinely enthusiastic by your portfolio projects!Do that by working on projects that genuinely excite you. Classic patterns for identifying projects that elicit genuine enthusiasm are:ğŸ¤© Projects based on yourÂ hobbies.ğŸ¤© Project that solve a real problem that you genuinely face.ğŸ¤© Projects that solve a real problem for people you genuinely careÂ about.Projects that are harder to elicit genuine enthusiasm forÂ are:ğŸ¥± Any project you â€œhad to doâ€ / â€œwere told to doâ€ e.g assigned coursework.ğŸ¥± Projects that solve hypothetical problems for hypothetical people. (due to their lack of an authentic story)Come interview time youâ€™ll find it a lot easier to speak enthusiastically about your work, and that genuine enthusiasm is infectious. So lean into, let yourself geek out and exude that energy that embodies the excitement of being a junior developer!An example of a portfolio projects of mine that I was able to easy get enthusiastic about was my rugby recording robot. Without any more context, you can probably guess what sport IÂ like.Additional perks of working on intrinsically interesting projectsIt will help keep your engaged in the project when things get challenging.You are are going to be more inclined to talk about the project with those around you. This gives you free practice in your technical communication skills.Often times they will be projects that are naturally unique to you. Great portfolio projects are the oneâ€™s only you could have made. Their uniqueness embody a part of your personality e.g an app that was inspired by your hobby and the tech skills you want to become proficient in.3. â€œMany and Smallerâ€ vs â€œFewer andÂ Biggerâ€You probably have an idea for an app in your head (or are currently building one) that is a bit of a beast e.g multi-social user login, full admin CRUD, email notifications, payment system, plus whatever the USP isÂ etc.As a portfolio project these kind of projects have merit in giving you holistic experience in every part of application development. However, with one big project you get to setup your repoâ€¦ once, your get you design your UXâ€¦ once, you get to identify a problem specâ€¦ once. Vs with many smaller projects were you get to go through the whole Problem -&gt; Idea -&gt; Design -&gt; Implementation process multiple times starting from a blankÂ slate.This is the same principle as the â€œPhotography Quantity vs Qualityâ€ story. Give yourself many opportunities to try things, learn from them, and take those learnings to the next project, and repeat as many times as possible.So the algorithm to a fruitful portfolio journey isÂ :knowledge = Nonewhile(1):   learnings = build_project(size="small", applied_knowledge=knowledge)  knowledge += learnings4. Make it instantly interactive ğŸª„Having interviewed many devs straight out of bootcamps, the first thing I check is their github and go to their latest (or pinned) project. For web devs itâ€™s usually a website, coolâ€¦ but where is it? Oh I have to first clone it then do â€œnpm runâ€ yadayadayada â€œlocalhostâ€â€¦ sigh!Donâ€™t make interviewers work to see your work. If youâ€™ve made a website, have it deployed somewhere (e.g Github pages) and put the link at the top of the projectsâ€™ README. Did you make some cool python stuff? have a link to a Colab or Sreamlit app and put the link at the top of the projectsâ€™ README.If there is literally no way to make your work instantly interactive then at least have some kind of videos or gifs embedded in the README. Have some way to instantly show yourÂ work.5. Put a bow on itÂ ğŸ€Youâ€™ve probably put days/weeks of time into your project. However youâ€™ve probably put zero time into the first thing someone will see when looking at your projectâ€¦ theÂ README!Take 15 minutes to really spruce up your projects README with nice formatting, links, images, gifs etc. The README is your projects (and likely you as a developersâ€™) first impression, so make it one that shows you care about communication and presentation of your work. Check out this list by @matiassingers for inspirationÂ : awesomeÂ READMEsBonus tip (especially when applying to larger companies likeÂ Google)At Google there is a specific emphasis put on knowledge sharing. Showcasing that you not only add value as an individual, but also elevate those aroundÂ you.Showcasing your enthusiasm for sharing knowledge by creating resources that help others. Whether itâ€™s visualization tools or blog posts, sharing your insights and resources can make a powerful impression.ConclusionSo, there you have it: five (+1) actionable tips to for building great portfolio projects. By venturing â€œoff trail,â€ embracing your enthusiasm, building many smaller projects, making your work instantly interactive, and putting a bow on it with a polished README, youâ€™ll create a portfolio that not only showcases your skills but also tells a compelling story about you as a developer.Remember the tech world is constantly evolving, and the best developers are those who never stop learning. Use your portfolio as a tool for growth, pushing your boundaries and showcasing your ability to adapt and master new skills. Good luck on yourÂ journey!

== Article 2
* Title: 'How to Identify Innovative Ideas with the Technical Horizon Framework'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/how-to-identify-innovative-ideas-with-the-technical-horizon-framework-f7c9c613682e?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Sat, 12 Aug 2023 15:52:26 GMT'
* Categories: technology, innovation, ideas, management, ai

â€œItâ€™s been done before!â€ or â€œItâ€™s not possible!â€, the last thing you want to hear when you pitch your new idea. What if you could quickly and pragmatically identify better, more innovative ideas that are actually feasible? Enter the â€œTechnical Horizon Frameworkâ€.What are Technical Horizons?As technology advances with time, what is technically feasible to consumers, developers, and researchers is constantly expanding, these are the Technical Horizons. The key points about themÂ are:Technical Horizons are always expanding as technology advances.Ideas are static and are defined by the simplest technology required to buildÂ them.Every idea within your Technical Horizon is technically feasible.Consumers can build ideas within the Consumer Technical Horizon, developers can build ideas within the Developer and Consumer Technical Horizon, and researchers can build ideas within the Researcher, Developer and Consumer Technical Horizon.The Technical Horizon chart aboveÂ shows:Plotting data (e.g charts in spreadsheets) is easy for consumers i.e very feasible.Image Generation is newly feasible to consumers (as of 2023) through consumerÂ apps.Video classification is very feasible to developers with tools like Googles Video Intelligence API, but would be infeasible for consumers without developer knowledge.Video summarisation is a newly feasible idea for developers by combining multiple APIs like the previously mentioned Video Intelligence API and Googles PaLMÂ API.Video clip generation is currently not feasible without research level knowledge however in the near future APIs to do this may be available.Full movie generation would only be feasible with a lot of researchÂ time.Where are your innovative ideas?You can use the Technical Horizon Framework to identify which of your ideas are â€œinnovativeâ€. Simply by looking to the immediate inside edge of your Technical Horizon. If you are a consumer, Image Generation is an innovative idea that is good for consumers toÂ explore.As a developer; Video summarisation is currently an innovative space. The reason the space on the immediate inside of your Technical Horizons is innovative, is because in this space there are ideas that have never been done before; because they could never have been done before due to the enabling technology only recently becoming available.What about ideas outside of your Technical Horizon?Almost feasibleÂ ideasAssume your a developer and youâ€™ve come up with an idea were the APIs arenâ€™t quite there yet (e.g Video clip generation), itâ€™s just outside your Technical Horizon, or â€œalmost feasibleâ€. You have a fewÂ options:Park the idea and wait for the tools to become available, and spend your time on more feasibleÂ ideas.Or, embark on building the idea anyway, but risk wasting your developer resources as at any time an API could be released that nudges the Developer Technical Horizon forward, potentially making the idea buildable with a few lines of code, and rendering your work potentially redundant. Whatâ€™s especially dangerous in this scenario is you (or your team) are at risk of â€œsunk-cost fallacyâ€ if an API does come out after youâ€™ve already invested time into your own implementation when using someone else's solution might beÂ better.Technical Horizons are invisibleThe challenge here is the unknown. When will the developer tools be ready? A day, month, year? This is were having good channels for up to date news about new tools is valuable. Also being a part of early access programs and going to conferences lets you peek into the future of where your Technical Horizon isÂ going.Very infeasible ideas (Moonshots)Keep track of them, they might be infeasible now, but Technical Horizons can leap forward suddenly, like it has with the surge of LLMs (Large Language Model) technology.If waiting isnâ€™t an option, a word for these high risk - high reward ideas is a â€œMoonshotâ€. Accept that itâ€™s a bold undertaking so the pay off needs to be worth theÂ risk.Watch out for â€œShoehorn Innovationâ€â€œMy idea is innovative because itâ€™s using the latest technology!â€, aclichÃ© when any new technology is released.The â€œinnovativenessâ€ of an idea is not defined by the technology it uses, but by the actual technical requirements of the problem itâ€™s solving. An innovative idea is solving a problem that can only be solved with new technology, so ask the question â€œCan this problem already be solved with an existing technology?â€, a hard but important pill to swallow sometimes.Wrap upWhen you find an idea that is genuinely newly feasible with a new technology, and itâ€™s solving a problem that could never have been solve before, then you know you have an innovative idea.Key takeawaysA truly innovative idea is one that solves a problem that could not previously beÂ solved.Listen out for news about new technology that could push your Technical Horizon forward. â€œWhat can I do now do that I couldnâ€™t beforeâ€, is a good question to be asking about any new technology you hearÂ about.Beware of â€œShoehorn Innovation.â€ Just because a new technology is available doesnâ€™t mean that you need to use it in your idea. Donâ€™t just use new technology for the sake of usingÂ it.

== Article 3
* Title: 'How I conquered writer's block with Google Assistant + Google Docs'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Fri, 30 Jul 2021 15:51:57 GMT'
* Categories: writing, google-docs, google-cloud-platform, productivity, google-assistant

Over lockdown I built a voice app that would interview me about my projects and help write articles for me. What would usually take me weeks, now takes me an hour or two, and as a testament to that I wrote one article a day for a week and lived to tell theÂ tale:Day 1See what Video Intelligence API can do with this visualisation toolDay 2ğŸ¥ ğŸ™ˆ Using AI to automate phobia safe film productionDay 3ğŸ¥ ğŸ‰ AppSheet + AppScript for sports videography highlights trackingDay 4ğŸš´ Safer cycling with ML and Edge TPUâ€™sDay 5 (this articleÂ ğŸ¤¯)How I conquered writer's block with Google Assistant + Google DocsBackgroundWriting is something Iâ€™ve always struggled with, and in my role as a Developer Advocate it can be an important part of the job to make and share content about what Iâ€™ve built in order to inspire and educate fellow developers. It wouldnâ€™t be uncommon for me to take longer writing an article than it did for me to build the thing I was writingÂ about.One time I had built something and someone reached out to interview me about my project so that they could write an article on my behalf. Obviously I found that experience a lot easier. So that gave me an idea; what if I could build some voice AI that would interview me about my projects and generate the articles forÂ me.SolutionUsing Google Action Builder I built a Google Assistant app that would ask me a set of questions about a single project and then output what I said into a generated GoogleÂ Doc.I followed this tutorial to learn how to build anÂ action:https://medium.com/media/d61d324ae46d344aa227912458dc9e14/hrefWithin the Assistant app I have a custom webhook that is a Google Cloud Function that takes what was said and writes it into a new Google Doc using the Google Docs python client API. See the code for that Cloud Function on myÂ github:GitHub - ZackAkil/project-reporter-voice-app: A voice app that interview me about my projects in order to generate articles.The first step is always the hardest so even though itâ€™s essentially just a dictation app with some prompting questions, it makes that initial draft phrase of article writing virtually zero friction for me. I never have to see that horrible immovable empty document again when writing articles. Here it is inÂ action:https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/hrefTricky challengesAuthentication! Authentication! Authentication!Getting Google Assistant to talk directly to Google Docs is especially tricky as you have to navigate a merade of authentication services in order to create and write docs on the usersâ€™ behalf. I created this slide deck to loosely document the process in case youâ€™d like to do something similar, but be warned it is a lot of throwing around of api keys, client secrets, and refresh tokens. But it can beÂ done!Speak quick and concise (look to using Dialogflow)Google Assistant apps have a 30 second response limit so I have to speak quite quickly, and as soon as there is a pause in my voice it cuts the mic and takes that as my answer. For my first version of this app Iâ€™m OK with that, however for a more natural interview experience where you can take your time and collect your thoughts before you answer Iâ€™d look into using Dialogflow which allows for more granulare configuration and longer talking response times. Check out Lee Boonstraâ€™s I/O talk to learn more about Dialogflow:https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/hrefRemembering the DocÂ IDBecause I was using a Cloud Function as my webhook, there was no guarantee that the exact same instance of that Cloud Function would be called during the same conversation (one of the tricky things with serverless tech). So I used Cloud Firestore as a storage medium to store which conversation IDs (which are passed to every webhook call) map to which generated Google DocÂ IDs.FutureAs an extra step in the article writing process it would be cool to have some machine learning text generation automatically generate tweets based on what I said about myÂ project.Also the interview process is currently just a static set list of questions. I could use some natural language ML tools like the Natural Language API or AutoML Natural Language to do some intelligent analysis into my responses in order to adapt the follow up questions like in a real interview.TakeawaysLeverage your strengths (for me itâ€™s building things) to overcome your weaknesses (for me itâ€™s writingÂ things).You can get Google Assistant to talk directly to Google Docs (and the rest of G Suite), itâ€™s just a fair bit of authentication loops you have to jumpÂ through.It can be a lot easier to say something than to write something so be aware of voice technologies like Google Assistant and Dialogflow.Follow and ask me questions on twitter ZackAkil!

== Article 4
* Title: ' Safer cycling with ML and Edge TPUâ€™s'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Thu, 29 Jul 2021 13:33:19 GMT'
* Categories: raspberry-pi, machine-learning, cycling, google, edge-computing

A while ago I tried to make my bike smarter with machine learning to help keep me safe whilst commuting through busy LondonÂ streets.BackgroundIf you live in a big city and cycle you probably have developed superhuman reflexes and awareness, especially in cities like London where a lot of the narrow roads were originally designed for horse andÂ cart.After enough close calls and some actual calls I figured it was time to try and solve this problem with a modern approach (without mirrors).SolutionUsing a small computer (Raspberry Pi) attached to the bike. It would run an object detection model on images being taken from a rear facing camera. The system would detect the different potential dangers behind and intuitively relay that information back to the rider (via a handlebar mounted LED strip) so that they can maintain their focus on what'sÂ ahead.what an object detection model wouldÂ detectNeed forÂ speedWith just the Raspberry Pi running the object detection model it was able to make predictions at a rate of about 2 predictions per second, which for this use case is on the slow side. That is where the Coral Edge TPU comes in. It is able to run the same model at 30â€“60 predictions per second giving the system the super low latency it needs. You can find out more about Coral Edge TPUâ€™s from this Google I/OÂ session:https://medium.com/media/7400903711405d60c51ba4a89e582f1c/hrefSpecifically I used the Edge TPU USB Accelerator and had it and the Raspberry Pi both attached to theÂ bike:prototype showcasing the Coral Edge TPU usb acceleratorAfter the Raspberry Pi + Edge TPU made itâ€™s prediction of what's behind the bike, it would then send commands to an RGB LED strip on the bikesâ€™ handlebars to then indicate to the rider where the dangers from behind are, and what level of danger those things are (because itâ€™s able to distinguish between the different types of vehicle e.g car, bus, truck, bike, pedestrian).prototype and real bikeÂ versionThe specificÂ modelDue to this being a pretty generic prediction task (traffic object detection), there were already pretrained models I could download and use that already detected different types of vehicles. In fact I ended up using the exact same model that Bill used in his 2019 Google I/OÂ demo:https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/hrefYou can explore different kinds of pretrained models on the CoralÂ website:Models | CoralYou can also find all the code for this project on myÂ github:GitHub - ZackAkil/edge-TPU-safe-bike: An application of realtime object-detection running on an Edge TPU for making cycling in busy cities a little less terrifying.Interesting pointsAustralia has it ownÂ dangersWhen I presented this idea in Sydney there were discussions around if a specific version of this could be made for their specific dangers. At the time I assumed they might have been talking about some cliche dangers when you think of Australiaâ€¦ kangaroos, snakes, drop bears. But actually for cyclists in Australia itâ€™s teritorial magpies that are constantly attacking. So a custom version that included swooping birds in the model would be perfect forÂ them.Pretrained models not designed for toyÂ carsItâ€™s important to test exactly how youâ€™re going to use pretrained models. When I was demoing this idea on stage I hadnâ€™t thoroughly tested how well it worked with toy cars and I got suboptimal results. This is where training custom models with tools like AutoML Object Detection would shine. Also interesting to note that my toy version of this demo worked a lot better when I included a printed out fake road for the toy cars to sit on, see in the previous gif aboveÂ ğŸ‘†.https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/hrefFutureI want to train a custom model that would be trained specifically on London traffic (including the distinctive taxis) and then I could look into training other custom models for different regions such as an Australian magpie edition. Oh, and actually make this on a real bike that I can cycle (not just a prototype).TakeawayItâ€™s good to be aware of technology like Edge TPUâ€™s in case you have a problem that requires super low latency and mobility. Also always checking to see if there are pretrained models that could already solve your problem and save you a huge amount of development time.Follow and ask me questions on twitter ZackAkil!

== Article 5
* Title: '  AppSheet + AppScript for sports videography highlights tracking'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Wed, 28 Jul 2021 14:59:12 GMT'
* Categories: productivity, appsheet, app-script, sports, google-cloud-platform

During the summer I was asked to help with some video recording for a rugby tournament, but rather than spending hours rewatching footage to find highlights, I quickly threw together some tech to make things a lotÂ easier.BackgroundSport videography is a hobby of mine (specifically rugby). It is however very tedious work rewatching full matches in order to find the highlights. That's why when I was asked to help a friend film an entire rugby tournament this summer I was hesitant. Rewatching a full dayâ€™s worth of rugby videos is not something I wanted either of us to have toÂ do.There are some cameras that have a â€œmark special momentâ€ button that will let you set a marker of when something cool happens, but we were using multiple cameras that didnâ€™t have that feature. We discussed a simple paper based solution where we just noted down the time of when cool things happened. This was the solution we settled on until the morning of the event which is when I discovered a technology that would make things a lotÂ easier.Solutionoriginal spreadsheetFor the data capture/entry we used AppSheet, which is a no code app builder that integrates seamlessly into Google Sheets (as well as other things). As long as you have your spreadsheet set up with the right data formats for your columns, AppSheet can generate a clean and robust data entry UI that feeds data directly back into the spreadsheet.I found out about AppSheet from this video by Carter MorganÂ ğŸ‘‡:https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/hrefThanks to Carterâ€™s intro I was able to build the perfect data entry app on the same morning of the tournament that would fit my needs (and was even able to style the colour theme of it), and both my friend and I were able to access it from a native Android App on theÂ field!generated AppSheetÂ appThroughout the day, anytime something cool happened whilst the cameras were rolling I just put in a note using the app, and because of how I set it up; the app knew to use the current time as the defaultÂ value.After the tournament wasÂ overWhen it came to finding the actual clips that contained the highlights after the tournament, this is where AppScript came into play (I know AppScript and AppSheet are annoying similar names). AppScript is a technology that lets you build automation within GSuit e.g Google Drive, Sheets, Docs, etc. Check out this video for a quick overviewÂ ğŸ‘‡https://medium.com/media/72a41db491bd122711fb457c959c30e7/hrefWe first uploaded all of the video clips from the tournament into Google Drive. Then within the Google Sheet that contained all of our inputted highlight timestamps I wrote a AppScript that would loop through each rowÂ and:extract out the time data for that specific highlightscan through the file metadata of all the videos in a specified Google Drive folder to find the video that overlapped with thatÂ timeautomatically paste in a link to that video file back into the spreadsheetThe AppScript that does this is attached to the Google Sheet so that I get a nice toolbar button to trigger the script. When the script is triggered it prompts me to specify the Google Drive folder url containing the tournament videos to search through. Watch it in actionÂ ğŸ‘‡the automated clip finding process using myÂ scriptYou can see all the AppScript code in myÂ github:gsuit-sports-videography-tool/script.gs at main Â· ZackAkil/gsuit-sports-videography-tooland hereâ€™s a visual overview of the wholeÂ system:high level diagram of the entireÂ solutionTricky challengesCamera clocksWhen I first ran the script to find the matching clips they were all wrong. Turns out the system clock of the camera was wrong ğŸ¤¦â€â™‚ï¸. So I added a quick and dirty bit of code to the script to offset theÂ times.â€˜File createdâ€™ time vs â€˜File updatedâ€™Â timeWhen I was writing the code to fetch the video metadata from Google Drive I thought that the file.getDateCreated() method from the AppScript Google Drive API would tell me when the video was captured. Turns out that method just returns when the file was created/uploaded â€œin Google Driveâ€. This had me panicked for a while thinking the project was dead as I could not extract the lower level file metadata, but turns out file.getLastUpdated() will tell you the last time the file was edited which in our case was the time the video was captured on theÂ camera.FutureNow that we have timestamps of highlights along with the specific video clips we can look into training machine learning models that could automatically detect highlights in videos. This could be done with a tool like AutoML Video Intelligence which can train powerful video models without us writing any machine learningÂ code.TakeawayThe time it takes to build an app is not correlated to how useful that app can be. Tools like AppSheet take minutes to use and when the right moment comes for them they can save you literally days of work. Combine that with scripting tools like AppScript and you have a powerful problem solvingÂ duo.Follow and ask me questions on twitter ZackAkil!

== Article 6
* Title: '  Using AI to automate phobia safe film production'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Tue, 27 Jul 2021 12:58:37 GMT'
* Categories: machine-learning, machine-vision, google-cloud-platform, video-production, film

ğŸ¥ğŸ™ˆ Using AI to automate phobia safe film productionAt Google I/O this year I used ML to build an automated video processing pipeline that detects whatâ€™s in a video and automatically hides the parts you would findÂ scary.https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/hrefBackgroundWhilst watching some movies with some friends, we discovered that one of our friends has a deathly fear snakes and so we had a tell them exactly when the snakes were going to appearÂ , and sometimes we got this wrong ğŸ˜¬ (watching Harry Potter was a poor choice in hindsight).SolutionI built an automated video processing pipeline using Video Intelligence API that would scan through a video to detect all the things within the video. Then if it detects something that matches with the phobia that we told the system about, it would use the Transcoder API to insert an overlay and hide it from view. So the solution was based on these twoÂ APIs:Video Intelligence APIhttps://zackakil.github.io/video-intelligence-api-visualiser/A powerful video analyse API on Google Cloud Platform. It uses machine learning to detect what is in a video e.g faces, people, objects, textÂ etc.If you want to see all that it can do you can check out this interactive demo. Also check out the docsÂ here.Transcoder APIA scalable cloud based video transcoding API on Google Cloud Platform that lets you perform complex video transcoding jobs. Some of itsâ€™ features includeÂ :generating different bit rates andÂ formatsgenerating thumbnailsinserting overlaysinserting adÂ breakssee more details on the API in theÂ docs.Connecting theseÂ togetherThe system pipeline uses the Label Detection feature of the Video Intelligence API (see visualisation of this feature here) to detect whatâ€™s in each scene of the video and return a list of labels with time segments. If any of those labels match the phobia we are scared of (e.g snakes, swans, bread, birds) we then use the Transcoder API to inject a full screen overlay on the video for those time segments, hiding them fromÂ view.high level APIÂ pipelineTo get these APIâ€™s to run in a scalable way I used a chain of Cloud Storage Buckets and triggered Cloud Functions that automatically run when a new video is uploaded.exact tech stack including storage andÂ triggersTricky challengeIn order to tell the system what my phobia/phobias were without some fancy UI I came up with the idea of just using the â€˜see no evilâ€™ emoji ğŸ™ˆ in the uploaded file name followed by the phobia. Then get the cloud function code to read the filename and extract the phobia from that. See the specific code in the githubÂ repo.OutputFinally I would just drag and drop a scary video into the input storage bucket with a filename like test_natureğŸ™ˆswan.mp4Â â€¦original scary video (containing swans)â€¦ and in less time that it would take to watch the video the Video Intelligence API has already scanned the video and the Transcoder API has already generated the new non-scary video. Check out the I/O video â˜ï¸ that demo segment was all realtime!generated phobia safe video (hidingÂ swans)Future workIn the future I want to use the same kind of pipeline to solve video processing problems surrounding sports analysis and sports video production e.g highlights generation. You can see the kind of things I do with sport in this episode of Dale Markowitzâ€™s Making with MLÂ series:https://medium.com/media/25c04956d43e7829875470e071585770/hrefTakeawaysBuilding ML powered video production pipeline need only take you plugging together a couple ofÂ APIâ€™s.GitHub - ZackAkil/phobia-safe-videos-with-ml: How you can use ML to make watching videos safer for people with unique and/or serious phobias.Follow and ask me questions on twitter ZackAkil!

== Article 7
* Title: 'See what Video Intelligence API can do with this visualisation tool'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Mon, 26 Jul 2021 12:53:43 GMT'
* Categories: google-cloud-platform, google-cloud, machine-vision, machine-learning, python

I built a visualiser for the Google Cloud Video intelligence API that allows anybody to explore all of the features of theÂ API.https://zackakil.github.io/video-intelligence-api-visualiser/If youâ€™re working with video, then the Video Intelligence API has a lot of powerful features for analysing and detecting whatâ€™s in your videos, for small pet projects or massive scale applications.Here is a list of the features of the API along with some example use cases for those features (see the docs for moreÂ info):Label Detection (phobia detection)Shot Detection (detect ad break segments)Object Tracking (traffic counting)Person Detection (sports player analysis)Face Detection (analyse eye contact withÂ camera)Logo Recognition (product placement detection)Speech Transcription (generate interactive subtitles)Text Detection (search videos of presentations based on text inÂ slides)Explicit Content Detection (automated content monitoring)Celebrity Detection**restricted access and not shown in visualiserAnalyse and visualise your ownÂ videosYou can follow this tutorial in order to analyse your own videos. You can also use this script I made to quickly analyse videos stored in Google CloudÂ Storage:video-intelligence-util/video_intel_util.py at main Â· ZackAkil/video-intelligence-utilAfter you have analysed your video you can drag the outputÂ .json along with the original video into the visualiser to visualise theÂ results.Hope this tools helps you explore and work with the Video Intelligence API! Please reach out to me with any questions,Happy video analysing!JavaScript is not available.

== Article 8
* Title: 'Zombies & Model Rot (with ML Engine + DataStore)'
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Wed, 24 Oct 2018 14:48:41 GMT'
* Categories: machine-learning, cloud-computing, zombies, big-data, google-cloud-platform

Donâ€™t leave your models to rot into obscuritySo youâ€™ve deployed your machine learning model to the cloud and all of your apps and services are able to fetch predictions from it, nice! You can leave that model alone to do its thing foreverâ€¦ maybe not. Most machine learning models are modeling something about this world, and this world is constantly changing. Either change with it, or be leftÂ behind!What is modelÂ rot?Model rot, data rot, AI rot, whatever you want to call it, itâ€™s not good! Letâ€™s say weâ€™ve built a model that predicts if a zombie is friendly or not. We deploy it to the cloud and now apps all over the world are using it to help the general public know which zombies they can befriend without getting bitten. Amazing, people seem super happy with your model, but after a couple of months you start getting angry emails from people who say that your model is terrible! Turns out that the zombie population mutated! Now your model is out of date, or rotten! You need to update your model, and even better, add in a way to keep track of your models state of rot so that this doesnâ€™t happenÂ again.This is an example of a very sudden case of modelÂ rot!Itâ€™s not just zombies thatÂ changeSure, fictional creatures can change, but so can financial markets, residential environments, traffic patterns, weather patterns, the way people write tweets, the way cats look! Ok maybe cats will always look like cats (although give it a few million years and maybe not). The point is that depending on what your models are predicting will effect how fast they are going toÂ rot.Itâ€™s also important to note that the thing you are predicting doesnâ€™t need to change for your model to rot. Maybe the sensor you are using to capture input data gets changed. Anything that negatively effects the performance of your model when itâ€™s deployed is effectually causing model rot, either remove the thing causing the reduced performance or update the model (most likely going to be the latterÂ choice).Letâ€™s fight model rot (with ML Engine + DataStore)Thereâ€™s a zombie outbreak, however itâ€™s not as scary as the movies would lead you to believe. They are pretty slow moving creatures, and a lot of them are just looking for human friends, but some arenâ€™t. To help people make the right choice of zombie friends we developed a model that predicts if a zombie is friendly or not based on a few characteristics:We used Scikit-Learn to build a Decision Tree Classifier model. See the notebook here to see the exact code to doÂ this.The plan is now to deploy our model to ML Engine which will host our model for us on the cloud. (see how we do this using gcloud commands in a notebook)First weâ€™ll throw our model into cloudÂ storage:gsutil cp model.joblib gs://your-storage-bucket/v1/model.joblibThen create a new ML Engine model, which you can do using the Google Cloud Console UI or using gcloud commands (which you can see here used in a notebook):ML EngineÂ UIThen we deploy our Decision Tree model as versionÂ 1:Creating a new version of your model using the Cloud ConsoleÂ UITo make it easier for apps to fetch predictions from our model, weâ€™ll create a public endpoint using Cloud Functions. You can read more about how to do this in this blog post I wrote with my colleague Sara Robinson.hereâ€™s the current architecture of ourÂ systemOK, our model is deployed and apps can easily fetch predictions from it! Now to monitor for model rot with DataStore!DataStore?Imagine a place in the cloud where you can store millions of python dictionaries quickly, and then query them (also quickly). Thatâ€™s DataStore, a fully managed NoSQL database on Google Cloud Platform. If you have previous experience with databases you might be used to carefully planning out exactly what structure of data to store in tables, then experience the pain of creating migration scripts to update the structure of your database. Non of that nonsense with DataStore, want to store the following data:{  "name": "Alex"  "occupation": "Zombr trainer"}then do it (using the python client library):# create new entity/rownew_person = datastore.Entity(key=client.key('person'))new_person['name'] = 'Alex'new_person['occupation'] = 'Zombr trainer'# save to datastoreclient.put(new_person)Oh wait, you want to start storing peoples githubs and twitters? go forÂ it:# create new entity/rownew_person = datastore.Entity(key=client.key('person'))new_person['name'] = 'Zack'new_person['occupation'] = 'Zombr CEO'new_person['github'] = 'https://github.com/zackakil'new_person['twitter'] = '@zackakil'# save to datastoreclient.put(new_person)and DataStore will say â€œthankÂ youâ€:DataStoreâ€™s UIUsing DataStore to collect modelÂ feedbackThe feedback data that we are going to collect will look like the following:{"model": "v1","prediction input": [2.1, 1.4, 5.3, 8.0],"prediction output": 1,"was correct": False,"time": "23-10-2018,14:45:23"}This data will tell us what version of our model on ML Engine was used to generate the prediction (model), what the input data for the prediction was (prediction input), what the prediction made by the model was (prediction output), if the prediction was correct (the actual feedback from the user) (was correct), and the time that the feedback was submitted (time).Weâ€™ll use Cloud Functions again to make another web API endpoint, this time to receive the feedback data and store it in DataStore:https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/hrefNow our system architecture looks like the following:the new architecture of ourÂ systemThe client apps just need to add in a intuitive way for the users to submit their feedback. In our case it could be a simple â€˜thumbs up or thumbs downâ€™ prompt after the user is presented with a prediction:You may have come across feedback prompts like thisÂ beforeGet creative with how you collectÂ feedbackOften times you can infer feedback about your model, rather than explicitly requesting it from the users like Iâ€™ve done in the Zombr interface. For example if we see that a user stops using the app immediately after a prediction, we could use that data to indicate a wrong prediction ğŸ˜¬.Back in reality, a dog adoption agency might have a recommender system for new owners. The rate of successful adoptions made by the model is itâ€™s own performance feedback. If the agency suddenly sees that the system is making a lot fewer successful matches than usual, then they can use that as the indication that the model is rotten and may need updating.Feedback is collected, nowÂ what?Now we can analyse the feedback data. For any data analyse work I default to using Jupyter Notebooks.Click here for the full notebook of how I fetch data from DataStore and analyse the feedback.The important bits of fetching data from DataStore are, first installing the DataStore python clientÂ library:pip install google-cloud-datastorethen you can import it and connect to DataStore:from google.cloud import datastore# connect to DataStoreclient = datastore.Client('your project id')# query for all prediction-feedback items query = client.query(kind='prediction-feedback')  # order by the time field query.order = ['time']  # fetch the items # (returns an iterator so we will empty it into a list) data = list(query.fetch())The library with automatically convert all of the data into python dictionaries:print(data[0]['was correct'])print(data[0]['model'])print(data[0]['time'])print(data[0]['input data'])&gt;&gt;&gt; True&gt;&gt;&gt; v1&gt;&gt;&gt; 2018-10-22 14:21:02.199917+00:00&gt;&gt;&gt; [-0.8300105114555543, 0.3990742221560673, 1.9084475908892906, 0.3804372006233603]Thanks to us saving a â€œwas correctâ€ boolean in our feedback data we can easily calculate the accuracy of our model from the feedback by looking at the ratio of â€˜Truesâ€™ for thisÂ field:number_of_items = len(data)number_of_was_correct = len([d for d in data if d['was correct']])print(number_of_was_correct / number_of_items)&gt;&gt;&gt; 0.840.84 is not much rot since we first trained our model which scored ~0.9 accuracy, but thatâ€™s calculated using all of the feedback data together. What if we do this same accuracy calculation on a sliding window across our data and plot it? (you can see the code for doing this in the analysis notebook)Thatâ€™s a big drop in performance for the most recent feedback.We should investigate further. Letâ€™s compare the input data (i.e the zombie characteristic data) from the times of high accuracy to the times of low accuracy. Good thing we also collected that in our feedback:blue = correct prediction, red = incorrect predictionblue = correct prediction, red = incorrect predictionAh, the data looks completely different. I guess the zombie population has mutated! We need to retrain our model ASAP with new data. Good thing we collected the input data in the feedback, we can use that as the new training data (saves us having to manually collect new data). We can use information about the prediction the model made (â€œpredictionâ€ field) and the usersâ€™ feedback (â€œwas correctâ€ field) to infer the correct prediction label for the new trainingÂ data:https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/hrefSee how this code is used in the bottom of the feedback analysis notebook.With this new data set we can train a new version of our model. This is an identical process to training the initial model but using a different data set (see the notebook), and then uploading it to ML Engine as a new version of theÂ model.Once itâ€™s on ML Engine you can either set it as the new default version of the zombies model so that all of your clients will automatically start having their prediction requests sent to the new model, or you can instruct your clients to specify the version name in their prediction requests:setting v2 as the defaultÂ modelIf you set the default model to v2 then all prediction request to â€œzombiesâ€ will go to the v2Â version:PREDICTION REQUEST BODY:{"instances":[[2.0, 3.4, 5.1, 1.0]],"model":"zombies"}or your clients can just be more specific:PREDICTION REQUEST BODY:{"instances":[[2.0, 3.4, 5.1, 1.0]],"model":"zombies/versions/v2"}After all that you can sit back and just run the same analysis after some more feedback has been collected:seems like people find our v2 modelÂ helpfulHopefully this has given you a few ideas on how you can monitor your deployed models for model rot. All of the code used can be found in the githubÂ repo:ZackAkil/rotting-zombie-modelReach out to me @ZackAkil with any thoughts/questions on monitoring modelÂ rot.Zombies &amp; Model Rot (with ML Engine + DataStore) was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'TensorFlow & reflective tapeÂ : why Iâ€™m bad at basketball '
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Fri, 05 Oct 2018 14:00:27 GMT'
* Categories: basketball, tensorflow, sports, data-science, machine-learning

TensorFlow &amp; reflective tape ğŸ€ (am I bad at basketball?)Recently a friend got me into basketball. Turns out, itâ€™s a lot harder than it looks. No matter, I can over engineer a solution using machine learning. If your into ML and shooting hoops then thereâ€™s also this article that combined TensorFlow and basketball in a simulation.nothing but netâ€¦ if there was aÂ netThe task is to find the exact angle of my shots. Then I can hopefully use that information in a proactive way to getÂ better.psst! the code for all of this is on myÂ GithubTask 1: collecting dataI didnâ€™t need to follow the seems of the ball, but it looksÂ coolI donâ€™t have access to 3D tracking studios fitted with 200 cameras, but I do have Ebay. Itâ€™s quite easy to buy reflective tape online and stick it to the ball. Then (thanks to the lack of lighting at my local court) I can record some footage of me practicing in the evening and capture the balls movements.The torch build into my phone provides the perfect light source to bounce off the reflective tap of theÂ ball.As a result the footage captured shows a sparkly object flying through a mostly dark scene, perfect for doing some image manipulation inÂ python.Task 2: Getting our video intoÂ pythonFirstly I do everything in Python, and a really easy way to import video into Python is to use a library called â€˜scikit-videoâ€™, so I installed that:pip install scikit-videoand then used it to load in my video as aÂ matrix:from skvideo.io import vreadvideo_data = vread('VID_20180930_193148_2.mp4')The shape (that you can find by running video_data.shape) of this data is (220, 1080, 1920, 3). Which means 220 frames of 1080x1920 pixels of 3 channels of colour (red, green,Â blue):raw videoÂ dataTask 3: Extracting the shot (image processing)So I want to get the data on just the balls movement. Fortunately, itâ€™s one of the only things moving in the video. So I can do my favourite video processing trick: delta frame extraction! (thatâ€™s what I call it, but thereâ€™s probably another name forÂ it).By subtracting all of the pixel values in one frame from all of the pixel values in the next frame you will be left with non-zero values in just the pixels that haveÂ changed.calculating delta frame in order to isolate movingÂ pixelsCool cool cool, now I do that for each frame in the video and combine the result into oneÂ image:adding together all of the delta frames from the videoÂ sequenceNext is extracting the shot data into a usable format. So weâ€™ll covert the pixel values that are lite up into a list of x and y points. The code to do this is a numpy function called numpy.where which will find all of the values that are True in an array and return their indices (i.e their positions in theÂ matrix).But before we do that, weâ€™ll quickly crop out just the balls trajectory and flip the data so that it starts at the origin (the bottom left of theÂ scene):and the resulting image:Notice how it still seems upside down? Thatâ€™s only because images tend to be drawn starting at the top left corner (note the axis numbering). When we convert the pixels to data points and draw them on a normal graph they will get drawn starting at the bottom leftÂ corner.now we run our numpy.where code to get the pixels as dataÂ points:pixels_that_are_not_black = cropped_trail.sum(axis=2) &gt; 0y_data, x_data = numpy.where(pixels_that_are_not_black)awesome! our relatively clean ball trajectory dataTask 4: Build TensorFlow modelThis is where TensorFlow shines. You may be used to hearing about using TensorFlow for building neural networks, but you can define almost any mathematical formula and tell it to optimise whatever parts of it you want. In our case we will use the formula for a trajectory which we know from primary school toÂ be:extremely mathematical equation of trajectory that I foundÂ onlineÎ¸ (theta) is the angle of the shot (the value we really careÂ about)v is the initialÂ velocityg is gravityÂ (9.8m/s)x is the horizontal position (data we haveÂ already)y is the vertical position (data we haveÂ already)A far more interesting way to see the equation in action is to play around with this trajectory tool.We can use the ball trail of my shot as the x and y of the equation and task TensorFlow with finding the correct angle (Î¸) and initial velocity (v) that fits my shots x and yÂ data:Weâ€™ll start be recreating our trajectory equation in TensorFlow:first tell it what data we will feed it when we run the optimisation:x = tf.placeholder(tf.float32, [None, 1])y = tf.placeholder(tf.float32, [None, 1])next tell it what variables we want it to tweak and tune in order to fit the trajectory curve to ourÂ data:angle_variable = tf.Variable(40.0, name='angle_variable')force_variable = tf.Variable(100.0, name='force_variable')gravity_constant = tf.constant(9.8, name='gravity_constant')and join all of these together (warning: itâ€™s going to look quite messy, but itâ€™s just the maths equation seen before written in TensorFlow syntax):left_hand_side = x * tf.tan(deg2rad(angle_variable))top = gravity_constant * x ** 2bottom = (2*(force_variable)**2) *             (tf.cos(deg2rad(angle_variable))**2)output = left_hand_side - (top / bottom)then tell TensorFlow how to tell if itâ€™s doing a good job or not at fitting the trajectory function to ourÂ data:# the lower this score, the bettererror_score = tf.losses.mean_squared_error(y, output)create an optimiser that will do the actual tweaking of variables (angle_variable and force_variable) in order to reduce the error_score:optimiser = tf.train.AdamOptimizer(learning_rate=5) optimiser_op = optimiser.minimize(error_score)Task 5Â :Â MagicWe can now run the optimisation task to find the angle_variable and force_variable values that fit myÂ shot.sess = tf.Session()sess.run(tf.global_variables_initializer())# do 150 steps of optimisationfor i in range(150):    sess.run([optimiser_op],              feed_dict={x: np.array(x_data).reshape(-1, 1),                         y: np.array(y_data).reshape(-1, 1)})found_angle = sess.run(angle_constant.value())print(found_angle)TensorFlow finding the angle of myÂ shotAt the end of that optimisation we find out that the trajectory function that best fits my shot data has an angle of ~61Â°â€¦ not sure what to do with that informationâ€¦ I guess I could look at what professional shooting angles are for comparisonâ€¦ to be continued.The lesson to take away: you can always distract your-self with completely unnecessary (but fun) machine learning.All of the code I used is available on myÂ Github:ZackAkil/optimising-basketballTensorFlow &amp; reflective tapeÂ : why Iâ€™m bad at basketball ğŸ€ was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Finally learn how to use command line appsâ€¦ by making one!'
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Mon, 21 May 2018 21:07:45 GMT'
* Categories: beginner, command-line, linux, bash, python

At the risk of alienating a lot of readersâ€¦ I grew up with GUIâ€™s, so I never needed to learn the ways of the terminal! This is socially acceptable in todays society of friendly user interfaces, except maybe if youâ€™re in software engineeringâ€¦ whoops!Managerâ€Šâ€”â€Šâ€œOh this is easy, just use this command line appâ€, Meâ€Šâ€”â€Šâ€Yesâ€¦the command lineâ€¦ Iâ€™ll useÂ thatâ€¦â€Getting back my software engineering streetÂ credâ€™Iâ€™ve gotten pretty far just through copy and pasting full command-line operations from Stack-Overflow but have never become comfortable enough to use a command line app â€œproperlyâ€. What finally caused it to click for me was when I stumbled across a very handy python library called argparse that allows you to build a nice robust command line interface for your pythonÂ scripts.This tutorial is great in-depth explanation about how to use argparse, but Iâ€™ll go over the key eyeÂ openers:argparse is part of the standard python library so pop open your code editor and follow along (you donâ€™t need to install anything)!HELPThe most useful part of every command lineÂ app!# inside a file called my_app.pyimport argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.parse_args()The code above will do nothing, except by default you have the helpÂ flag!In the command line you canÂ run:python my_app.py --helporpython my_app.py -hand youâ€™ll get an output likeÂ this:usage: my_app.py [-h]Nice little CL app!optional arguments:-h, --help  show this help message and exitSeems pretty cool right? but wait, when you use argparse to add more functions (see below) this help output will automatically fill up with all of the instructions of how you can use yourÂ app!REQUIRED ARGUMENTSLets say you want to have your app take in some variable, we just use the parser.add_argument() function and give our argument some label (in this caseÂ â€œnameâ€):import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")args = parser.parse_args()print("Your name is what? " + args.name)Notice how we added the help text! Now when we run python my_app.y -h we get all of the appÂ details:usage: my_app.py [-h] nameNice little CL app!positional arguments:name        Just your name, nothing specialoptional arguments:-h, --help  show this help message and exitPretty cool, but letâ€™s run our app with python my_app.pyusage: my_app.py [-h] namemy_app.py: error: too few argumentsThatâ€™s right! Automatic input checking!Now lets run python my_app.py "SlimÂ Shady"Your name is what? Slim ShadyPretty slick!OPTIONAL ARGUMENTSMaybe you want to give someone the option of telling you little more about themselves? Adding a double dash when using parser.add_argument() function will make that argument optional!import argparseparser = argparse.ArgumentParser(description=â€Nice little CL app!â€)parser.add_argument(â€œnameâ€, help=â€Just your name, nothing specialâ€)parser.add_argument("--professionâ€, help=â€Your nobel professionâ€)args = parser.parse_args()print(â€œYour name is what? â€œ + args.name)if args.profession:    print(â€œWhat is your profession!? a â€œ + args.profession)If you want to pass in a variable for that argument you just need to specify the double-dashed argument name before the variable you want toÂ pass:python my_app.py "Slim Shady" --profession "gift wrapper"which gives youÂ :Your name is what? Slim ShadyWhat is your profession!? a gift wrapperOr you donâ€™t have to, it is optional afterÂ all!python my_app.py "Slim Shady"still givesÂ you:Your name is what? Slim Shadyand the magic once again when running python my_app.py -hÂ :usage: my_app.py [-h] [--profession PROFESSION] nameNice little CL app!positional arguments:name                      Just your name, nothing specialoptional arguments:-h, --help                show this help message and exit--profession PROFESSION   Your nobel professionFLAGSMaybe you just want to enable something cool to happen. Add action="store_true" to your parser.add_argument() function and you have yourself a flag argumentÂ :import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")parser.add_argument("--profession", help="Your nobel profession")parser.add_argument("--cool", action="store_true", help="Add a little cool")args = parser.parse_args()print("Your name is what? " + args.name)cool_addition = " and dragon tamer" if args.cool else ""if args.profession:    print("What is your profession!? a " + args.profession + cool_addition)Itâ€™s pretty neat, you just plop the flag name into your command likeÂ so:python my_app.py "Slim Shady" --profession "gift wrapper" --cooland presto!Your name is what? Slim ShadyWhat is your profession!? a gift wrapper and dragon tamerand remember, you donâ€™t have to use it, itâ€™s just aÂ flag:python my_app.py "Slim Shady" --profession "gift wrapper"will stillÂ give:Your name is what? Slim ShadyWhat is your profession!? a gift wrapperlook though at the help command python my_app.py -hÂ :usage: my_app.py [-h] [--profession PROFESSION] [--cool] nameNice little CL app!positional arguments:name                      Just your name, nothing specialoptional arguments:-h, --help                show this help message and exit--profession PROFESSION   Your nobel profession--cool                    Add a little coolIâ€™m just going to assume youâ€™re as satisfied with that as I am from nowÂ on.SHORT FORMUnveiling the mysterious one character arguments that confused me for so long. Just by adding a single letter prefixed with a single dash to your parser.add_argument() functions you have super short versions of the same arguments:import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")parser.add_argument("-p", "--profession", help="Your nobel profession")parser.add_argument("-c", "--cool", action="store_true", help="Add a little cool")args = parser.parse_args()print("Your name is what? " + args.name)cool_addition = " and dragon tamer" if args.cool else ""if args.profession:    print("What is your profession!? a " + args.profession + cool_addition)So that instead of typing inÂ :python my_app.py "Slim Shady" --profession "gift wrapper" --coolyou can justÂ type:python my_app.py "Slim Shady" -p "gift wrapper" -cand youâ€™ll get the sameÂ output:Your name is what? Slim ShadyWhat is your profession!? a gift wrapper and dragon tamerAnd this is reflected in the help text (python my_app.py -hÂ ):usage: my_app.py [-h] [--profession PROFESSION] [--cool] nameNice little CL app!positional arguments:name                           Just your name, nothing specialoptional arguments:-h, --help                      show this help message and exit-p PROFESSION, --profession PROFESSION   Your nobel profession-c, --cool                      Add a little coola perfect little command lineÂ app!You now have a perfect little command line app and are hopefully more comfortable with finding your way around command lineÂ apps!Just remember --helpÂ !ZackAkil/super-simple-command-line-appFinally learn how to use command line appsâ€¦ by making one! was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.
