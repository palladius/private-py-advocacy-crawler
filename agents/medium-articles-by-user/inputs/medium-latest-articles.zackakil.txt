
== Article 1
* Title: 'Hobbies to Hired: 5 tips for making great Portfolio Projects to Land Your First Tech Job'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/hobbies-to-hired-5-tips-for-making-great-portfolio-projects-to-land-your-first-tech-job-8c45aaeafd06?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Sun, 14 Apr 2024 00:07:10 GMT'
* Categories: portfolio, ai, junior-developer, tech, jobs

When you starting off in tech, building strong portfolio projects is an essential step in showcasing your skills and impressing potential employers. However, figuring out what projects to create and how to make them shine can feel overwhelming. This guide will break down 5 actionable tips to help you start building projects that you’ll be excited to show off!1. Go “off trail”Especially for those coming from bootcamps, just having projects that were “part of the curriculum” in your portfolio shows to the interviewer that you can follow instructions in a safe environment. However in the wild those conditions are rare. You need projects that show you ability to (independently) IDENTIFY a problem, then (independently) LEARN something new that can be used to solve that problem, and of course successfully (independently) APPLY that learning to solve the problem.When telling the story of these “off trail projects” a good structure is :I saw this problem.I learnt about this technology that could be used to solve the problem.I built this thing using what I learnt to solve the problem.When hiring juniors employers main concern is that they will have to provide extensive hand-holding. These kind of projects directly address that concern by showing that you can take ownership of your learning rather than always having to rely on explicit instructions.So back yourself and go “off trail”!Key tip about your first “off trail projects”One new thing at a time: It’s good practice focus on one new skill at a time with each new project. This helps keep the complexity manageable, allowing you to apply your existing knowledge while stretching your abilities with your new learnings. E.g I might want to learn about Tensorflow.js and also d3js, and I already know how to make HTML webpages. So I’ll make a project were I apply my existing HTML knowledge to my new Tensorflow.js knowledge in some app. Then I’ll do a separate project were I build something with d3js in a HTML app. Then finally another separate project that combines everything! That’s 3 unique projects that can go my portfolio!2. The Enthusiasm hackPeople want to work with enthusiastic people that do exciting work. As a junior developer, enthusiasm should be one of your strengths. But, how do you get interviewers enthusiastic about your portfolio projects?First off, YOU have to be genuinely enthusiastic by your portfolio projects!Do that by working on projects that genuinely excite you. Classic patterns for identifying projects that elicit genuine enthusiasm are:🤩 Projects based on your hobbies.🤩 Project that solve a real problem that you genuinely face.🤩 Projects that solve a real problem for people you genuinely care about.Projects that are harder to elicit genuine enthusiasm for are:🥱 Any project you “had to do” / “were told to do” e.g assigned coursework.🥱 Projects that solve hypothetical problems for hypothetical people. (due to their lack of an authentic story)Come interview time you’ll find it a lot easier to speak enthusiastically about your work, and that genuine enthusiasm is infectious. So lean into, let yourself geek out and exude that energy that embodies the excitement of being a junior developer!An example of a portfolio projects of mine that I was able to easy get enthusiastic about was my rugby recording robot. Without any more context, you can probably guess what sport I like.Additional perks of working on intrinsically interesting projectsIt will help keep your engaged in the project when things get challenging.You are are going to be more inclined to talk about the project with those around you. This gives you free practice in your technical communication skills.Often times they will be projects that are naturally unique to you. Great portfolio projects are the one’s only you could have made. Their uniqueness embody a part of your personality e.g an app that was inspired by your hobby and the tech skills you want to become proficient in.3. “Many and Smaller” vs “Fewer and Bigger”You probably have an idea for an app in your head (or are currently building one) that is a bit of a beast e.g multi-social user login, full admin CRUD, email notifications, payment system, plus whatever the USP is etc.As a portfolio project these kind of projects have merit in giving you holistic experience in every part of application development. However, with one big project you get to setup your repo… once, your get you design your UX… once, you get to identify a problem spec… once. Vs with many smaller projects were you get to go through the whole Problem -&gt; Idea -&gt; Design -&gt; Implementation process multiple times starting from a blank slate.This is the same principle as the “Photography Quantity vs Quality” story. Give yourself many opportunities to try things, learn from them, and take those learnings to the next project, and repeat as many times as possible.So the algorithm to a fruitful portfolio journey is :knowledge = Nonewhile(1):   learnings = build_project(size="small", applied_knowledge=knowledge)  knowledge += learnings4. Make it instantly interactive 🪄Having interviewed many devs straight out of bootcamps, the first thing I check is their github and go to their latest (or pinned) project. For web devs it’s usually a website, cool… but where is it? Oh I have to first clone it then do “npm run” yadayadayada “localhost”… sigh!Don’t make interviewers work to see your work. If you’ve made a website, have it deployed somewhere (e.g Github pages) and put the link at the top of the projects’ README. Did you make some cool python stuff? have a link to a Colab or Sreamlit app and put the link at the top of the projects’ README.If there is literally no way to make your work instantly interactive then at least have some kind of videos or gifs embedded in the README. Have some way to instantly show your work.5. Put a bow on it 🎀You’ve probably put days/weeks of time into your project. However you’ve probably put zero time into the first thing someone will see when looking at your project… the README!Take 15 minutes to really spruce up your projects README with nice formatting, links, images, gifs etc. The README is your projects (and likely you as a developers’) first impression, so make it one that shows you care about communication and presentation of your work. Check out this list by @matiassingers for inspiration : awesome READMEsBonus tip (especially when applying to larger companies like Google)At Google there is a specific emphasis put on knowledge sharing. Showcasing that you not only add value as an individual, but also elevate those around you.Showcasing your enthusiasm for sharing knowledge by creating resources that help others. Whether it’s visualization tools or blog posts, sharing your insights and resources can make a powerful impression.ConclusionSo, there you have it: five (+1) actionable tips to for building great portfolio projects. By venturing “off trail,” embracing your enthusiasm, building many smaller projects, making your work instantly interactive, and putting a bow on it with a polished README, you’ll create a portfolio that not only showcases your skills but also tells a compelling story about you as a developer.Remember the tech world is constantly evolving, and the best developers are those who never stop learning. Use your portfolio as a tool for growth, pushing your boundaries and showcasing your ability to adapt and master new skills. Good luck on your journey!

== Article 2
* Title: 'How to Identify Innovative Ideas with the Technical Horizon Framework'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/how-to-identify-innovative-ideas-with-the-technical-horizon-framework-f7c9c613682e?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Sat, 12 Aug 2023 15:52:26 GMT'
* Categories: technology, innovation, ideas, management, ai

“It’s been done before!” or “It’s not possible!”, the last thing you want to hear when you pitch your new idea. What if you could quickly and pragmatically identify better, more innovative ideas that are actually feasible? Enter the “Technical Horizon Framework”.What are Technical Horizons?As technology advances with time, what is technically feasible to consumers, developers, and researchers is constantly expanding, these are the Technical Horizons. The key points about them are:Technical Horizons are always expanding as technology advances.Ideas are static and are defined by the simplest technology required to build them.Every idea within your Technical Horizon is technically feasible.Consumers can build ideas within the Consumer Technical Horizon, developers can build ideas within the Developer and Consumer Technical Horizon, and researchers can build ideas within the Researcher, Developer and Consumer Technical Horizon.The Technical Horizon chart above shows:Plotting data (e.g charts in spreadsheets) is easy for consumers i.e very feasible.Image Generation is newly feasible to consumers (as of 2023) through consumer apps.Video classification is very feasible to developers with tools like Googles Video Intelligence API, but would be infeasible for consumers without developer knowledge.Video summarisation is a newly feasible idea for developers by combining multiple APIs like the previously mentioned Video Intelligence API and Googles PaLM API.Video clip generation is currently not feasible without research level knowledge however in the near future APIs to do this may be available.Full movie generation would only be feasible with a lot of research time.Where are your innovative ideas?You can use the Technical Horizon Framework to identify which of your ideas are “innovative”. Simply by looking to the immediate inside edge of your Technical Horizon. If you are a consumer, Image Generation is an innovative idea that is good for consumers to explore.As a developer; Video summarisation is currently an innovative space. The reason the space on the immediate inside of your Technical Horizons is innovative, is because in this space there are ideas that have never been done before; because they could never have been done before due to the enabling technology only recently becoming available.What about ideas outside of your Technical Horizon?Almost feasible ideasAssume your a developer and you’ve come up with an idea were the APIs aren’t quite there yet (e.g Video clip generation), it’s just outside your Technical Horizon, or “almost feasible”. You have a few options:Park the idea and wait for the tools to become available, and spend your time on more feasible ideas.Or, embark on building the idea anyway, but risk wasting your developer resources as at any time an API could be released that nudges the Developer Technical Horizon forward, potentially making the idea buildable with a few lines of code, and rendering your work potentially redundant. What’s especially dangerous in this scenario is you (or your team) are at risk of “sunk-cost fallacy” if an API does come out after you’ve already invested time into your own implementation when using someone else's solution might be better.Technical Horizons are invisibleThe challenge here is the unknown. When will the developer tools be ready? A day, month, year? This is were having good channels for up to date news about new tools is valuable. Also being a part of early access programs and going to conferences lets you peek into the future of where your Technical Horizon is going.Very infeasible ideas (Moonshots)Keep track of them, they might be infeasible now, but Technical Horizons can leap forward suddenly, like it has with the surge of LLMs (Large Language Model) technology.If waiting isn’t an option, a word for these high risk - high reward ideas is a “Moonshot”. Accept that it’s a bold undertaking so the pay off needs to be worth the risk.Watch out for “Shoehorn Innovation”“My idea is innovative because it’s using the latest technology!”, acliché when any new technology is released.The “innovativeness” of an idea is not defined by the technology it uses, but by the actual technical requirements of the problem it’s solving. An innovative idea is solving a problem that can only be solved with new technology, so ask the question “Can this problem already be solved with an existing technology?”, a hard but important pill to swallow sometimes.Wrap upWhen you find an idea that is genuinely newly feasible with a new technology, and it’s solving a problem that could never have been solve before, then you know you have an innovative idea.Key takeawaysA truly innovative idea is one that solves a problem that could not previously be solved.Listen out for news about new technology that could push your Technical Horizon forward. “What can I do now do that I couldn’t before”, is a good question to be asking about any new technology you hear about.Beware of “Shoehorn Innovation.” Just because a new technology is available doesn’t mean that you need to use it in your idea. Don’t just use new technology for the sake of using it.

== Article 3
* Title: 'How I conquered writer's block with Google Assistant + Google Docs'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Fri, 30 Jul 2021 15:51:57 GMT'
* Categories: writing, google-docs, google-cloud-platform, productivity, google-assistant

Over lockdown I built a voice app that would interview me about my projects and help write articles for me. What would usually take me weeks, now takes me an hour or two, and as a testament to that I wrote one article a day for a week and lived to tell the tale:Day 1See what Video Intelligence API can do with this visualisation toolDay 2🎥 🙈 Using AI to automate phobia safe film productionDay 3🎥 🏉 AppSheet + AppScript for sports videography highlights trackingDay 4🚴 Safer cycling with ML and Edge TPU’sDay 5 (this article 🤯)How I conquered writer's block with Google Assistant + Google DocsBackgroundWriting is something I’ve always struggled with, and in my role as a Developer Advocate it can be an important part of the job to make and share content about what I’ve built in order to inspire and educate fellow developers. It wouldn’t be uncommon for me to take longer writing an article than it did for me to build the thing I was writing about.One time I had built something and someone reached out to interview me about my project so that they could write an article on my behalf. Obviously I found that experience a lot easier. So that gave me an idea; what if I could build some voice AI that would interview me about my projects and generate the articles for me.SolutionUsing Google Action Builder I built a Google Assistant app that would ask me a set of questions about a single project and then output what I said into a generated Google Doc.I followed this tutorial to learn how to build an action:https://medium.com/media/d61d324ae46d344aa227912458dc9e14/hrefWithin the Assistant app I have a custom webhook that is a Google Cloud Function that takes what was said and writes it into a new Google Doc using the Google Docs python client API. See the code for that Cloud Function on my github:GitHub - ZackAkil/project-reporter-voice-app: A voice app that interview me about my projects in order to generate articles.The first step is always the hardest so even though it’s essentially just a dictation app with some prompting questions, it makes that initial draft phrase of article writing virtually zero friction for me. I never have to see that horrible immovable empty document again when writing articles. Here it is in action:https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/hrefTricky challengesAuthentication! Authentication! Authentication!Getting Google Assistant to talk directly to Google Docs is especially tricky as you have to navigate a merade of authentication services in order to create and write docs on the users’ behalf. I created this slide deck to loosely document the process in case you’d like to do something similar, but be warned it is a lot of throwing around of api keys, client secrets, and refresh tokens. But it can be done!Speak quick and concise (look to using Dialogflow)Google Assistant apps have a 30 second response limit so I have to speak quite quickly, and as soon as there is a pause in my voice it cuts the mic and takes that as my answer. For my first version of this app I’m OK with that, however for a more natural interview experience where you can take your time and collect your thoughts before you answer I’d look into using Dialogflow which allows for more granulare configuration and longer talking response times. Check out Lee Boonstra’s I/O talk to learn more about Dialogflow:https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/hrefRemembering the Doc IDBecause I was using a Cloud Function as my webhook, there was no guarantee that the exact same instance of that Cloud Function would be called during the same conversation (one of the tricky things with serverless tech). So I used Cloud Firestore as a storage medium to store which conversation IDs (which are passed to every webhook call) map to which generated Google Doc IDs.FutureAs an extra step in the article writing process it would be cool to have some machine learning text generation automatically generate tweets based on what I said about my project.Also the interview process is currently just a static set list of questions. I could use some natural language ML tools like the Natural Language API or AutoML Natural Language to do some intelligent analysis into my responses in order to adapt the follow up questions like in a real interview.TakeawaysLeverage your strengths (for me it’s building things) to overcome your weaknesses (for me it’s writing things).You can get Google Assistant to talk directly to Google Docs (and the rest of G Suite), it’s just a fair bit of authentication loops you have to jump through.It can be a lot easier to say something than to write something so be aware of voice technologies like Google Assistant and Dialogflow.Follow and ask me questions on twitter ZackAkil!

== Article 4
* Title: ' Safer cycling with ML and Edge TPU’s'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Thu, 29 Jul 2021 13:33:19 GMT'
* Categories: raspberry-pi, machine-learning, cycling, google, edge-computing

A while ago I tried to make my bike smarter with machine learning to help keep me safe whilst commuting through busy London streets.BackgroundIf you live in a big city and cycle you probably have developed superhuman reflexes and awareness, especially in cities like London where a lot of the narrow roads were originally designed for horse and cart.After enough close calls and some actual calls I figured it was time to try and solve this problem with a modern approach (without mirrors).SolutionUsing a small computer (Raspberry Pi) attached to the bike. It would run an object detection model on images being taken from a rear facing camera. The system would detect the different potential dangers behind and intuitively relay that information back to the rider (via a handlebar mounted LED strip) so that they can maintain their focus on what's ahead.what an object detection model would detectNeed for speedWith just the Raspberry Pi running the object detection model it was able to make predictions at a rate of about 2 predictions per second, which for this use case is on the slow side. That is where the Coral Edge TPU comes in. It is able to run the same model at 30–60 predictions per second giving the system the super low latency it needs. You can find out more about Coral Edge TPU’s from this Google I/O session:https://medium.com/media/7400903711405d60c51ba4a89e582f1c/hrefSpecifically I used the Edge TPU USB Accelerator and had it and the Raspberry Pi both attached to the bike:prototype showcasing the Coral Edge TPU usb acceleratorAfter the Raspberry Pi + Edge TPU made it’s prediction of what's behind the bike, it would then send commands to an RGB LED strip on the bikes’ handlebars to then indicate to the rider where the dangers from behind are, and what level of danger those things are (because it’s able to distinguish between the different types of vehicle e.g car, bus, truck, bike, pedestrian).prototype and real bike versionThe specific modelDue to this being a pretty generic prediction task (traffic object detection), there were already pretrained models I could download and use that already detected different types of vehicles. In fact I ended up using the exact same model that Bill used in his 2019 Google I/O demo:https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/hrefYou can explore different kinds of pretrained models on the Coral website:Models | CoralYou can also find all the code for this project on my github:GitHub - ZackAkil/edge-TPU-safe-bike: An application of realtime object-detection running on an Edge TPU for making cycling in busy cities a little less terrifying.Interesting pointsAustralia has it own dangersWhen I presented this idea in Sydney there were discussions around if a specific version of this could be made for their specific dangers. At the time I assumed they might have been talking about some cliche dangers when you think of Australia… kangaroos, snakes, drop bears. But actually for cyclists in Australia it’s teritorial magpies that are constantly attacking. So a custom version that included swooping birds in the model would be perfect for them.Pretrained models not designed for toy carsIt’s important to test exactly how you’re going to use pretrained models. When I was demoing this idea on stage I hadn’t thoroughly tested how well it worked with toy cars and I got suboptimal results. This is where training custom models with tools like AutoML Object Detection would shine. Also interesting to note that my toy version of this demo worked a lot better when I included a printed out fake road for the toy cars to sit on, see in the previous gif above 👆.https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/hrefFutureI want to train a custom model that would be trained specifically on London traffic (including the distinctive taxis) and then I could look into training other custom models for different regions such as an Australian magpie edition. Oh, and actually make this on a real bike that I can cycle (not just a prototype).TakeawayIt’s good to be aware of technology like Edge TPU’s in case you have a problem that requires super low latency and mobility. Also always checking to see if there are pretrained models that could already solve your problem and save you a huge amount of development time.Follow and ask me questions on twitter ZackAkil!

== Article 5
* Title: '  AppSheet + AppScript for sports videography highlights tracking'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Wed, 28 Jul 2021 14:59:12 GMT'
* Categories: productivity, appsheet, app-script, sports, google-cloud-platform

During the summer I was asked to help with some video recording for a rugby tournament, but rather than spending hours rewatching footage to find highlights, I quickly threw together some tech to make things a lot easier.BackgroundSport videography is a hobby of mine (specifically rugby). It is however very tedious work rewatching full matches in order to find the highlights. That's why when I was asked to help a friend film an entire rugby tournament this summer I was hesitant. Rewatching a full day’s worth of rugby videos is not something I wanted either of us to have to do.There are some cameras that have a “mark special moment” button that will let you set a marker of when something cool happens, but we were using multiple cameras that didn’t have that feature. We discussed a simple paper based solution where we just noted down the time of when cool things happened. This was the solution we settled on until the morning of the event which is when I discovered a technology that would make things a lot easier.Solutionoriginal spreadsheetFor the data capture/entry we used AppSheet, which is a no code app builder that integrates seamlessly into Google Sheets (as well as other things). As long as you have your spreadsheet set up with the right data formats for your columns, AppSheet can generate a clean and robust data entry UI that feeds data directly back into the spreadsheet.I found out about AppSheet from this video by Carter Morgan 👇:https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/hrefThanks to Carter’s intro I was able to build the perfect data entry app on the same morning of the tournament that would fit my needs (and was even able to style the colour theme of it), and both my friend and I were able to access it from a native Android App on the field!generated AppSheet appThroughout the day, anytime something cool happened whilst the cameras were rolling I just put in a note using the app, and because of how I set it up; the app knew to use the current time as the default value.After the tournament was overWhen it came to finding the actual clips that contained the highlights after the tournament, this is where AppScript came into play (I know AppScript and AppSheet are annoying similar names). AppScript is a technology that lets you build automation within GSuit e.g Google Drive, Sheets, Docs, etc. Check out this video for a quick overview 👇https://medium.com/media/72a41db491bd122711fb457c959c30e7/hrefWe first uploaded all of the video clips from the tournament into Google Drive. Then within the Google Sheet that contained all of our inputted highlight timestamps I wrote a AppScript that would loop through each row and:extract out the time data for that specific highlightscan through the file metadata of all the videos in a specified Google Drive folder to find the video that overlapped with that timeautomatically paste in a link to that video file back into the spreadsheetThe AppScript that does this is attached to the Google Sheet so that I get a nice toolbar button to trigger the script. When the script is triggered it prompts me to specify the Google Drive folder url containing the tournament videos to search through. Watch it in action 👇the automated clip finding process using my scriptYou can see all the AppScript code in my github:gsuit-sports-videography-tool/script.gs at main · ZackAkil/gsuit-sports-videography-tooland here’s a visual overview of the whole system:high level diagram of the entire solutionTricky challengesCamera clocksWhen I first ran the script to find the matching clips they were all wrong. Turns out the system clock of the camera was wrong 🤦‍♂️. So I added a quick and dirty bit of code to the script to offset the times.‘File created’ time vs ‘File updated’ timeWhen I was writing the code to fetch the video metadata from Google Drive I thought that the file.getDateCreated() method from the AppScript Google Drive API would tell me when the video was captured. Turns out that method just returns when the file was created/uploaded “in Google Drive”. This had me panicked for a while thinking the project was dead as I could not extract the lower level file metadata, but turns out file.getLastUpdated() will tell you the last time the file was edited which in our case was the time the video was captured on the camera.FutureNow that we have timestamps of highlights along with the specific video clips we can look into training machine learning models that could automatically detect highlights in videos. This could be done with a tool like AutoML Video Intelligence which can train powerful video models without us writing any machine learning code.TakeawayThe time it takes to build an app is not correlated to how useful that app can be. Tools like AppSheet take minutes to use and when the right moment comes for them they can save you literally days of work. Combine that with scripting tools like AppScript and you have a powerful problem solving duo.Follow and ask me questions on twitter ZackAkil!

== Article 6
* Title: '  Using AI to automate phobia safe film production'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Tue, 27 Jul 2021 12:58:37 GMT'
* Categories: machine-learning, machine-vision, google-cloud-platform, video-production, film

🎥🙈 Using AI to automate phobia safe film productionAt Google I/O this year I used ML to build an automated video processing pipeline that detects what’s in a video and automatically hides the parts you would find scary.https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/hrefBackgroundWhilst watching some movies with some friends, we discovered that one of our friends has a deathly fear snakes and so we had a tell them exactly when the snakes were going to appear , and sometimes we got this wrong 😬 (watching Harry Potter was a poor choice in hindsight).SolutionI built an automated video processing pipeline using Video Intelligence API that would scan through a video to detect all the things within the video. Then if it detects something that matches with the phobia that we told the system about, it would use the Transcoder API to insert an overlay and hide it from view. So the solution was based on these two APIs:Video Intelligence APIhttps://zackakil.github.io/video-intelligence-api-visualiser/A powerful video analyse API on Google Cloud Platform. It uses machine learning to detect what is in a video e.g faces, people, objects, text etc.If you want to see all that it can do you can check out this interactive demo. Also check out the docs here.Transcoder APIA scalable cloud based video transcoding API on Google Cloud Platform that lets you perform complex video transcoding jobs. Some of its’ features include :generating different bit rates and formatsgenerating thumbnailsinserting overlaysinserting ad breakssee more details on the API in the docs.Connecting these togetherThe system pipeline uses the Label Detection feature of the Video Intelligence API (see visualisation of this feature here) to detect what’s in each scene of the video and return a list of labels with time segments. If any of those labels match the phobia we are scared of (e.g snakes, swans, bread, birds) we then use the Transcoder API to inject a full screen overlay on the video for those time segments, hiding them from view.high level API pipelineTo get these API’s to run in a scalable way I used a chain of Cloud Storage Buckets and triggered Cloud Functions that automatically run when a new video is uploaded.exact tech stack including storage and triggersTricky challengeIn order to tell the system what my phobia/phobias were without some fancy UI I came up with the idea of just using the ‘see no evil’ emoji 🙈 in the uploaded file name followed by the phobia. Then get the cloud function code to read the filename and extract the phobia from that. See the specific code in the github repo.OutputFinally I would just drag and drop a scary video into the input storage bucket with a filename like test_nature🙈swan.mp4 …original scary video (containing swans)… and in less time that it would take to watch the video the Video Intelligence API has already scanned the video and the Transcoder API has already generated the new non-scary video. Check out the I/O video ☝️ that demo segment was all realtime!generated phobia safe video (hiding swans)Future workIn the future I want to use the same kind of pipeline to solve video processing problems surrounding sports analysis and sports video production e.g highlights generation. You can see the kind of things I do with sport in this episode of Dale Markowitz’s Making with ML series:https://medium.com/media/25c04956d43e7829875470e071585770/hrefTakeawaysBuilding ML powered video production pipeline need only take you plugging together a couple of API’s.GitHub - ZackAkil/phobia-safe-videos-with-ml: How you can use ML to make watching videos safer for people with unique and/or serious phobias.Follow and ask me questions on twitter ZackAkil!

== Article 7
* Title: 'See what Video Intelligence API can do with this visualisation tool'
* Author: 'Zack Akil'
* URL: 'https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Mon, 26 Jul 2021 12:53:43 GMT'
* Categories: google-cloud-platform, google-cloud, machine-vision, machine-learning, python

I built a visualiser for the Google Cloud Video intelligence API that allows anybody to explore all of the features of the API.https://zackakil.github.io/video-intelligence-api-visualiser/If you’re working with video, then the Video Intelligence API has a lot of powerful features for analysing and detecting what’s in your videos, for small pet projects or massive scale applications.Here is a list of the features of the API along with some example use cases for those features (see the docs for more info):Label Detection (phobia detection)Shot Detection (detect ad break segments)Object Tracking (traffic counting)Person Detection (sports player analysis)Face Detection (analyse eye contact with camera)Logo Recognition (product placement detection)Speech Transcription (generate interactive subtitles)Text Detection (search videos of presentations based on text in slides)Explicit Content Detection (automated content monitoring)Celebrity Detection**restricted access and not shown in visualiserAnalyse and visualise your own videosYou can follow this tutorial in order to analyse your own videos. You can also use this script I made to quickly analyse videos stored in Google Cloud Storage:video-intelligence-util/video_intel_util.py at main · ZackAkil/video-intelligence-utilAfter you have analysed your video you can drag the output .json along with the original video into the visualiser to visualise the results.Hope this tools helps you explore and work with the Video Intelligence API! Please reach out to me with any questions,Happy video analysing!JavaScript is not available.

== Article 8
* Title: 'Zombies & Model Rot (with ML Engine + DataStore)'
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Wed, 24 Oct 2018 14:48:41 GMT'
* Categories: machine-learning, cloud-computing, zombies, big-data, google-cloud-platform

Don’t leave your models to rot into obscuritySo you’ve deployed your machine learning model to the cloud and all of your apps and services are able to fetch predictions from it, nice! You can leave that model alone to do its thing forever… maybe not. Most machine learning models are modeling something about this world, and this world is constantly changing. Either change with it, or be left behind!What is model rot?Model rot, data rot, AI rot, whatever you want to call it, it’s not good! Let’s say we’ve built a model that predicts if a zombie is friendly or not. We deploy it to the cloud and now apps all over the world are using it to help the general public know which zombies they can befriend without getting bitten. Amazing, people seem super happy with your model, but after a couple of months you start getting angry emails from people who say that your model is terrible! Turns out that the zombie population mutated! Now your model is out of date, or rotten! You need to update your model, and even better, add in a way to keep track of your models state of rot so that this doesn’t happen again.This is an example of a very sudden case of model rot!It’s not just zombies that changeSure, fictional creatures can change, but so can financial markets, residential environments, traffic patterns, weather patterns, the way people write tweets, the way cats look! Ok maybe cats will always look like cats (although give it a few million years and maybe not). The point is that depending on what your models are predicting will effect how fast they are going to rot.It’s also important to note that the thing you are predicting doesn’t need to change for your model to rot. Maybe the sensor you are using to capture input data gets changed. Anything that negatively effects the performance of your model when it’s deployed is effectually causing model rot, either remove the thing causing the reduced performance or update the model (most likely going to be the latter choice).Let’s fight model rot (with ML Engine + DataStore)There’s a zombie outbreak, however it’s not as scary as the movies would lead you to believe. They are pretty slow moving creatures, and a lot of them are just looking for human friends, but some aren’t. To help people make the right choice of zombie friends we developed a model that predicts if a zombie is friendly or not based on a few characteristics:We used Scikit-Learn to build a Decision Tree Classifier model. See the notebook here to see the exact code to do this.The plan is now to deploy our model to ML Engine which will host our model for us on the cloud. (see how we do this using gcloud commands in a notebook)First we’ll throw our model into cloud storage:gsutil cp model.joblib gs://your-storage-bucket/v1/model.joblibThen create a new ML Engine model, which you can do using the Google Cloud Console UI or using gcloud commands (which you can see here used in a notebook):ML Engine UIThen we deploy our Decision Tree model as version 1:Creating a new version of your model using the Cloud Console UITo make it easier for apps to fetch predictions from our model, we’ll create a public endpoint using Cloud Functions. You can read more about how to do this in this blog post I wrote with my colleague Sara Robinson.here’s the current architecture of our systemOK, our model is deployed and apps can easily fetch predictions from it! Now to monitor for model rot with DataStore!DataStore?Imagine a place in the cloud where you can store millions of python dictionaries quickly, and then query them (also quickly). That’s DataStore, a fully managed NoSQL database on Google Cloud Platform. If you have previous experience with databases you might be used to carefully planning out exactly what structure of data to store in tables, then experience the pain of creating migration scripts to update the structure of your database. Non of that nonsense with DataStore, want to store the following data:{  "name": "Alex"  "occupation": "Zombr trainer"}then do it (using the python client library):# create new entity/rownew_person = datastore.Entity(key=client.key('person'))new_person['name'] = 'Alex'new_person['occupation'] = 'Zombr trainer'# save to datastoreclient.put(new_person)Oh wait, you want to start storing peoples githubs and twitters? go for it:# create new entity/rownew_person = datastore.Entity(key=client.key('person'))new_person['name'] = 'Zack'new_person['occupation'] = 'Zombr CEO'new_person['github'] = 'https://github.com/zackakil'new_person['twitter'] = '@zackakil'# save to datastoreclient.put(new_person)and DataStore will say “thank you”:DataStore’s UIUsing DataStore to collect model feedbackThe feedback data that we are going to collect will look like the following:{"model": "v1","prediction input": [2.1, 1.4, 5.3, 8.0],"prediction output": 1,"was correct": False,"time": "23-10-2018,14:45:23"}This data will tell us what version of our model on ML Engine was used to generate the prediction (model), what the input data for the prediction was (prediction input), what the prediction made by the model was (prediction output), if the prediction was correct (the actual feedback from the user) (was correct), and the time that the feedback was submitted (time).We’ll use Cloud Functions again to make another web API endpoint, this time to receive the feedback data and store it in DataStore:https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/hrefNow our system architecture looks like the following:the new architecture of our systemThe client apps just need to add in a intuitive way for the users to submit their feedback. In our case it could be a simple ‘thumbs up or thumbs down’ prompt after the user is presented with a prediction:You may have come across feedback prompts like this beforeGet creative with how you collect feedbackOften times you can infer feedback about your model, rather than explicitly requesting it from the users like I’ve done in the Zombr interface. For example if we see that a user stops using the app immediately after a prediction, we could use that data to indicate a wrong prediction 😬.Back in reality, a dog adoption agency might have a recommender system for new owners. The rate of successful adoptions made by the model is it’s own performance feedback. If the agency suddenly sees that the system is making a lot fewer successful matches than usual, then they can use that as the indication that the model is rotten and may need updating.Feedback is collected, now what?Now we can analyse the feedback data. For any data analyse work I default to using Jupyter Notebooks.Click here for the full notebook of how I fetch data from DataStore and analyse the feedback.The important bits of fetching data from DataStore are, first installing the DataStore python client library:pip install google-cloud-datastorethen you can import it and connect to DataStore:from google.cloud import datastore# connect to DataStoreclient = datastore.Client('your project id')# query for all prediction-feedback items query = client.query(kind='prediction-feedback')  # order by the time field query.order = ['time']  # fetch the items # (returns an iterator so we will empty it into a list) data = list(query.fetch())The library with automatically convert all of the data into python dictionaries:print(data[0]['was correct'])print(data[0]['model'])print(data[0]['time'])print(data[0]['input data'])&gt;&gt;&gt; True&gt;&gt;&gt; v1&gt;&gt;&gt; 2018-10-22 14:21:02.199917+00:00&gt;&gt;&gt; [-0.8300105114555543, 0.3990742221560673, 1.9084475908892906, 0.3804372006233603]Thanks to us saving a “was correct” boolean in our feedback data we can easily calculate the accuracy of our model from the feedback by looking at the ratio of ‘Trues’ for this field:number_of_items = len(data)number_of_was_correct = len([d for d in data if d['was correct']])print(number_of_was_correct / number_of_items)&gt;&gt;&gt; 0.840.84 is not much rot since we first trained our model which scored ~0.9 accuracy, but that’s calculated using all of the feedback data together. What if we do this same accuracy calculation on a sliding window across our data and plot it? (you can see the code for doing this in the analysis notebook)That’s a big drop in performance for the most recent feedback.We should investigate further. Let’s compare the input data (i.e the zombie characteristic data) from the times of high accuracy to the times of low accuracy. Good thing we also collected that in our feedback:blue = correct prediction, red = incorrect predictionblue = correct prediction, red = incorrect predictionAh, the data looks completely different. I guess the zombie population has mutated! We need to retrain our model ASAP with new data. Good thing we collected the input data in the feedback, we can use that as the new training data (saves us having to manually collect new data). We can use information about the prediction the model made (“prediction” field) and the users’ feedback (“was correct” field) to infer the correct prediction label for the new training data:https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/hrefSee how this code is used in the bottom of the feedback analysis notebook.With this new data set we can train a new version of our model. This is an identical process to training the initial model but using a different data set (see the notebook), and then uploading it to ML Engine as a new version of the model.Once it’s on ML Engine you can either set it as the new default version of the zombies model so that all of your clients will automatically start having their prediction requests sent to the new model, or you can instruct your clients to specify the version name in their prediction requests:setting v2 as the default modelIf you set the default model to v2 then all prediction request to “zombies” will go to the v2 version:PREDICTION REQUEST BODY:{"instances":[[2.0, 3.4, 5.1, 1.0]],"model":"zombies"}or your clients can just be more specific:PREDICTION REQUEST BODY:{"instances":[[2.0, 3.4, 5.1, 1.0]],"model":"zombies/versions/v2"}After all that you can sit back and just run the same analysis after some more feedback has been collected:seems like people find our v2 model helpfulHopefully this has given you a few ideas on how you can monitor your deployed models for model rot. All of the code used can be found in the github repo:ZackAkil/rotting-zombie-modelReach out to me @ZackAkil with any thoughts/questions on monitoring model rot.Zombies &amp; Model Rot (with ML Engine + DataStore) was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'TensorFlow & reflective tape : why I’m bad at basketball '
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Fri, 05 Oct 2018 14:00:27 GMT'
* Categories: basketball, tensorflow, sports, data-science, machine-learning

TensorFlow &amp; reflective tape 🏀 (am I bad at basketball?)Recently a friend got me into basketball. Turns out, it’s a lot harder than it looks. No matter, I can over engineer a solution using machine learning. If your into ML and shooting hoops then there’s also this article that combined TensorFlow and basketball in a simulation.nothing but net… if there was a netThe task is to find the exact angle of my shots. Then I can hopefully use that information in a proactive way to get better.psst! the code for all of this is on my GithubTask 1: collecting dataI didn’t need to follow the seems of the ball, but it looks coolI don’t have access to 3D tracking studios fitted with 200 cameras, but I do have Ebay. It’s quite easy to buy reflective tape online and stick it to the ball. Then (thanks to the lack of lighting at my local court) I can record some footage of me practicing in the evening and capture the balls movements.The torch build into my phone provides the perfect light source to bounce off the reflective tap of the ball.As a result the footage captured shows a sparkly object flying through a mostly dark scene, perfect for doing some image manipulation in python.Task 2: Getting our video into pythonFirstly I do everything in Python, and a really easy way to import video into Python is to use a library called ‘scikit-video’, so I installed that:pip install scikit-videoand then used it to load in my video as a matrix:from skvideo.io import vreadvideo_data = vread('VID_20180930_193148_2.mp4')The shape (that you can find by running video_data.shape) of this data is (220, 1080, 1920, 3). Which means 220 frames of 1080x1920 pixels of 3 channels of colour (red, green, blue):raw video dataTask 3: Extracting the shot (image processing)So I want to get the data on just the balls movement. Fortunately, it’s one of the only things moving in the video. So I can do my favourite video processing trick: delta frame extraction! (that’s what I call it, but there’s probably another name for it).By subtracting all of the pixel values in one frame from all of the pixel values in the next frame you will be left with non-zero values in just the pixels that have changed.calculating delta frame in order to isolate moving pixelsCool cool cool, now I do that for each frame in the video and combine the result into one image:adding together all of the delta frames from the video sequenceNext is extracting the shot data into a usable format. So we’ll covert the pixel values that are lite up into a list of x and y points. The code to do this is a numpy function called numpy.where which will find all of the values that are True in an array and return their indices (i.e their positions in the matrix).But before we do that, we’ll quickly crop out just the balls trajectory and flip the data so that it starts at the origin (the bottom left of the scene):and the resulting image:Notice how it still seems upside down? That’s only because images tend to be drawn starting at the top left corner (note the axis numbering). When we convert the pixels to data points and draw them on a normal graph they will get drawn starting at the bottom left corner.now we run our numpy.where code to get the pixels as data points:pixels_that_are_not_black = cropped_trail.sum(axis=2) &gt; 0y_data, x_data = numpy.where(pixels_that_are_not_black)awesome! our relatively clean ball trajectory dataTask 4: Build TensorFlow modelThis is where TensorFlow shines. You may be used to hearing about using TensorFlow for building neural networks, but you can define almost any mathematical formula and tell it to optimise whatever parts of it you want. In our case we will use the formula for a trajectory which we know from primary school to be:extremely mathematical equation of trajectory that I found onlineθ (theta) is the angle of the shot (the value we really care about)v is the initial velocityg is gravity (9.8m/s)x is the horizontal position (data we have already)y is the vertical position (data we have already)A far more interesting way to see the equation in action is to play around with this trajectory tool.We can use the ball trail of my shot as the x and y of the equation and task TensorFlow with finding the correct angle (θ) and initial velocity (v) that fits my shots x and y data:We’ll start be recreating our trajectory equation in TensorFlow:first tell it what data we will feed it when we run the optimisation:x = tf.placeholder(tf.float32, [None, 1])y = tf.placeholder(tf.float32, [None, 1])next tell it what variables we want it to tweak and tune in order to fit the trajectory curve to our data:angle_variable = tf.Variable(40.0, name='angle_variable')force_variable = tf.Variable(100.0, name='force_variable')gravity_constant = tf.constant(9.8, name='gravity_constant')and join all of these together (warning: it’s going to look quite messy, but it’s just the maths equation seen before written in TensorFlow syntax):left_hand_side = x * tf.tan(deg2rad(angle_variable))top = gravity_constant * x ** 2bottom = (2*(force_variable)**2) *             (tf.cos(deg2rad(angle_variable))**2)output = left_hand_side - (top / bottom)then tell TensorFlow how to tell if it’s doing a good job or not at fitting the trajectory function to our data:# the lower this score, the bettererror_score = tf.losses.mean_squared_error(y, output)create an optimiser that will do the actual tweaking of variables (angle_variable and force_variable) in order to reduce the error_score:optimiser = tf.train.AdamOptimizer(learning_rate=5) optimiser_op = optimiser.minimize(error_score)Task 5 : MagicWe can now run the optimisation task to find the angle_variable and force_variable values that fit my shot.sess = tf.Session()sess.run(tf.global_variables_initializer())# do 150 steps of optimisationfor i in range(150):    sess.run([optimiser_op],              feed_dict={x: np.array(x_data).reshape(-1, 1),                         y: np.array(y_data).reshape(-1, 1)})found_angle = sess.run(angle_constant.value())print(found_angle)TensorFlow finding the angle of my shotAt the end of that optimisation we find out that the trajectory function that best fits my shot data has an angle of ~61°… not sure what to do with that information… I guess I could look at what professional shooting angles are for comparison… to be continued.The lesson to take away: you can always distract your-self with completely unnecessary (but fun) machine learning.All of the code I used is available on my Github:ZackAkil/optimising-basketballTensorFlow &amp; reflective tape : why I’m bad at basketball 🏀 was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Finally learn how to use command line apps… by making one!'
* Author: 'Zack Akil'
* URL: 'https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd?source=rss-5b7ea6e5016a------2'
* PublicationDate: 'Mon, 21 May 2018 21:07:45 GMT'
* Categories: beginner, command-line, linux, bash, python

At the risk of alienating a lot of readers… I grew up with GUI’s, so I never needed to learn the ways of the terminal! This is socially acceptable in todays society of friendly user interfaces, except maybe if you’re in software engineering… whoops!Manager — “Oh this is easy, just use this command line app”, Me — ”Yes…the command line… I’ll use that…”Getting back my software engineering street cred’I’ve gotten pretty far just through copy and pasting full command-line operations from Stack-Overflow but have never become comfortable enough to use a command line app “properly”. What finally caused it to click for me was when I stumbled across a very handy python library called argparse that allows you to build a nice robust command line interface for your python scripts.This tutorial is great in-depth explanation about how to use argparse, but I’ll go over the key eye openers:argparse is part of the standard python library so pop open your code editor and follow along (you don’t need to install anything)!HELPThe most useful part of every command line app!# inside a file called my_app.pyimport argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.parse_args()The code above will do nothing, except by default you have the help flag!In the command line you can run:python my_app.py --helporpython my_app.py -hand you’ll get an output like this:usage: my_app.py [-h]Nice little CL app!optional arguments:-h, --help  show this help message and exitSeems pretty cool right? but wait, when you use argparse to add more functions (see below) this help output will automatically fill up with all of the instructions of how you can use your app!REQUIRED ARGUMENTSLets say you want to have your app take in some variable, we just use the parser.add_argument() function and give our argument some label (in this case “name”):import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")args = parser.parse_args()print("Your name is what? " + args.name)Notice how we added the help text! Now when we run python my_app.y -h we get all of the app details:usage: my_app.py [-h] nameNice little CL app!positional arguments:name        Just your name, nothing specialoptional arguments:-h, --help  show this help message and exitPretty cool, but let’s run our app with python my_app.pyusage: my_app.py [-h] namemy_app.py: error: too few argumentsThat’s right! Automatic input checking!Now lets run python my_app.py "Slim Shady"Your name is what? Slim ShadyPretty slick!OPTIONAL ARGUMENTSMaybe you want to give someone the option of telling you little more about themselves? Adding a double dash when using parser.add_argument() function will make that argument optional!import argparseparser = argparse.ArgumentParser(description=”Nice little CL app!”)parser.add_argument(“name”, help=”Just your name, nothing special”)parser.add_argument("--profession”, help=”Your nobel profession”)args = parser.parse_args()print(“Your name is what? “ + args.name)if args.profession:    print(“What is your profession!? a “ + args.profession)If you want to pass in a variable for that argument you just need to specify the double-dashed argument name before the variable you want to pass:python my_app.py "Slim Shady" --profession "gift wrapper"which gives you :Your name is what? Slim ShadyWhat is your profession!? a gift wrapperOr you don’t have to, it is optional after all!python my_app.py "Slim Shady"still gives you:Your name is what? Slim Shadyand the magic once again when running python my_app.py -h :usage: my_app.py [-h] [--profession PROFESSION] nameNice little CL app!positional arguments:name                      Just your name, nothing specialoptional arguments:-h, --help                show this help message and exit--profession PROFESSION   Your nobel professionFLAGSMaybe you just want to enable something cool to happen. Add action="store_true" to your parser.add_argument() function and you have yourself a flag argument :import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")parser.add_argument("--profession", help="Your nobel profession")parser.add_argument("--cool", action="store_true", help="Add a little cool")args = parser.parse_args()print("Your name is what? " + args.name)cool_addition = " and dragon tamer" if args.cool else ""if args.profession:    print("What is your profession!? a " + args.profession + cool_addition)It’s pretty neat, you just plop the flag name into your command like so:python my_app.py "Slim Shady" --profession "gift wrapper" --cooland presto!Your name is what? Slim ShadyWhat is your profession!? a gift wrapper and dragon tamerand remember, you don’t have to use it, it’s just a flag:python my_app.py "Slim Shady" --profession "gift wrapper"will still give:Your name is what? Slim ShadyWhat is your profession!? a gift wrapperlook though at the help command python my_app.py -h :usage: my_app.py [-h] [--profession PROFESSION] [--cool] nameNice little CL app!positional arguments:name                      Just your name, nothing specialoptional arguments:-h, --help                show this help message and exit--profession PROFESSION   Your nobel profession--cool                    Add a little coolI’m just going to assume you’re as satisfied with that as I am from now on.SHORT FORMUnveiling the mysterious one character arguments that confused me for so long. Just by adding a single letter prefixed with a single dash to your parser.add_argument() functions you have super short versions of the same arguments:import argparseparser = argparse.ArgumentParser(description="Nice little CL app!")parser.add_argument("name", help="Just your name, nothing special")parser.add_argument("-p", "--profession", help="Your nobel profession")parser.add_argument("-c", "--cool", action="store_true", help="Add a little cool")args = parser.parse_args()print("Your name is what? " + args.name)cool_addition = " and dragon tamer" if args.cool else ""if args.profession:    print("What is your profession!? a " + args.profession + cool_addition)So that instead of typing in :python my_app.py "Slim Shady" --profession "gift wrapper" --coolyou can just type:python my_app.py "Slim Shady" -p "gift wrapper" -cand you’ll get the same output:Your name is what? Slim ShadyWhat is your profession!? a gift wrapper and dragon tamerAnd this is reflected in the help text (python my_app.py -h ):usage: my_app.py [-h] [--profession PROFESSION] [--cool] nameNice little CL app!positional arguments:name                           Just your name, nothing specialoptional arguments:-h, --help                      show this help message and exit-p PROFESSION, --profession PROFESSION   Your nobel profession-c, --cool                      Add a little coola perfect little command line app!You now have a perfect little command line app and are hopefully more comfortable with finding your way around command line apps!Just remember --help !ZackAkil/super-simple-command-line-appFinally learn how to use command line apps… by making one! was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.
