<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Zack Akil on Medium]]></title>
        <description><![CDATA[Stories by Zack Akil on Medium]]></description>
        <link>https://medium.com/@zackakil?source=rss-5b7ea6e5016a------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*fKWeN3MkK_yBAEXYAPGvjA.jpeg</url>
            <title>Stories by Zack Akil on Medium</title>
            <link>https://medium.com/@zackakil?source=rss-5b7ea6e5016a------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 14:45:35 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@zackakil/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Hobbies to Hired: 5 tips for making great Portfolio Projects to Land Your First Tech Job]]></title>
            <link>https://medium.com/@zackakil/hobbies-to-hired-5-tips-for-making-great-portfolio-projects-to-land-your-first-tech-job-8c45aaeafd06?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/8c45aaeafd06</guid>
            <category><![CDATA[portfolio]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[junior-developer]]></category>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[jobs]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Sun, 14 Apr 2024 00:07:10 GMT</pubDate>
            <atom:updated>2024-04-15T18:43:33.585Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rO_yOG2uhV-dw4Q4FQwHcg.jpeg" /></figure><p>When you starting off in tech, building strong portfolio projects is an essential step in showcasing your skills and impressing potential employers. However, figuring out what projects to create and how to make them shine can feel overwhelming. This guide will break down 5 actionable tips to help you start building projects that youâ€™ll be excited to showÂ off!</p><h3>1. Go â€œoffÂ trailâ€</h3><p>Especially for those coming from bootcamps, just having projects that were â€œ<strong><em>part of the curriculum</em></strong>â€ in your portfolio shows to the interviewer that you can follow instructions in a safe environment. However in the wild those conditions are rare. You need projects that show you ability to (<strong><em>independently)</em></strong> <strong>IDENTIFY</strong> a problem, then (<strong><em>independently)</em></strong> <strong>LEARN</strong> something new that can be used to solve that problem, and of course successfully (<strong><em>independently)</em></strong> <strong>APPLY</strong> that learning to solve theÂ problem.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Cd-gE6QDB3f9V1bZ7Nf3hA.jpeg" /></figure><p>When telling the story of these â€œoff trail projectsâ€ a good structure isÂ :</p><ol><li>I saw thisÂ problem.</li><li>I learnt about this technology that could be used to solve theÂ problem.</li><li>I built this thing using what I learnt to solve theÂ problem.</li></ol><p>When hiring juniors employers main concern is that they will have to provide extensive hand-holding. These kind of projects directly address that concern by <strong>showing </strong>that you can take ownership of your learning rather than always having to rely on explicit instructions.</p><p>So back yourself and go â€œ<strong>offÂ trail</strong>â€!</p><h4>Key tip about your first â€œoff trail projectsâ€</h4><p><strong>One new thing at a time</strong>: Itâ€™s good practice focus on one new skill at a time with each new project. This helps keep the complexity manageable, allowing you to apply your existing knowledge while stretching your abilities with your new learnings. <em>E.g</em> <em>I might want to learn about Tensorflow.js and also d3js, and I already know how to make HTML webpages. So Iâ€™ll make a project were I apply my existing HTML knowledge to my new Tensorflow.js knowledge in some app. Then Iâ€™ll do a separate project were I build something with d3js in a HTML app. Then finally another separate project that combines everything! Thatâ€™s 3 unique projects that can go my portfolio!</em></p><h3>2. The Enthusiasm hack</h3><p>People want to work with enthusiastic people that do exciting work. As a junior developer, enthusiasm should be one of your strengths. But, how do you get interviewers enthusiastic about your portfolio projects?</p><p>First off, <strong>YOU</strong> have to be genuinely enthusiastic by your portfolio projects!</p><p>Do that by working on projects that genuinely excite you. Classic patterns for identifying projects that elicit genuine enthusiasm are:</p><ul><li>ğŸ¤© Projects based on yourÂ hobbies.</li><li>ğŸ¤© Project that solve a real problem that you genuinely face.</li><li>ğŸ¤© Projects that solve a real problem for people you genuinely careÂ about.</li></ul><p>Projects that are harder to elicit genuine enthusiasm forÂ are:</p><ul><li>ğŸ¥± Any project you â€œhad to doâ€ / â€œwere told to doâ€ e.g assigned coursework.</li><li>ğŸ¥± Projects that solve hypothetical problems for hypothetical people. (due to their lack of an authentic story)</li></ul><p>Come interview time youâ€™ll find it a lot easier to speak enthusiastically about your work, and that genuine enthusiasm is infectious. So lean into, let yourself geek out and exude that energy that embodies the excitement of being a junior developer!</p><p>An example of a portfolio projects of mine that I was able to easy get enthusiastic about was my <a href="https://www.youtube.com/watch?v=H2UNijCq-v4">rugby recording robot</a>. Without any more context, you can probably guess what sport IÂ like.</p><h4>Additional perks of working on intrinsically interesting projects</h4><ol><li>It will help keep your engaged in the project when things get challenging.</li><li>You are are going to be more inclined to talk about the project with those around you. This gives you free practice in your technical communication skills.</li><li>Often times they will be projects that are naturally unique to you. Great portfolio projects are the oneâ€™s <strong>only you</strong> could have made. Their uniqueness embody a part of your personality e.g <em>an app that was inspired by your hobby and the tech skills you want to become proficient in.</em></li></ol><h3>3. â€œMany and Smallerâ€ vs â€œFewer andÂ Biggerâ€</h3><p>You probably have an idea for an app in your head (or are currently building one) that is a bit of a beast e.g multi-social user login, full admin CRUD, email notifications, payment system, plus whatever the USP isÂ etc.</p><p>As a portfolio project these kind of projects have merit in giving you holistic experience in every part of application development. However, with one big project you get to setup your repoâ€¦ once, your get you design your UXâ€¦ once, you get to identify a problem specâ€¦ once. Vs with many smaller projects were you get to go through the whole <strong>Problem -&gt; Idea -&gt; Design -&gt; Implementation</strong> process multiple times starting from a blankÂ slate.</p><p>This is the same principle as the â€œ<a href="https://medium.com/illumination/quality-is-the-destination-quantity-is-the-path-and-photos-are-the-unexpected-proof-b131347e178e">Photography Quantity vs Qualityâ€ story</a>. Give yourself many opportunities to try things, learn from them, and take those learnings to the next project, and repeat as many times as possible.</p><p>So the algorithm to a fruitful portfolio journey isÂ :</p><pre>knowledge = None<br>while(1): <br>  learnings = build_project(size=&quot;small&quot;, applied_knowledge=knowledge)<br>  knowledge += learnings</pre><h3><strong>4. Make it instantly interactive </strong>ğŸª„</h3><p>Having interviewed many devs straight out of bootcamps, the first thing I check is their github and go to their latest (or pinned) project. For web devs itâ€™s usually a website, coolâ€¦ but where is it? Oh I have to first clone it then do â€œnpm runâ€ yadayadayada â€œlocalhostâ€â€¦ sigh!</p><p>Donâ€™t make interviewers work to see your work. If youâ€™ve made a website, have it deployed somewhere (e.g <a href="https://pages.github.com/">Github pages</a>) <strong>and put the link at the top of the projectsâ€™ README</strong>. Did you make some cool python stuff? have a link to a <a href="https://colab.research.google.com/">Colab</a> or <a href="https://streamlit.io/">Sreamlit app</a> <strong>and put the link at the top of the projectsâ€™ README</strong>.</p><p>If there is literally no way to make your work instantly interactive then at least have some kind of videos or gifs embedded in the README. Have some way to <strong>instantly</strong> <strong>show</strong> yourÂ work.</p><h3>5. Put a bow on itÂ ğŸ€</h3><p>Youâ€™ve probably put days/weeks of time into your project. However youâ€™ve probably put zero time into the first thing someone will see when looking at your projectâ€¦ theÂ <strong>README</strong>!</p><p>Take 15 minutes to really spruce up your projects README with nice formatting, links, images, gifs etc. The README is your projects (and likely you as a developersâ€™) first impression, so make it one that shows you care about communication and presentation of your work. Check out this list by <a href="https://twitter.com/matiassingers">@matiassingers</a> for inspirationÂ : <a href="https://github.com/matiassingers/awesome-readme?tab=readme-ov-file">awesomeÂ READMEs</a></p><h3>Bonus tip (especially when applying to larger companies likeÂ Google)</h3><p>At Google there is a specific emphasis put on <strong>knowledge sharing</strong>. Showcasing that you not only add value as an individual, but also elevate those aroundÂ you.</p><p>Showcasing your enthusiasm for sharing knowledge by creating resources that help others. Whether itâ€™s visualization tools or blog posts, sharing your insights and resources can make a powerful impression.</p><h3>Conclusion</h3><p>So, there you have it: five (+1) actionable tips to for building great portfolio projects. By venturing â€œoff trail,â€ embracing your enthusiasm, building many smaller projects, making your work instantly interactive, and putting a bow on it with a polished README, youâ€™ll create a portfolio that not only showcases your skills but also tells a compelling story about you as a developer.</p><p>Remember the tech world is constantly evolving, and the best developers are those who never stop learning. Use your portfolio as a tool for growth, pushing your boundaries and showcasing your ability to adapt and master new skills. Good luck on yourÂ journey!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8c45aaeafd06" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to Identify Innovative Ideas with the Technical Horizon Framework]]></title>
            <link>https://medium.com/@zackakil/how-to-identify-innovative-ideas-with-the-technical-horizon-framework-f7c9c613682e?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/f7c9c613682e</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[innovation]]></category>
            <category><![CDATA[ideas]]></category>
            <category><![CDATA[management]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Sat, 12 Aug 2023 15:52:26 GMT</pubDate>
            <atom:updated>2023-08-12T15:52:26.534Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*WEqQPfG9khm8JEQAheVRqw.png" /></figure><p>â€œ<strong><em>Itâ€™s been done before!</em></strong>â€ or â€œ<strong><em>Itâ€™s not possible!</em></strong>â€, the last thing you want to hear when you pitch your new idea. What if you could quickly and pragmatically identify better, more innovative ideas that are actually feasible? Enter the â€œTechnical Horizon Frameworkâ€.</p><h3>What are Technical Horizons?</h3><p>As technology advances with time, what is technically feasible to <em>consumers</em>, <em>developers</em>, and <em>researchers</em> is constantly expanding, these are the <strong>Technical Horizons</strong>. The key points about themÂ are:</p><ol><li>Technical Horizons are always expanding as technology advances.</li><li>Ideas are static and are defined by the <strong>simplest technology required</strong> to buildÂ them.</li><li>Every idea within your Technical Horizon is technically feasible.</li><li>Consumers can build ideas within the <em>Consumer</em> Technical Horizon, developers can build ideas within the <em>Developer</em> <strong>and </strong><em>Consumer</em><strong> </strong>Technical Horizon, and researchers can build ideas within the <em>Researcher</em>, <em>Developer</em> <strong>and </strong><em>Consumer</em> Technical Horizon.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*WEqQPfG9khm8JEQAheVRqw.png" /></figure><p>The Technical Horizon chart aboveÂ shows:</p><ul><li>Plotting data (e.g charts in spreadsheets) is easy for consumers i.e very feasible.</li><li>Image Generation is newly feasible to consumers (as of 2023) through consumerÂ apps.</li><li>Video classification is very feasible to developers with tools like Googles <a href="https://cloud.google.com/video-intelligence">Video Intelligence API</a>, but would be infeasible for consumers without developer knowledge.</li><li>Video summarisation is a newly feasible idea for developers by combining multiple APIs like the previously mentioned Video Intelligence API and <a href="https://developers.generativeai.google/">Googles PaLMÂ API</a>.</li><li>Video clip generation is currently not feasible without research level knowledge however in the near future APIs to do this may be available.</li><li>Full movie generation would only be feasible with a lot of researchÂ time.</li></ul><h3>Where are your innovative ideas?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*2fNDQ5rsFEJsbdKzNscXoQ.png" /></figure><p>You can use the Technical Horizon Framework to identify which of your ideas are â€œinnovativeâ€. Simply by looking to the immediate inside edge of <strong>your</strong> Technical Horizon. If you are a consumer, Image Generation is an innovative idea that is good for consumers toÂ explore.</p><p>As a developer; Video summarisation is currently an innovative space. The reason the space on the immediate inside of your Technical Horizons is innovative, is because in this space there are ideas that have never been done before; <strong>because they could never have been done before</strong> due to the enabling technology only recently becoming available.</p><h3>What about ideas outside of your Technical Horizon?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/676/1*VOIioOr4stjIQnPGFJ1vDg.png" /></figure><h4><strong>Almost feasibleÂ ideas</strong></h4><p>Assume your a developer and youâ€™ve come up with an idea were the APIs arenâ€™t quite there yet (e.g Video clip generation), itâ€™s just outside your Technical Horizon, or â€œ<strong><em>almost feasible</em></strong>â€<strong><em>. </em></strong>You have a fewÂ options:</p><ul><li>Park the idea and wait for the tools to become available, and spend your time on more feasibleÂ ideas.</li><li>Or, embark on building the idea anyway, but risk wasting your developer resources as at any time an API could be released that nudges the Developer Technical Horizon forward, potentially making the idea buildable with a few lines of code, and rendering your work potentially redundant. Whatâ€™s especially dangerous in this scenario is you (or your team) are at risk of â€œ<a href="https://www.google.co.uk/search?q=sunk-cost+fallacy">sunk-cost fallacy</a>â€ if an API does come out after youâ€™ve already invested time into your own implementation when using someone else&#39;s solution might beÂ better.</li></ul><h4>Technical Horizons are invisible</h4><p>The challenge here is the <strong>unknown</strong>. When will the developer tools be ready? A day, month, year? This is were having good channels for up to date news about new tools is valuable. Also being a part of early access programs and going to conferences lets you peek into the future of where your Technical Horizon isÂ going.</p><h4>Very infeasible ideas (Moonshots)</h4><p>Keep track of them, they might be infeasible now, but Technical Horizons can leap forward suddenly, like it has with the surge of LLMs (Large Language Model) technology.</p><p>If waiting isnâ€™t an option, a word for these high risk - high reward ideas is a â€œMoonshotâ€. Accept that itâ€™s a bold undertaking so the pay off needs to be worth theÂ risk.</p><h3>Watch out for â€œShoehorn Innovationâ€</h3><p>â€œ<em>My idea is innovative because itâ€™s using the latest technology!</em>â€, a<br>clichÃ© when any new technology is released.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/839/1*4Mmyz0dZ-M3eNi-tyrvbHQ.jpeg" /></figure><p>The â€œinnovativenessâ€ of an idea is not defined by the technology it uses, but by the actual technical requirements of the problem itâ€™s solving. An innovative idea is solving a problem that <strong>can only be solved</strong> with new technology, so ask the question â€œCan this problem already be solved with an existing technology?â€, a hard but important pill to swallow sometimes.</p><h3>Wrap up</h3><p>When you find an idea that is genuinely newly feasible with a new technology, and itâ€™s solving a problem that could never have been solve before, then you know you have an innovative idea.</p><h4>Key takeaways</h4><ol><li>A truly innovative idea is one that solves a problem that could not previously beÂ solved.</li><li>Listen out for news about new technology that could push your Technical Horizon forward. â€œWhat can I do now do that I couldnâ€™t beforeâ€, is a good question to be asking about any new technology you hearÂ about.</li><li>Beware of â€œShoehorn Innovation.â€ Just because a new technology is available doesnâ€™t mean that you need to use it in your idea. Donâ€™t just use new technology for the sake of usingÂ it.</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f7c9c613682e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How I conquered writer's block with Google Assistant + Google Docs]]></title>
            <link>https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/e327d18b52b</guid>
            <category><![CDATA[writing]]></category>
            <category><![CDATA[google-docs]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[productivity]]></category>
            <category><![CDATA[google-assistant]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Fri, 30 Jul 2021 15:51:57 GMT</pubDate>
            <atom:updated>2021-07-30T18:26:55.370Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VRQzx-1P5Qu7uRzJ_dDorQ.png" /></figure><p>Over lockdown I built a voice app that would interview me about my projects and help write articles for me. What would usually take me weeks, now takes me an hour or two, and as a testament to that I wrote one article a day for a week and lived to tell theÂ tale:</p><p>Day 1</p><p><a href="https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505">See what Video Intelligence API can do with this visualisation tool</a></p><p>Day 2</p><p><a href="https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533">ğŸ¥ ğŸ™ˆ Using AI to automate phobia safe film production</a></p><p>Day 3</p><p><a href="https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d">ğŸ¥ ğŸ‰ AppSheet + AppScript for sports videography highlights tracking</a></p><p>Day 4</p><p><a href="https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9">ğŸš´ Safer cycling with ML and Edge TPUâ€™s</a></p><p>Day 5 (<em>this articleÂ </em>ğŸ¤¯)</p><p><a href="https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b">How I conquered writer&#39;s block with Google Assistant + Google Docs</a></p><h3>Background</h3><p>Writing is something Iâ€™ve always struggled with, and in my role as a Developer Advocate it can be an important part of the job to make and share content about what Iâ€™ve built in order to inspire and educate fellow developers. It wouldnâ€™t be uncommon for me to take longer writing an article than it did for me to build the thing I was writingÂ about.</p><p>One time I had built something and someone reached out to interview me about my project so that they could write an article on my behalf. Obviously I found that experience a lot easier. So that gave me an idea; what if I could build some voice AI that would interview me about my projects and generate the articles forÂ me.</p><h3>Solution</h3><p>Using <a href="https://developers.google.com/assistant/console/builder">Google Action Builder</a> I built a Google Assistant app that would ask me a set of questions about a single project and then output what I said into a generated GoogleÂ Doc.</p><p>I followed this tutorial to learn how to build anÂ action:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FZ1hxvniJ18s%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DZ1hxvniJ18s&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FZ1hxvniJ18s%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/d61d324ae46d344aa227912458dc9e14/href">https://medium.com/media/d61d324ae46d344aa227912458dc9e14/href</a></iframe><p>Within the Assistant app I have a custom webhook that is a Google Cloud Function that takes what was said and writes it into a new Google Doc using the <a href="https://developers.google.com/docs/api/quickstart/python">Google Docs python client API</a>. See the code for that Cloud Function on myÂ github:</p><p><a href="https://github.com/ZackAkil/project-reporter-voice-app">GitHub - ZackAkil/project-reporter-voice-app: A voice app that interview me about my projects in order to generate articles.</a></p><p>The first step is always the hardest so even though itâ€™s essentially just a dictation app with some prompting questions, it makes that initial draft phrase of article writing virtually zero friction for me. I never have to see that horrible immovable empty document again when writing articles. Here it is inÂ action:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FuRJAftUEuW8%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DuRJAftUEuW8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FuRJAftUEuW8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/href">https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/href</a></iframe><h3>Tricky challenges</h3><h4>Authentication! Authentication! Authentication!</h4><p>Getting Google Assistant to talk directly to Google Docs is especially tricky as you have to navigate a merade of authentication services in order to create and write docs on the usersâ€™ behalf. I created <a href="https://docs.google.com/presentation/d/11aLNFEqMWUiKYbeh3MdWrSMw-pEKQQ4PmUL0mhbC5x4/edit?usp=sharing\">this slide deck</a> to loosely document the process in case youâ€™d like to do something similar, but be warned it is a lot of throwing around of api keys, client secrets, and refresh tokens. <strong>But it can beÂ done!</strong></p><h4>Speak quick and concise (look to using Dialogflow)</h4><p>Google Assistant apps have a 30 second response limit so I have to speak quite quickly, and as soon as there is a pause in my voice it cuts the mic and takes that as my answer. For my first version of this app Iâ€™m OK with that, however for a more natural interview experience where you can take your time and collect your thoughts before you answer Iâ€™d look into using Dialogflow which allows for more granulare configuration and longer talking response times. Check out <a href="https://twitter.com/ladysign"><strong>Lee Boonstra</strong></a>â€™s I/O talk to learn more about Dialogflow:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FO7JfSF3CJ84%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DO7JfSF3CJ84&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FO7JfSF3CJ84%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/href">https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/href</a></iframe><h4>Remembering the DocÂ ID</h4><p>Because I was using a Cloud Function as my webhook, there was no guarantee that the exact same instance of that Cloud Function would be called during the same conversation (<em>one of the tricky things with serverless tech</em>). So I used Cloud Firestore as a storage medium to store which conversation IDs (<em>which are passed to every webhook call</em>) map to which generated Google DocÂ IDs.</p><h3>Future</h3><p>As an extra step in the article writing process it would be cool to have some machine learning text generation automatically generate tweets based on what I said about myÂ project.</p><p>Also the interview process is currently just a static set list of questions. I could use some natural language ML tools like the <a href="https://cloud.google.com/natural-language">Natural Language API</a> or <a href="https://cloud.google.com/natural-language/automl/docs">AutoML Natural Language</a> to do some intelligent analysis into my responses in order to adapt the follow up questions like in a real interview.</p><h3>Takeaways</h3><p>Leverage your strengths (<em>for me itâ€™s building things</em>) to overcome your weaknesses (<em>for me itâ€™s writingÂ things</em>).</p><p>You can get Google Assistant to talk directly to Google Docs (<em>and the rest of G Suite</em>), itâ€™s just a fair bit of authentication loops you have to jumpÂ through.</p><p>It can be a lot easier to say something than to write something so be aware of voice technologies like <a href="https://developers.google.com/assistant">Google Assistant</a> and <a href="https://cloud.google.com/dialogflow">Dialogflow</a>.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e327d18b52b" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ Safer cycling with ML and Edge TPUâ€™s]]></title>
            <link>https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/c4883a82dae9</guid>
            <category><![CDATA[raspberry-pi]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[cycling]]></category>
            <category><![CDATA[google]]></category>
            <category><![CDATA[edge-computing]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Thu, 29 Jul 2021 13:33:19 GMT</pubDate>
            <atom:updated>2021-07-29T13:39:54.411Z</atom:updated>
            <content:encoded><![CDATA[<p>A while ago I tried to make my bike smarter with machine learning to help keep me safe whilst commuting through busy LondonÂ streets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9-QqkNPrP9lB7SQ3vzGZfA.png" /></figure><h3>Background</h3><p>If you live in a big city and cycle you probably have developed superhuman reflexes and awareness, especially in cities like London where a lot of the narrow roads were originally designed for horse andÂ cart.</p><p>After enough close calls and some actual calls I figured it was time to try and solve this problem with a modern approach (<em>without mirrors</em>).</p><h3>Solution</h3><p>Using a small computer (<em>Raspberry Pi</em>) attached to the bike. It would run an object detection model on images being taken from a rear facing camera. The system would detect the different potential dangers behind and intuitively relay that information back to the rider (<em>via a handlebar mounted LED strip</em>) so that they can maintain their focus on what&#39;sÂ ahead.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/907/1*J0IQG8Bx_h9TGEIUYk1bNA.png" /><figcaption>what an object detection model wouldÂ detect</figcaption></figure><h4>Need forÂ speed</h4><p>With just the Raspberry Pi running the object detection model it was able to make predictions at a rate of about 2 predictions per second, which for this use case is on the slow side. That is where the <a href="https://coral.ai/">Coral Edge TPU</a> comes in. It is able to run the same model at 30â€“60 predictions per second giving the system the super low latency it needs. You can find out more about Coral Edge TPUâ€™s from this Google I/OÂ session:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FnWExsl6iAxA&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DnWExsl6iAxA&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FnWExsl6iAxA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/7400903711405d60c51ba4a89e582f1c/href">https://medium.com/media/7400903711405d60c51ba4a89e582f1c/href</a></iframe><p>Specifically I used the <a href="https://coral.ai/products/accelerator">Edge TPU USB Accelerator</a> and had it and the Raspberry Pi both attached to theÂ bike:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/980/1*j1woMnVtKe1-SD5Gjybuvg.png" /><figcaption>prototype showcasing the Coral Edge TPU usb accelerator</figcaption></figure><p>After the Raspberry Pi + Edge TPU made itâ€™s prediction of what&#39;s behind the bike, it would then send commands to an RGB LED strip on the bikesâ€™ handlebars to then indicate to the rider where the dangers from behind are, and what level of danger those things are (<em>because itâ€™s able to distinguish between the different types of vehicle e.g car, bus, truck, bike, pedestrian</em>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4d4wVG7EaaO-aE9C.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/677/0*RdcmfLmp9WXkJN-O" /><figcaption>prototype and real bikeÂ version</figcaption></figure><h4>The specificÂ model</h4><p>Due to this being a pretty generic prediction task (<em>traffic object detection</em>), there were already pretrained models I could download and use that already detected different types of vehicles. In fact I ended up using the exact same model that Bill used in his 2019 Google I/OÂ demo:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FJgm25QdF90A%3Fstart%3D1789%26feature%3Doembed%26start%3D1789&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJgm25QdF90A&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FJgm25QdF90A%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/href">https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/href</a></iframe><p>You can explore different kinds of pretrained models on the CoralÂ website:</p><p><a href="https://coral.ai/models/">Models | Coral</a></p><p>You can also find all the code for this project on myÂ github:</p><p><a href="https://github.com/ZackAkil/edge-TPU-safe-bike">GitHub - ZackAkil/edge-TPU-safe-bike: An application of realtime object-detection running on an Edge TPU for making cycling in busy cities a little less terrifying.</a></p><h3>Interesting points</h3><h4>Australia has it ownÂ dangers</h4><p>When I presented this idea in Sydney there were discussions around if a specific version of this could be made for their specific dangers. At the time I assumed they might have been talking about some cliche dangers when you think of Australiaâ€¦ kangaroos, snakes, drop bears. But actually for cyclists in Australia itâ€™s teritorial magpies that are <a href="https://www.google.com/search?q=australia+magpie+cyclist">constantly attacking</a>. So a custom version that included swooping birds in the model would be perfect forÂ them.</p><h4>Pretrained models not designed for toyÂ cars</h4><p>Itâ€™s important to test exactly how youâ€™re going to use pretrained models. When I was demoing this idea on stage I hadnâ€™t thoroughly tested how well it worked with toy cars and I got suboptimal results. This is where training custom models with tools like <a href="https://cloud.google.com/vision/automl/object-detection/docs">AutoML Object Detection</a> would shine. <em>Also interesting to note that my toy version of this demo worked a lot better when I included a printed out fake road for the toy cars to sit on, see in the previous gif aboveÂ </em>ğŸ‘†.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FUmDeKDgFRj4%3Fstart%3D1699%26feature%3Doembed%26start%3D1699&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DUmDeKDgFRj4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FUmDeKDgFRj4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/href">https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/href</a></iframe><h3>Future</h3><p>I want to train a custom model that would be trained specifically on London traffic (<em>including the distinctive taxis</em>) and then I could look into training other custom models for different regions such as an Australian magpie edition. Oh, and actually make this on a real bike that I can cycle (<em>not just a prototype</em>).</p><h3>Takeaway</h3><p>Itâ€™s good to be aware of technology like Edge TPUâ€™s in case you have a problem that requires super low latency and mobility. Also always checking to see if there are pretrained models that could already solve your problem and save you a huge amount of development time.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c4883a82dae9" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[  AppSheet + AppScript for sports videography highlights tracking]]></title>
            <link>https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/1b40fdf9a47d</guid>
            <category><![CDATA[productivity]]></category>
            <category><![CDATA[appsheet]]></category>
            <category><![CDATA[app-script]]></category>
            <category><![CDATA[sports]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Wed, 28 Jul 2021 14:59:12 GMT</pubDate>
            <atom:updated>2021-07-28T15:42:31.973Z</atom:updated>
            <content:encoded><![CDATA[<p>During the summer I was asked to help with some video recording for a rugby tournament, but rather than spending hours rewatching footage to find highlights, I quickly threw together some tech to make things a lotÂ easier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Oh6iX2qwulAu63rcv3pejA.png" /></figure><h3>Background</h3><p>Sport videography is a hobby of mine (<em>specifically rugby</em>). It is however very tedious work rewatching full matches in order to find the highlights. That&#39;s why when I was asked to help a friend film an entire rugby tournament this summer I was hesitant. Rewatching a full dayâ€™s worth of rugby videos is not something I wanted either of us to have toÂ do.</p><p>There are some cameras that have a â€œ<strong>mark special moment</strong>â€ button that will let you set a marker of when something cool happens, but we were using multiple cameras that didnâ€™t have that feature. We discussed a simple paper based solution where we just noted down the time of when cool things happened. This was the solution we settled on until the morning of the event which is when I discovered a technology that would make things a lotÂ easier.</p><h3>Solution</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c9NZx_vB5qs01clp3NFodg.png" /><figcaption>original spreadsheet</figcaption></figure><p>For the data capture/entry we used <a href="https://www.appsheet.com/">AppSheet</a>, which is a no code app builder that integrates seamlessly into Google Sheets (<em>as well as other things</em>). As long as you have your spreadsheet set up with the right data formats for your columns, AppSheet can generate a clean and robust data entry UI that feeds data directly back into the spreadsheet.</p><p>I found out about AppSheet from this video by <a href="https://twitter.com/carterthecomic"><strong>Carter Morgan</strong></a><strong>Â </strong>ğŸ‘‡:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FDjAD81A9nYk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DDjAD81A9nYk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FDjAD81A9nYk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/href">https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/href</a></iframe><p>Thanks to Carterâ€™s intro I was able to build the perfect data entry app on the same morning of the tournament that would fit my needs (and was even able to style the colour theme of it), and both my friend and I were able to access it from a <a href="https://play.google.com/store/apps/details?id=x1Trackmaster.x1Trackmaster&amp;hl=en_GB&amp;gl=US">native Android App</a> on theÂ field!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mE0iDvtEh3sfes1fPpl2Vg.gif" /><figcaption>generated AppSheetÂ app</figcaption></figure><p>Throughout the day, anytime something cool happened whilst the cameras were rolling I just put in a note using the app, and because of how I set it up; the app knew to use the current time as the defaultÂ value.</p><h4>After the tournament wasÂ over</h4><p>When it came to finding the actual clips that contained the highlights after the tournament, this is where <a href="https://developers.google.com/apps-script">AppScript</a> came into play (<em>I know AppScript and AppSheet are annoying similar names</em>). AppScript is a technology that lets you build automation within GSuit e.g Google Drive, Sheets, Docs, etc. Check out this video for a quick overviewÂ ğŸ‘‡</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FAs3keJaPmmk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DAs3keJaPmmk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FAs3keJaPmmk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/72a41db491bd122711fb457c959c30e7/href">https://medium.com/media/72a41db491bd122711fb457c959c30e7/href</a></iframe><p>We first uploaded all of the video clips from the tournament into Google Drive. Then within the Google Sheet that contained all of our inputted highlight timestamps I wrote a AppScript that would loop through each rowÂ and:</p><ol><li>extract out the time data for that specific highlight</li><li>scan through the file metadata of all the videos in a specified Google Drive folder to find the video that overlapped with thatÂ time</li><li>automatically paste in a link to that video file back into the spreadsheet</li></ol><p>The AppScript that does this is attached to the Google Sheet so that I get a nice toolbar button to trigger the script. When the script is triggered it prompts me to specify the Google Drive folder url containing the tournament videos to search through. Watch it in actionÂ ğŸ‘‡</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*OzJQdhWJwhZkz_F_0IzHtQ.gif" /><figcaption>the automated clip finding process using myÂ script</figcaption></figure><p>You can see all the AppScript code in myÂ github:</p><p><a href="https://github.com/ZackAkil/gsuit-sports-videography-tool/blob/main/script.gs">gsuit-sports-videography-tool/script.gs at main Â· ZackAkil/gsuit-sports-videography-tool</a></p><p>and hereâ€™s a visual overview of the wholeÂ system:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G40tztSeU_YnkrWeFZY0CA.png" /><figcaption>high level diagram of the entireÂ solution</figcaption></figure><h3>Tricky challenges</h3><h4>Camera clocks</h4><p>When I first ran the script to find the matching clips they were all wrong. Turns out the system clock of the camera was wrong ğŸ¤¦â€â™‚ï¸. So I added a quick and dirty bit of code to the script to offset theÂ times.</p><h4>â€˜File createdâ€™ time vs â€˜File updatedâ€™Â time</h4><p>When I was writing the code to fetch the video metadata from Google Drive I thought that the file.getDateCreated() method from the <a href="https://developers.google.com/apps-script/reference/drive/file">AppScript Google Drive API</a> would tell me when the video was captured. Turns out that method just returns when the file was created/uploaded â€œ<strong>in Google Drive</strong>â€. This had me panicked for a while thinking the project was dead as I could not extract the lower level file metadata, but turns out file.getLastUpdated() will tell you the last time the file was edited which in our case was the time the video was captured on theÂ camera.</p><h3>Future</h3><p>Now that we have timestamps of highlights along with the specific video clips we can look into training machine learning models that could automatically detect highlights in videos. This could be done with a tool like <a href="https://cloud.google.com/video-intelligence/automl/docs">AutoML Video Intelligence</a> which can train powerful video models without us writing any machine learningÂ code.</p><h3>Takeaway</h3><p>The time it takes to build an app is <strong>not correlated</strong> to how useful that app can be. Tools like AppSheet take minutes to use and when the right moment comes for them they can save you literally days of work. Combine that with scripting tools like AppScript and you have a powerful problem solvingÂ duo.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1b40fdf9a47d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[  Using AI to automate phobia safe film production]]></title>
            <link>https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/d57a2051f533</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[machine-vision]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[video-production]]></category>
            <category><![CDATA[film]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Tue, 27 Jul 2021 12:58:37 GMT</pubDate>
            <atom:updated>2021-07-27T13:18:42.528Z</atom:updated>
            <content:encoded><![CDATA[<h3>ğŸ¥ğŸ™ˆ Using AI to automate phobia safe film production</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/429/1*4IJ3uqqnyf0hTp0zb6KMgA.png" /></figure><p>At Google I/O this year I used ML to build an automated video processing pipeline that detects whatâ€™s in a video and automatically hides the parts you would findÂ scary.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FIWPclIcRCrM%3Fstart%3D23%26feature%3Doembed%26start%3D23&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIWPclIcRCrM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FIWPclIcRCrM%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/href">https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/href</a></iframe><h3>Background</h3><p>Whilst watching some movies with some friends, we discovered that one of our friends has a deathly fear snakes and so we had a tell them exactly when the snakes were going to appearÂ , and sometimes we got this wrong ğŸ˜¬ (<em>watching</em> <em>Harry Potter was a poor choice in hindsight</em>).</p><h3>Solution</h3><p>I built an automated video processing pipeline using <a href="https://cloud.google.com/video-intelligence">Video Intelligence API</a> that would scan through a video to detect all the things within the video. Then if it detects something that matches with the phobia that we told the system about, it would use the <a href="https://cloud.google.com/transcoder/docs/concepts/overview">Transcoder API</a> to insert an overlay and hide it from view. So the solution was based on these twoÂ APIs:</p><h4>Video Intelligence API</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*JerXnfAbOCQNy_EX.gif" /><figcaption><a href="https://zackakil.github.io/video-intelligence-api-visualiser/">https://zackakil.github.io/video-intelligence-api-visualiser/</a></figcaption></figure><p>A powerful video analyse API on Google Cloud Platform. It uses machine learning to detect what is in a video e.g faces, people, objects, textÂ etc.</p><p>If you want to see all that it can do you can check out <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">this interactive demo</a>. Also check out the docsÂ <a href="https://cloud.google.com/video-intelligence">here</a>.</p><h4>Transcoder API</h4><p>A scalable cloud based video transcoding API on Google Cloud Platform that lets you perform complex video transcoding jobs. Some of itsâ€™ features includeÂ :</p><ul><li>generating different bit rates andÂ formats</li><li>generating thumbnails</li><li>inserting overlays</li><li>inserting adÂ breaks</li></ul><p>see more details on the API in <a href="https://cloud.google.com/transcoder/docs/concepts/overview">theÂ docs</a>.</p><h4>Connecting theseÂ together</h4><p>The system pipeline uses the <a href="https://cloud.google.com/video-intelligence/docs/analyze-labels">Label Detection feature</a> of the Video Intelligence API (see visualisation of this feature <a href="https://zackakil.github.io/video-intelligence-api-visualiser/#Label%20Detection">here</a>) to detect whatâ€™s in each scene of the video and return a list of labels with time segments. If any of those labels match the phobia we are scared of (e.g snakes, swans, bread, birds) we then use the Transcoder API to inject a <a href="https://cloud.google.com/transcoder/docs/how-to/create-overlays">full screen overlay</a> on the video for those time segments, hiding them fromÂ view.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FibGeuf-QOPQUwCfwrjtLw.png" /><figcaption>high level APIÂ pipeline</figcaption></figure><p>To get these APIâ€™s to run in a scalable way I used a chain of <a href="https://cloud.google.com/storage">Cloud Storage Buckets</a> and <a href="https://cloud.google.com/functions/docs/calling/storage">triggered Cloud Functions</a> that automatically run when a new video is uploaded.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*41kbfziVlWuSArvwB-6icw.png" /><figcaption>exact tech stack including storage andÂ triggers</figcaption></figure><h3>Tricky challenge</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/408/1*hAJhh5bF3zzMVUOIPEUpZA.png" /></figure><p>In order to tell the system what my phobia/phobias were without some fancy UI I came up with the idea of just using the â€˜<em>see no evil</em>â€™ emoji ğŸ™ˆ in the uploaded file name followed by the phobia. Then get the cloud function code to read the filename and extract the phobia from that. See the specific code in the <a href="https://github.com/ZackAkil/phobia-safe-videos-with-ml">githubÂ repo</a>.</p><h3>Output</h3><p>Finally I would just drag and drop a scary video into the input storage bucket with a filename like <a href="https://console.cloud.google.com/storage/browser/_details/scary-videos/test_nature%F0%9F%99%88swan.mp4?project=phobia-safe-video-player&amp;supportedpurview=project">test_natureğŸ™ˆswan.mp4</a>Â â€¦</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*JAJTrpj1pYr6-F2Z" /><figcaption>original scary video (containing swans)</figcaption></figure><p>â€¦ and in less time that it would take to watch the video the Video Intelligence API has already scanned the video and the Transcoder API has already generated the new non-scary video. Check out the<a href="https://youtu.be/IWPclIcRCrM?t=23"> I/O video</a> â˜ï¸ that demo segment was all realtime!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*KDPicz5wkVyKvD24" /><figcaption>generated phobia safe video (hidingÂ swans)</figcaption></figure><h3>Future work</h3><p>In the future I want to use the same kind of pipeline to solve video processing problems surrounding sports analysis and sports video production e.g <em>highlights generation</em>. You can see the kind of things I do with sport in this episode of <a href="https://twitter.com/dalequark"><strong>Dale Markowitz</strong></a>â€™s Making with MLÂ series:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FyLrOy2Xedgk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyLrOy2Xedgk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FyLrOy2Xedgk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/25c04956d43e7829875470e071585770/href">https://medium.com/media/25c04956d43e7829875470e071585770/href</a></iframe><h3>Takeaways</h3><p>Building ML powered video production pipeline need only take you plugging together a couple ofÂ APIâ€™s.</p><p><a href="https://github.com/ZackAkil/phobia-safe-videos-with-ml">GitHub - ZackAkil/phobia-safe-videos-with-ml: How you can use ML to make watching videos safer for people with unique and/or serious phobias.</a></p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d57a2051f533" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[See what Video Intelligence API can do with this visualisation tool]]></title>
            <link>https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/4303e371505</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud]]></category>
            <category><![CDATA[machine-vision]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Mon, 26 Jul 2021 12:53:43 GMT</pubDate>
            <atom:updated>2021-07-26T12:55:28.042Z</atom:updated>
            <content:encoded><![CDATA[<p>I built a <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">visualiser</a> for the <a href="https://cloud.google.com/video-intelligence">Google Cloud Video intelligence API</a> that allows anybody to explore all of the features of theÂ API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*wGtCPL_11IkYbect3il1RQ.gif" /><figcaption><a href="https://zackakil.github.io/video-intelligence-api-visualiser/">https://zackakil.github.io/video-intelligence-api-visualiser/</a></figcaption></figure><p>If youâ€™re working with video, then the Video Intelligence API has a lot of powerful features for analysing and detecting whatâ€™s in your videos, for small pet projects or massive scale applications.</p><p>Here is a list of the features of the API along with some example use cases for those features (see <a href="https://cloud.google.com/video-intelligence/docs/features">the docs</a> for moreÂ info):</p><ul><li><strong>Label Detection</strong> <em>(</em><a href="https://youtu.be/IWPclIcRCrM?t=26"><em>phobia detection</em></a><em>)</em></li><li><strong>Shot Detection</strong> <em>(detect ad break segments)</em></li><li><strong>Object Tracking</strong><em> (traffic counting)</em></li><li><strong>Person Detection</strong> <em>(</em><a href="https://www.youtube.com/watch?v=yLrOy2Xedgk"><em>sports player analysis</em></a><em>)</em></li><li><strong>Face Detection</strong> <em>(analyse eye contact withÂ camera)</em></li><li><strong>Logo Recognition</strong><em> (product placement detection)</em></li><li><strong>Speech Transcriptio</strong>n <em>(generate interactive subtitles)</em></li><li><strong>Text Detection</strong> <em>(search videos of presentations based on text inÂ slides)</em></li><li><strong>Explicit Content Detection</strong> <em>(automated content monitoring)</em></li><li><strong>Celebrity Detection*</strong></li></ul><p><em>*restricted access and not shown in visualiser</em></p><h3>Analyse and visualise your ownÂ videos</h3><p>You can follow <a href="https://cloud.google.com/video-intelligence/docs/quickstart?utm_source=ext&amp;utm_medium=partner&amp;utm_campaign=CDR_zac_aiml_vid_intel_demo_interactive%20demo_060221&amp;utm_content=-">this tutorial</a> in order to analyse your own videos. You can also use this script I made to quickly analyse videos stored in Google CloudÂ Storage:</p><p><a href="https://github.com/ZackAkil/video-intelligence-util/blob/main/video_intel_util.py">video-intelligence-util/video_intel_util.py at main Â· ZackAkil/video-intelligence-util</a></p><p>After you have analysed your video you can drag the outputÂ <strong>.json </strong>along with the original video into the <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">visualiser</a> to visualise theÂ results.</p><p>Hope this tools helps you explore and work with the Video Intelligence API! Please reach out to me with any questions,</p><p>Happy video analysing!</p><p><a href="https://twitter.com/ZackAkil">JavaScript is not available.</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4303e371505" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zombies & Model Rot (with ML Engine + DataStore)]]></title>
            <link>https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/747b299526e9</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[zombies]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Wed, 24 Oct 2018 14:48:41 GMT</pubDate>
            <atom:updated>2018-10-24T14:48:41.431Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/592/1*uWwW72D4mHiZHOJYJa-Ctg.png" /><figcaption>Donâ€™t leave your models to rot into obscurity</figcaption></figure><p>So youâ€™ve deployed your machine learning model to the cloud and all of your apps and services are able to fetch predictions from it, nice! You can leave that model alone to do its thing foreverâ€¦ maybe not. Most machine learning models are modeling something about this world, and this world is constantly changing. Either change with it, or be leftÂ behind!</p><h3>What is modelÂ rot?</h3><p>Model rot, data rot, AI rot, whatever you want to call it, itâ€™s not good! Letâ€™s say weâ€™ve built a model that predicts if a zombie is friendly or not. We deploy it to the cloud and now apps all over the world are using it to help the general public know which zombies they can befriend without getting bitten. Amazing, people seem super happy with your model, but after a couple of months you start getting angry emails from people who say that your model is terrible! Turns out that the zombie population mutated! Now your model is out of date, or <strong>rotten</strong>! You need to update your model, and even better, add in a way to keep track of your models state of rot so that this doesnâ€™t happenÂ again.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/696/1*AW5ZhXxEa8OV-pVc908C8w.png" /><figcaption>This is an example of a very sudden case of modelÂ rot!</figcaption></figure><h3>Itâ€™s not just zombies thatÂ change</h3><p>Sure, fictional creatures can change, but so can financial markets, residential environments, traffic patterns, weather patterns, the way people write tweets, the way cats look! Ok maybe cats will always look like cats (although give it a few million years and maybe not). The point is that depending on what your models are predicting will effect how fast they are going toÂ rot.</p><p>Itâ€™s also important to note that the thing you are predicting doesnâ€™t need to change for your model to rot. Maybe the sensor you are using to capture input data gets changed. Anything that negatively effects the performance of your model when itâ€™s deployed is effectually causing model rot, either remove the thing causing the reduced performance or update the model (most likely going to be the latterÂ choice).</p><h3>Letâ€™s fight model rot (with ML Engine + DataStore)</h3><p>Thereâ€™s a zombie outbreak, however itâ€™s not as scary as the movies would lead you to believe. They are pretty slow moving creatures, and a lot of them are just looking for human friends, but some arenâ€™t. To help people make the right choice of zombie friends we developed a model that predicts if a zombie is friendly or not based on a few characteristics:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*F8Dby00KywP6Z29Xj5x7vQ.png" /></figure><p>We used Scikit-Learn to build a Decision Tree Classifier model. See the <a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor.ipynb"><strong>notebook here</strong></a> to see the exact code to doÂ this.</p><p>The plan is now to deploy our model to <a href="https://cloud.google.com/ml-engine/">ML Engine</a> which will host our model for us on the cloud. (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Deploying_a_Sk_Learn_model_to_ML_Engine.ipynb"><em>see how we do this using gcloud commands in a notebook</em></a>)</p><p>First weâ€™ll throw our model into cloudÂ storage:</p><pre>gsutil cp model.joblib gs://your-storage-bucket/v1/model.joblib</pre><p>Then create a new ML Engine model, which you can do using the Google Cloud Console UI or using <strong>gcloud</strong> commands (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Deploying_a_Sk_Learn_model_to_ML_Engine.ipynb"><em>which you can see here used in a notebook</em></a>):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZF3DVx-l5CL92Drwg0L9TQ.png" /><figcaption>ML EngineÂ UI</figcaption></figure><p>Then we deploy our Decision Tree model as versionÂ 1:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0sHZ1Xwom_e3hImLqOhZyQ.png" /><figcaption>Creating a new version of your model using the Cloud ConsoleÂ UI</figcaption></figure><p>To make it easier for apps to fetch predictions from our model, weâ€™ll create a public endpoint using Cloud Functions. You can read more about how to do this in <a href="https://cloud.google.com/blog/products/ai-machine-learning/simplifying-ml-predictions-with-google-cloud-functions"><strong>this blog post</strong></a> I wrote with my colleague <a href="https://twitter.com/SRobTweets"><strong>Sara Robinson</strong></a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1O-GzUlURbvYYyRcQ6XreA.png" /><figcaption>hereâ€™s the current architecture of ourÂ system</figcaption></figure><p>OK, our model is deployed and apps can easily fetch predictions from it! Now to monitor for model rot with DataStore!</p><h3>DataStore?</h3><p>Imagine a place in the cloud where you can store millions of python dictionaries quickly, and then query them (also quickly). Thatâ€™s <a href="https://cloud.google.com/datastore/">DataStore</a>, a fully managed NoSQL database on Google Cloud Platform. If you have previous experience with databases you might be used to carefully planning out exactly what structure of data to store in tables, then experience the pain of creating migration scripts to update the structure of your database. Non of that nonsense with DataStore, want to store the following data:</p><pre>{<br>  &quot;name&quot;: &quot;Alex&quot;<br>  &quot;occupation&quot;: &quot;Zombr trainer&quot;<br>}</pre><p>then do it (<em>using the python client library</em>):</p><pre># create new entity/row<br>new_person = datastore.Entity(key=client.key(&#39;person&#39;))</pre><pre>new_person[&#39;name&#39;] = &#39;Alex&#39;<br>new_person[&#39;occupation&#39;] = &#39;Zombr trainer&#39;</pre><pre># save to datastore<br>client.put(new_person)</pre><p>Oh wait, you want to start storing peoples githubs and twitters? go forÂ it:</p><pre># create new entity/row<br>new_person = datastore.Entity(key=client.key(&#39;person&#39;))</pre><pre>new_person[&#39;name&#39;] = &#39;Zack&#39;<br>new_person[&#39;occupation&#39;] = &#39;Zombr CEO&#39;<br>new_person[&#39;github&#39;] = &#39;<a href="https://github.com/zackakil">https://github.com/zackakil</a>&#39;<br>new_person[&#39;twitter&#39;] = &#39;<a href="https://twitter.com/zackakil">@zackakil</a>&#39;</pre><pre># save to datastore<br>client.put(new_person)</pre><p>and DataStore will say â€œthankÂ youâ€:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*m2cUpreghQwk8Wl75Pjnrw.png" /><figcaption>DataStoreâ€™s UI</figcaption></figure><h3>Using DataStore to collect modelÂ feedback</h3><p>The feedback data that we are going to collect will look like the following:</p><pre>{<br>&quot;model&quot;: &quot;v1&quot;,<br>&quot;prediction input&quot;: [2.1, 1.4, 5.3, 8.0],<br>&quot;prediction output&quot;: 1,<br>&quot;was correct&quot;: False,<br>&quot;time&quot;: &quot;23-10-2018,14:45:23&quot;<br>}</pre><p>This data will tell us what version of our model on ML Engine was used to generate the prediction (<strong>model</strong>), what the input data for the prediction was (<strong>prediction input</strong>), what the prediction made by the model was (<strong>prediction output</strong>), if the prediction was correct (the actual feedback from the user) (<strong>was correct</strong>), and the time that the feedback was submitted (<strong>time</strong>).</p><p>Weâ€™ll use Cloud Functions again to make another web API endpoint, this time to receive the feedback data and store it in DataStore:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/href">https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/href</a></iframe><p>Now our system architecture looks like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LEYpFJCmZ3294kR_fMibqQ.png" /><figcaption>the new architecture of ourÂ system</figcaption></figure><p>The client apps just need to add in a intuitive way for the users to submit their feedback. In our case it could be a simple â€˜thumbs up or thumbs downâ€™ prompt after the user is presented with a prediction:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hW0dAlgij1W1fdR_J7WTg.png" /><figcaption>You may have come across feedback prompts like thisÂ before</figcaption></figure><h3>Get creative with how you collectÂ feedback</h3><p>Often times you can infer feedback about your model, rather than explicitly requesting it from the users like Iâ€™ve done in the <strong>Zombr </strong>interface<strong>. </strong>For example if we see that a user stops using the app immediately after a prediction, we could use that data to indicate a wrong prediction ğŸ˜¬.</p><p>Back in reality, a dog adoption agency might have a recommender system for new owners. The rate of successful adoptions made by the model is itâ€™s own performance feedback. If the agency suddenly sees that the system is making a lot fewer successful matches than usual, then they can use that as the indication that the model is rotten and may need updating.</p><h3>Feedback is collected, nowÂ what?</h3><p>Now we can analyse the feedback data. For any data analyse work I default to using Jupyter Notebooks.</p><p><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb">Click here for the full notebook of how I fetch data from DataStore and analyse the feedback</a>.</p><p>The important bits of fetching data from DataStore are, first installing the DataStore python clientÂ library:</p><pre>pip install google-cloud-datastore</pre><p>then you can import it and connect to DataStore:</p><pre><strong>from</strong> google.cloud <strong>import</strong> datastore</pre><pre><em># connect to DataStore</em><br><strong>client = datastore.Client(&#39;your project id&#39;)</strong></pre><pre><em># query for all prediction-feedback items</em> <br><strong>query = client.query(kind=&#39;prediction-feedback&#39;) <br> </strong><br><em># order by the time field</em> <br><strong>query.order = [&#39;time&#39;] </strong><br> <br><em># fetch the items <br># (returns an iterator so we will empty it into a list)</em> <br><strong>data = list(query.fetch())</strong></pre><p>The library with automatically convert all of the data into python dictionaries:</p><pre>print(data[0][&#39;was correct&#39;])<br>print(data[0][&#39;model&#39;])<br>print(data[0][&#39;time&#39;])<br>print(data[0][&#39;input data&#39;])</pre><pre>&gt;&gt;&gt; True<br>&gt;&gt;&gt; v1<br>&gt;&gt;&gt; 2018-10-22 14:21:02.199917+00:00<br>&gt;&gt;&gt; [-0.8300105114555543, 0.3990742221560673, 1.9084475908892906, 0.3804372006233603]</pre><p>Thanks to us saving a â€œwas correctâ€ boolean in our feedback data we can easily calculate the accuracy of our model from the feedback by looking at the ratio of â€˜<strong>Truesâ€™ </strong>for thisÂ field:</p><pre>number_of_items = len(data)<br>number_of_was_correct = len([d <strong>for</strong> d <strong>in</strong> data <strong>if</strong> d[&#39;was correct&#39;]])</pre><pre>print(number_of_was_correct / number_of_items)</pre><pre>&gt;&gt;&gt; 0.84</pre><p>0.84 is not much rot since we <a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor.ipynb">first trained our model</a> which scored ~0.9 accuracy, but thatâ€™s calculated using all of the feedback data together. What if we do this same accuracy calculation on a sliding window across our data and plot it? (<em>you can see the code for doing this in the </em><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb"><em>analysis notebook</em></a>)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/501/1*fsLb8wHZohYfFdzwRytfyA.png" /></figure><p>Thatâ€™s a big drop in performance for the most recent feedback.</p><p>We should investigate further. Letâ€™s compare the input data (i.e the zombie characteristic data) from the times of high accuracy to the times of low accuracy. Good thing we also collected that in our feedback:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/479/1*epRqaSvi6ZFhjgflekeGSw.png" /><figcaption>blue = correct prediction, red = incorrect prediction</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/479/1*u0LnadB81REpsSRmGjdjCw.png" /><figcaption>blue = correct prediction, red = incorrect prediction</figcaption></figure><p>Ah, the data looks completely different. I guess the zombie population has mutated! We need to retrain our model ASAP with new data. Good thing we collected the input data in the feedback, we can use that as the new training data (saves us having to manually collect new data). We can use information about the prediction the model made (â€œpredictionâ€ field) and the usersâ€™ feedback (â€œwas correctâ€ field) to infer the correct prediction label for the new trainingÂ data:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/href">https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/href</a></iframe><p><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb">See how this code is used in the bottom of the feedback analysis notebook</a>.</p><p>With this new data set we can train a new version of our model. This is an identical process to training the initial model but using a different data set (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor_re_train.ipynb">see the notebook</a>), and then uploading it to ML Engine as a new version of theÂ model.</p><p>Once itâ€™s on ML Engine you can either set it as the new default version of the zombies model so that all of your clients will automatically start having their prediction requests sent to the new model, or you can instruct your clients to specify the version name in their prediction requests:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PjAjfJd5k4S6-JCfYFNHTw.png" /><figcaption>setting v2 as the defaultÂ model</figcaption></figure><p>If you set the default model to v2 then all prediction request to â€œzombiesâ€ will go to the v2Â version:</p><pre>PREDICTION REQUEST BODY:<br>{<br>&quot;instances&quot;:[[2.0, 3.4, 5.1, 1.0]],<br>&quot;model&quot;:&quot;zombies&quot;<br>}</pre><p>or your clients can just be more specific:</p><pre>PREDICTION REQUEST BODY:<br>{<br>&quot;instances&quot;:[[2.0, 3.4, 5.1, 1.0]],<br>&quot;model&quot;:&quot;zombies/versions/v2&quot;<br>}</pre><p>After all that you can sit back and just run the same analysis after some more feedback has been collected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/688/1*paMZjeQqTQUgzXJekmXWXA.png" /><figcaption>seems like people find our v2 modelÂ helpful</figcaption></figure><p>Hopefully this has given you a few ideas on how you can monitor your deployed models for model rot. All of the code used can be found in the <a href="http://ZackAkil/rotting-zombie-model">githubÂ repo</a>:</p><p><a href="https://github.com/ZackAkil/rotting-zombie-model">ZackAkil/rotting-zombie-model</a></p><p>Reach out to me <a href="https://twitter.com/zackakil">@ZackAkil</a> with any thoughts/questions on monitoring modelÂ rot.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=747b299526e9" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9">Zombies &amp; Model Rot (with ML Engine + DataStore)</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TensorFlow & reflective tapeÂ : why Iâ€™m bad at basketball ]]></title>
            <link>https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/a30a923332de</guid>
            <category><![CDATA[basketball]]></category>
            <category><![CDATA[tensorflow]]></category>
            <category><![CDATA[sports]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Fri, 05 Oct 2018 14:00:27 GMT</pubDate>
            <atom:updated>2018-10-05T21:22:21.016Z</atom:updated>
            <content:encoded><![CDATA[<h3>TensorFlow &amp; reflective tape ğŸ€ (am I bad at basketball?)</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J1rVK_jZ_Lodt7fimRJ3FQ.jpeg" /></figure><p>Recently a friend got me into basketball. Turns out, itâ€™s a lot harder than it looks. No matter, I can over engineer a solution using machine learning. If your into ML and shooting hoops then thereâ€™s also <a href="https://medium.com/tensorflow/tf-jam-shooting-hoops-with-machine-learning-7a96e1236c32">this article</a> that combined TensorFlow and basketball in a simulation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*V1eZtAdA1sUfqX7WNNIClg.gif" /><figcaption>nothing but netâ€¦ if there was aÂ net</figcaption></figure><p>The task is to find the exact angle of my shots. Then I can hopefully use that information in a proactive way to getÂ better.</p><p><em>psst! the code for all of this is on myÂ </em><a href="https://github.com/ZackAkil/optimising-basketball"><em>Github</em></a></p><h3>Task 1: collecting data</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w3-DOoutfsTxgGyxyF36tA.jpeg" /><figcaption>I didnâ€™t need to follow the seems of the ball, but it looksÂ cool</figcaption></figure><p>I donâ€™t have access to 3D tracking studios fitted with 200 cameras, but I do have Ebay. Itâ€™s quite easy to buy <a href="https://www.google.com/search?q=reflective+tape">reflective tape online</a> and stick it to the ball. Then (thanks to the lack of lighting at my local court) I can record some footage of me practicing in the evening and capture the balls movements.</p><p>The torch build into my phone provides the perfect light source to bounce off the reflective tap of theÂ ball.</p><p>As a result the footage captured shows a sparkly object flying through a mostly dark scene, perfect for doing some image manipulation inÂ python.</p><h3>Task 2: Getting our video intoÂ python</h3><p>Firstly I do everything in Python, and a really easy way to import video into Python is to use a library called â€˜<a href="http://www.scikit-video.org">scikit-video</a>â€™, so I installed that:</p><pre>pip install scikit-video</pre><p>and then used it to load in my video as aÂ matrix:</p><pre>from skvideo.io import vread</pre><pre>video_data = vread(&#39;VID_20180930_193148_2.mp4&#39;)</pre><p>The shape (that you can find by running <strong>video_data.shape</strong>) of this data is (220, 1080, 1920, 3). Which means 220 frames of 1080x1920 pixels of 3 channels of colour (red, green,Â blue):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*1tn3tKyuCMNidLyQillYZw.gif" /><figcaption>raw videoÂ data</figcaption></figure><h3>Task 3: Extracting the shot (image processing)</h3><p>So I want to get the data on just the balls movement. Fortunately, itâ€™s one of the only things moving in the video. So I can do my favourite video processing trick: <strong>delta frame extraction</strong>! (<em>thatâ€™s what I call it, but thereâ€™s probably another name forÂ it</em>).</p><p>By subtracting all of the pixel values in one frame from all of the pixel values in the next frame you will be left with non-zero values in just the pixels that haveÂ changed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/417/1*KeA9dwEEeuXqNE7AVKb8uw.png" /><figcaption>calculating delta frame in order to isolate movingÂ pixels</figcaption></figure><p>Cool cool cool, now I do that for each frame in the video and combine the result into oneÂ image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/490/1*tYmopdxUS4b0iiJ5QwQJ8w.png" /><figcaption>adding together all of the delta frames from the videoÂ sequence</figcaption></figure><p>Next is extracting the shot data into a usable format. So weâ€™ll covert the pixel values that are lite up into a list of <strong><em>x</em></strong> and <strong><em>y</em></strong> points. The code to do this is a numpy function called <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html"><strong>numpy.where</strong></a> which will find all of the values that are <strong>True</strong> in an array and return their indices (i.e their positions in theÂ matrix).</p><p>But before we do that, weâ€™ll quickly crop out just the balls trajectory and flip the data so that it starts at the origin (the bottom left of theÂ scene):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/850/1*wlWVcZEObJePb1zEzeNN1Q.png" /></figure><p>and the resulting image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1009/1*OxS6pId6J1aMXM7rlthu5w.png" /></figure><p>Notice how it still seems upside down? Thatâ€™s only because images tend to be drawn starting at the <strong>top</strong> left corner (note the axis numbering). When we convert the pixels to data points and draw them on a normal graph they will get drawn starting at the <strong>bottom</strong> leftÂ corner.</p><p>now we run our <strong>numpy.where</strong> code to get the pixels as dataÂ points:</p><pre>pixels_that_are_not_black = cropped_trail.sum(axis=2) &gt; 0</pre><pre>y_data, x_data = numpy.where(pixels_that_are_not_black)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/1*JFCUAmNTHhqWUB3lUyXDCg.png" /><figcaption>awesome! our relatively clean ball trajectory data</figcaption></figure><h3>Task 4: Build TensorFlow model</h3><p>This is where TensorFlow shines. You may be used to hearing about using TensorFlow for building neural networks, but you can define almost any mathematical formula and tell it to optimise whatever parts of it you want. In our case we will use the formula for a trajectory which we know from primary school toÂ be:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*5VB-_Rbf_mxycVAzxtY2fA.png" /><figcaption>extremely mathematical equation of trajectory that I foundÂ online</figcaption></figure><p><strong>Î¸</strong> (theta) is the angle of the shot (<em>the value we really careÂ about</em>)</p><p><strong><em>v</em></strong> is the initialÂ velocity</p><p><strong><em>g</em> </strong>is gravityÂ (9.8m/s)</p><p><strong><em>x </em></strong>is the horizontal position (<em>data we haveÂ already</em>)</p><p><strong><em>y</em></strong> is the vertical position (<em>data we haveÂ already</em>)</p><p>A far more interesting way to see the equation in action is to play around with <a href="https://www.desmos.com/calculator/gjnco6mzjo">this trajectory tool</a>.</p><p>We can use the ball trail of my shot as the <strong><em>x</em></strong> and <strong><em>y </em></strong>of the<strong><em> </em></strong>equation and task TensorFlow with finding the correct angle (<strong>Î¸</strong>) and initial velocity (<strong><em>v</em></strong>) that fits my shots <strong><em>x</em></strong> and <strong><em>y</em></strong>Â data:</p><p>Weâ€™ll start be recreating our trajectory equation in TensorFlow:</p><p>first tell it what data we will feed it when we run the optimisation:</p><pre>x = tf.placeholder(tf.float32, [<strong>None</strong>, 1])<br>y = tf.placeholder(tf.float32, [<strong>None</strong>, 1])</pre><p>next tell it what variables we want it to tweak and tune in order to fit the trajectory curve to ourÂ data:</p><pre>angle_variable = tf.Variable(40.0, name=&#39;angle_variable&#39;)<br>force_variable = tf.Variable(100.0, name=&#39;force_variable&#39;)<br><br>gravity_constant = tf.constant(9.8, name=&#39;gravity_constant&#39;)</pre><p>and join all of these together (<strong>warning</strong>: itâ€™s going to look quite messy, but itâ€™s just the maths equation seen before written in TensorFlow syntax):</p><pre>left_hand_side = x * tf.tan(deg2rad(angle_variable))<br>top = gravity_constant * x ** 2<br>bottom = (2*(force_variable)**2) * <br>            (tf.cos(deg2rad(angle_variable))**2)</pre><pre>output = left_hand_side - (top / bottom)</pre><p>then tell TensorFlow how to tell if itâ€™s doing a good job or not at fitting the trajectory function to ourÂ data:</p><pre># the lower this score, the better<br>error_score = tf.losses.mean_squared_error(y, output)</pre><p>create an optimiser that will do the actual tweaking of variables (angle_variable and force_variable) in order to reduce the error_score:</p><pre>optimiser = tf.train.AdamOptimizer(learning_rate=5) <br>optimiser_op = optimiser.minimize(error_score)</pre><h3>Task 5Â :Â Magic</h3><p>We can now run the optimisation task to find the <strong>angle_variable</strong> and <strong>force_variable</strong> values that fit myÂ shot.</p><pre>sess = tf.Session()<br>sess.run(tf.global_variables_initializer())</pre><pre># do 150 steps of optimisation<br>for i in range(150):</pre><pre>    sess.run([optimiser_op], <br>             feed_dict={x: np.array(x_data).reshape(-1, 1), <br>                        y: np.array(y_data).reshape(-1, 1)})</pre><pre>found_angle = sess.run(angle_constant.value())</pre><pre>print(found_angle)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/504/1*4DTRp298z0G0LHxFp3WWGA.gif" /><figcaption>TensorFlow finding the angle of myÂ shot</figcaption></figure><p>At the end of that optimisation we find out that the trajectory function that best fits my shot data has an angle of ~61Â°â€¦ not sure what to do with that informationâ€¦ I guess I could look at what professional shooting angles are for comparisonâ€¦ to be continued.</p><blockquote>The lesson to take away: you can always distract your-self with completely unnecessary (but fun) machine learning.</blockquote><p>All of the code I used is available on myÂ Github:</p><p><a href="https://github.com/ZackAkil/optimising-basketball">ZackAkil/optimising-basketball</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a30a923332de" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de">TensorFlow &amp; reflective tapeÂ : why Iâ€™m bad at basketball ğŸ€</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Finally learn how to use command line appsâ€¦ by making one!]]></title>
            <link>https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/bd5cf21a15cd</guid>
            <category><![CDATA[beginner]]></category>
            <category><![CDATA[command-line]]></category>
            <category><![CDATA[linux]]></category>
            <category><![CDATA[bash]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Mon, 21 May 2018 21:07:45 GMT</pubDate>
            <atom:updated>2018-05-22T23:15:06.221Z</atom:updated>
            <content:encoded><![CDATA[<p>At the risk of alienating a lot of readersâ€¦ I grew up with GUIâ€™s, so I never needed to learn the ways of the terminal! This is socially acceptable in todays society of friendly user interfaces, except maybe if youâ€™re in software engineeringâ€¦ whoops!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/455/1*cyhNkd0jd7Zd3zBDLaKpSg.png" /><figcaption>Managerâ€Šâ€”â€Šâ€œOh this is easy, just use this command line appâ€, Meâ€Šâ€”â€Šâ€Yesâ€¦the command lineâ€¦ Iâ€™ll useÂ thatâ€¦â€</figcaption></figure><h3>Getting back my software engineering streetÂ credâ€™</h3><p>Iâ€™ve gotten pretty far just through copy and pasting full command-line operations from Stack-Overflow but have never become comfortable enough to use a command line app â€œproperlyâ€. What finally caused it to click for me was when I stumbled across a very handy python library called <a href="https://docs.python.org/3/library/argparse.html"><strong>argparse</strong></a><strong> </strong>that allows you to build a nice robust command line interface for your pythonÂ scripts.</p><p><a href="https://docs.python.org/3.5/howto/argparse.html">This tutorial</a> is great in-depth explanation about how to use <strong>argparse, </strong>but Iâ€™ll go over the key eyeÂ openers<strong>:</strong></p><blockquote><a href="https://docs.python.org/3/library/argparse.html">argparse</a> is part of the standard python library so pop open your code editor and follow along (you donâ€™t need to install anything)!</blockquote><h3><strong>HELP</strong></h3><p>The most useful part of every command lineÂ app!</p><pre># inside a file called my_app.py</pre><pre><strong>import</strong> <strong>argparse</strong><br>parser = argparse.ArgumentParser(description=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.parse_args()</pre><p>The code above will do nothing, except by default you have the <strong>helpÂ </strong>flag!</p><p>In the command line you canÂ run:</p><pre>python my_app.py --help</pre><p>or</p><pre>python my_app.py -h</pre><p>and youâ€™ll get an output likeÂ this:</p><pre>usage: my_app.py [-h]<br></pre><pre><strong>Nice little CL app!</strong><br></pre><pre>optional arguments:</pre><pre>-h, --help  <strong>show this help message and exit</strong></pre><p>Seems pretty cool right? but wait, when you use <strong>argparse </strong>to<strong> </strong>add more functions (see below) this <strong>help</strong> output will automatically fill up with all of the instructions of how you can use yourÂ app!</p><h3>REQUIRED ARGUMENTS</h3><p>Lets say you want to have your app take in some variable, we just use the parser.add_argument() function and give our argument some label (in this caseÂ â€œnameâ€):</p><pre>import argparse<br>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br></pre><pre>parser.add_argument(&quot;<strong><em>name</em></strong>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)<br></pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what?</em> &quot; + args<strong>.name</strong>)</pre><p>Notice how we added the <strong>help </strong>text! Now when we run python my_app.y -h we get all of the appÂ details:</p><pre>usage: my_app.py [-h] name<br></pre><pre><strong>Nice little CL app!</strong><br></pre><pre>positional arguments:</pre><pre>name        <strong>Just your name, nothing special</strong><br></pre><pre>optional arguments:</pre><pre>-h, --help  <strong>show this help message and exit</strong></pre><p>Pretty cool, but letâ€™s run our app with python my_app.py</p><pre>usage: my_app.py [-h] name</pre><pre>my_app.py: error: too few arguments</pre><p>Thatâ€™s right! Automatic input checking!</p><p>Now lets run python my_app.py &quot;SlimÂ Shady&quot;</p><pre>Your name is what? Slim Shady</pre><p>Pretty slick!</p><h3>OPTIONAL ARGUMENTS</h3><p>Maybe you want to give someone the option of telling you little more about themselves? Adding a double dash when using parser.add_argument() function will make that argument optional!</p><pre>import argparse<br>parser = argparse.ArgumentParser(<strong>description</strong>=â€<em>Nice little CL app!</em>â€)<br>parser.add_argument(â€œ<em>name</em>â€, <strong>help</strong>=â€<em>Just your name, nothing special</em>â€)<br></pre><pre>parser.add_argument(&quot;<strong><em>--</em></strong><em>profession</em>â€, <strong>help</strong>=â€Y<em>our nobel profession</em>â€)<br></pre><pre>args = parser.parse_args()<br>print(â€œ<em>Your name is what? </em>â€œ + args.name)</pre><pre>if args.profession:<br>    print(â€œ<em>What is your profession!? a </em>â€œ + args.profession)</pre><p>If you want to pass in a variable for that argument you just need to specify the double-dashed argument name before the variable you want toÂ pass:</p><pre>python my_app.py &quot;Slim Shady&quot; <strong>--profession &quot;gift wrapper&quot;</strong></pre><p>which gives youÂ :</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper</pre><p>Or you donâ€™t have to, it is optional afterÂ all!</p><pre>python my_app.py &quot;Slim Shady&quot;</pre><p>still givesÂ you:</p><pre>Your name is what? Slim Shady</pre><p>and the magic once again when running python my_app.py -hÂ :</p><pre>usage: my_app.py [-h] [--profession PROFESSION] name<br></pre><pre><strong>Nice little CL app!<br></strong></pre><pre>positional arguments:</pre><pre>name                      <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                <strong>show this help message and exit</strong></pre><pre>--profession PROFESSION   <strong>Your nobel profession</strong></pre><h3>FLAGS</h3><p>Maybe you just want to enable something cool to happen. Add action=&quot;<em>store_true</em>&quot; to your parser.add_argument() function and you have yourself a <strong>flag</strong> argumentÂ :</p><pre>import argparse</pre><pre>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.add_argument(&quot;<em>name</em>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)<br>parser.add_argument(&quot;<em>--profession</em>&quot;, <strong>help</strong>=&quot;<em>Your nobel profession</em>&quot;)<br></pre><pre>parser.add_argument(&quot;<em>--cool</em>&quot;, <strong>action</strong>=&quot;<em>store_true</em>&quot;, <strong>help</strong>=&quot;<em>Add a little cool</em>&quot;)<br></pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what? </em>&quot; + args.name)</pre><pre>cool_addition = &quot;<em> and dragon tamer</em>&quot; if args.cool else &quot;&quot;</pre><pre>if args.profession:<br>    print(&quot;<em>What is your profession!? a </em>&quot; + args.profession + cool_addition)</pre><p>Itâ€™s pretty neat, you just plop the <strong>flag</strong> name into your command likeÂ so:</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot; <strong>--cool</strong></pre><p>and presto!</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper and dragon tamer</pre><p>and remember, you donâ€™t have to use it, itâ€™s just aÂ flag:</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot;</pre><p>will stillÂ give:</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper</pre><p>look though at the <strong>help </strong>command python my_app.py -hÂ :</p><pre>usage: my_app.py [-h] [--profession PROFESSION] [--cool] name<br></pre><pre><strong>Nice little CL app!<br></strong></pre><pre>positional arguments:</pre><pre>name                      <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                <strong>show this help message and exit</strong></pre><pre>--profession PROFESSION   <strong>Your nobel profession</strong></pre><pre>--cool                    <strong>Add a little cool</strong></pre><p>Iâ€™m just going to assume youâ€™re as satisfied with that as I am from nowÂ on.</p><h3>SHORT FORM</h3><p>Unveiling the mysterious one character arguments that confused me for so long. Just by adding a single letter prefixed with a single dash to your parser.add_argument() functions you have super short versions of the same arguments:</p><pre>import argparse</pre><pre>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.add_argument(&quot;<em>name</em>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)</pre><pre>parser.add_argument(&quot;-p&quot;, &quot;<em>--profession</em>&quot;, <strong>help</strong>=&quot;<em>Your nobel profession</em>&quot;)</pre><pre>parser.add_argument(&quot;-c&quot;, &quot;<em>--cool</em>&quot;, <strong>action</strong>=&quot;<em>store_true</em>&quot;, <strong>help</strong>=&quot;<em>Add a little cool</em>&quot;)</pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what? </em>&quot; + args.name)<br>cool_addition = &quot;<em> and dragon tamer</em>&quot; if args.cool else &quot;&quot;<br>if args.profession:<br>    print(&quot;<em>What is your profession!? a </em>&quot; + args.profession + cool_addition)</pre><p>So that instead of typing inÂ :</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot; --cool</pre><p>you can justÂ type:</p><pre>python my_app.py &quot;Slim Shady&quot; -p &quot;gift wrapper&quot; -c</pre><p>and youâ€™ll get the sameÂ output:</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper and dragon tamer</pre><p>And this is reflected in the help text (python my_app.py -hÂ ):</p><pre>usage: my_app.py [-h] [--profession PROFESSION] [--cool] name</pre><pre><strong>Nice little CL app!</strong></pre><pre>positional arguments:</pre><pre>name                           <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                      <strong>show this help message and exit</strong></pre><pre>-p PROFESSION, --profession PROFESSION   <strong>Your nobel profession</strong></pre><pre>-c, --cool                      <strong>Add a little cool</strong></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*OYyJ-v1Ao09QEhv71MW_Gg.jpeg" /><figcaption>a perfect little command lineÂ app!</figcaption></figure><p>You now have a perfect little command line app and are hopefully more comfortable with finding your way around command lineÂ apps!</p><p>Just remember --helpÂ !</p><p><a href="https://github.com/ZackAkil/super-simple-command-line-app">ZackAkil/super-simple-command-line-app</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd5cf21a15cd" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd">Finally learn how to use command line appsâ€¦ by making one!</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>