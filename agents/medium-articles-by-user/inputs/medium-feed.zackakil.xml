<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Zack Akil on Medium]]></title>
        <description><![CDATA[Stories by Zack Akil on Medium]]></description>
        <link>https://medium.com/@zackakil?source=rss-5b7ea6e5016a------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/1*fKWeN3MkK_yBAEXYAPGvjA.jpeg</url>
            <title>Stories by Zack Akil on Medium</title>
            <link>https://medium.com/@zackakil?source=rss-5b7ea6e5016a------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 14:45:35 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@zackakil/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Hobbies to Hired: 5 tips for making great Portfolio Projects to Land Your First Tech Job]]></title>
            <link>https://medium.com/@zackakil/hobbies-to-hired-5-tips-for-making-great-portfolio-projects-to-land-your-first-tech-job-8c45aaeafd06?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/8c45aaeafd06</guid>
            <category><![CDATA[portfolio]]></category>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[junior-developer]]></category>
            <category><![CDATA[tech]]></category>
            <category><![CDATA[jobs]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Sun, 14 Apr 2024 00:07:10 GMT</pubDate>
            <atom:updated>2024-04-15T18:43:33.585Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rO_yOG2uhV-dw4Q4FQwHcg.jpeg" /></figure><p>When you starting off in tech, building strong portfolio projects is an essential step in showcasing your skills and impressing potential employers. However, figuring out what projects to create and how to make them shine can feel overwhelming. This guide will break down 5 actionable tips to help you start building projects that you’ll be excited to show off!</p><h3>1. Go “off trail”</h3><p>Especially for those coming from bootcamps, just having projects that were “<strong><em>part of the curriculum</em></strong>” in your portfolio shows to the interviewer that you can follow instructions in a safe environment. However in the wild those conditions are rare. You need projects that show you ability to (<strong><em>independently)</em></strong> <strong>IDENTIFY</strong> a problem, then (<strong><em>independently)</em></strong> <strong>LEARN</strong> something new that can be used to solve that problem, and of course successfully (<strong><em>independently)</em></strong> <strong>APPLY</strong> that learning to solve the problem.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Cd-gE6QDB3f9V1bZ7Nf3hA.jpeg" /></figure><p>When telling the story of these “off trail projects” a good structure is :</p><ol><li>I saw this problem.</li><li>I learnt about this technology that could be used to solve the problem.</li><li>I built this thing using what I learnt to solve the problem.</li></ol><p>When hiring juniors employers main concern is that they will have to provide extensive hand-holding. These kind of projects directly address that concern by <strong>showing </strong>that you can take ownership of your learning rather than always having to rely on explicit instructions.</p><p>So back yourself and go “<strong>off trail</strong>”!</p><h4>Key tip about your first “off trail projects”</h4><p><strong>One new thing at a time</strong>: It’s good practice focus on one new skill at a time with each new project. This helps keep the complexity manageable, allowing you to apply your existing knowledge while stretching your abilities with your new learnings. <em>E.g</em> <em>I might want to learn about Tensorflow.js and also d3js, and I already know how to make HTML webpages. So I’ll make a project were I apply my existing HTML knowledge to my new Tensorflow.js knowledge in some app. Then I’ll do a separate project were I build something with d3js in a HTML app. Then finally another separate project that combines everything! That’s 3 unique projects that can go my portfolio!</em></p><h3>2. The Enthusiasm hack</h3><p>People want to work with enthusiastic people that do exciting work. As a junior developer, enthusiasm should be one of your strengths. But, how do you get interviewers enthusiastic about your portfolio projects?</p><p>First off, <strong>YOU</strong> have to be genuinely enthusiastic by your portfolio projects!</p><p>Do that by working on projects that genuinely excite you. Classic patterns for identifying projects that elicit genuine enthusiasm are:</p><ul><li>🤩 Projects based on your hobbies.</li><li>🤩 Project that solve a real problem that you genuinely face.</li><li>🤩 Projects that solve a real problem for people you genuinely care about.</li></ul><p>Projects that are harder to elicit genuine enthusiasm for are:</p><ul><li>🥱 Any project you “had to do” / “were told to do” e.g assigned coursework.</li><li>🥱 Projects that solve hypothetical problems for hypothetical people. (due to their lack of an authentic story)</li></ul><p>Come interview time you’ll find it a lot easier to speak enthusiastically about your work, and that genuine enthusiasm is infectious. So lean into, let yourself geek out and exude that energy that embodies the excitement of being a junior developer!</p><p>An example of a portfolio projects of mine that I was able to easy get enthusiastic about was my <a href="https://www.youtube.com/watch?v=H2UNijCq-v4">rugby recording robot</a>. Without any more context, you can probably guess what sport I like.</p><h4>Additional perks of working on intrinsically interesting projects</h4><ol><li>It will help keep your engaged in the project when things get challenging.</li><li>You are are going to be more inclined to talk about the project with those around you. This gives you free practice in your technical communication skills.</li><li>Often times they will be projects that are naturally unique to you. Great portfolio projects are the one’s <strong>only you</strong> could have made. Their uniqueness embody a part of your personality e.g <em>an app that was inspired by your hobby and the tech skills you want to become proficient in.</em></li></ol><h3>3. “Many and Smaller” vs “Fewer and Bigger”</h3><p>You probably have an idea for an app in your head (or are currently building one) that is a bit of a beast e.g multi-social user login, full admin CRUD, email notifications, payment system, plus whatever the USP is etc.</p><p>As a portfolio project these kind of projects have merit in giving you holistic experience in every part of application development. However, with one big project you get to setup your repo… once, your get you design your UX… once, you get to identify a problem spec… once. Vs with many smaller projects were you get to go through the whole <strong>Problem -&gt; Idea -&gt; Design -&gt; Implementation</strong> process multiple times starting from a blank slate.</p><p>This is the same principle as the “<a href="https://medium.com/illumination/quality-is-the-destination-quantity-is-the-path-and-photos-are-the-unexpected-proof-b131347e178e">Photography Quantity vs Quality” story</a>. Give yourself many opportunities to try things, learn from them, and take those learnings to the next project, and repeat as many times as possible.</p><p>So the algorithm to a fruitful portfolio journey is :</p><pre>knowledge = None<br>while(1): <br>  learnings = build_project(size=&quot;small&quot;, applied_knowledge=knowledge)<br>  knowledge += learnings</pre><h3><strong>4. Make it instantly interactive </strong>🪄</h3><p>Having interviewed many devs straight out of bootcamps, the first thing I check is their github and go to their latest (or pinned) project. For web devs it’s usually a website, cool… but where is it? Oh I have to first clone it then do “npm run” yadayadayada “localhost”… sigh!</p><p>Don’t make interviewers work to see your work. If you’ve made a website, have it deployed somewhere (e.g <a href="https://pages.github.com/">Github pages</a>) <strong>and put the link at the top of the projects’ README</strong>. Did you make some cool python stuff? have a link to a <a href="https://colab.research.google.com/">Colab</a> or <a href="https://streamlit.io/">Sreamlit app</a> <strong>and put the link at the top of the projects’ README</strong>.</p><p>If there is literally no way to make your work instantly interactive then at least have some kind of videos or gifs embedded in the README. Have some way to <strong>instantly</strong> <strong>show</strong> your work.</p><h3>5. Put a bow on it 🎀</h3><p>You’ve probably put days/weeks of time into your project. However you’ve probably put zero time into the first thing someone will see when looking at your project… the <strong>README</strong>!</p><p>Take 15 minutes to really spruce up your projects README with nice formatting, links, images, gifs etc. The README is your projects (and likely you as a developers’) first impression, so make it one that shows you care about communication and presentation of your work. Check out this list by <a href="https://twitter.com/matiassingers">@matiassingers</a> for inspiration : <a href="https://github.com/matiassingers/awesome-readme?tab=readme-ov-file">awesome READMEs</a></p><h3>Bonus tip (especially when applying to larger companies like Google)</h3><p>At Google there is a specific emphasis put on <strong>knowledge sharing</strong>. Showcasing that you not only add value as an individual, but also elevate those around you.</p><p>Showcasing your enthusiasm for sharing knowledge by creating resources that help others. Whether it’s visualization tools or blog posts, sharing your insights and resources can make a powerful impression.</p><h3>Conclusion</h3><p>So, there you have it: five (+1) actionable tips to for building great portfolio projects. By venturing “off trail,” embracing your enthusiasm, building many smaller projects, making your work instantly interactive, and putting a bow on it with a polished README, you’ll create a portfolio that not only showcases your skills but also tells a compelling story about you as a developer.</p><p>Remember the tech world is constantly evolving, and the best developers are those who never stop learning. Use your portfolio as a tool for growth, pushing your boundaries and showcasing your ability to adapt and master new skills. Good luck on your journey!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8c45aaeafd06" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to Identify Innovative Ideas with the Technical Horizon Framework]]></title>
            <link>https://medium.com/@zackakil/how-to-identify-innovative-ideas-with-the-technical-horizon-framework-f7c9c613682e?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/f7c9c613682e</guid>
            <category><![CDATA[technology]]></category>
            <category><![CDATA[innovation]]></category>
            <category><![CDATA[ideas]]></category>
            <category><![CDATA[management]]></category>
            <category><![CDATA[ai]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Sat, 12 Aug 2023 15:52:26 GMT</pubDate>
            <atom:updated>2023-08-12T15:52:26.534Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*WEqQPfG9khm8JEQAheVRqw.png" /></figure><p>“<strong><em>It’s been done before!</em></strong>” or “<strong><em>It’s not possible!</em></strong>”, the last thing you want to hear when you pitch your new idea. What if you could quickly and pragmatically identify better, more innovative ideas that are actually feasible? Enter the “Technical Horizon Framework”.</p><h3>What are Technical Horizons?</h3><p>As technology advances with time, what is technically feasible to <em>consumers</em>, <em>developers</em>, and <em>researchers</em> is constantly expanding, these are the <strong>Technical Horizons</strong>. The key points about them are:</p><ol><li>Technical Horizons are always expanding as technology advances.</li><li>Ideas are static and are defined by the <strong>simplest technology required</strong> to build them.</li><li>Every idea within your Technical Horizon is technically feasible.</li><li>Consumers can build ideas within the <em>Consumer</em> Technical Horizon, developers can build ideas within the <em>Developer</em> <strong>and </strong><em>Consumer</em><strong> </strong>Technical Horizon, and researchers can build ideas within the <em>Researcher</em>, <em>Developer</em> <strong>and </strong><em>Consumer</em> Technical Horizon.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*WEqQPfG9khm8JEQAheVRqw.png" /></figure><p>The Technical Horizon chart above shows:</p><ul><li>Plotting data (e.g charts in spreadsheets) is easy for consumers i.e very feasible.</li><li>Image Generation is newly feasible to consumers (as of 2023) through consumer apps.</li><li>Video classification is very feasible to developers with tools like Googles <a href="https://cloud.google.com/video-intelligence">Video Intelligence API</a>, but would be infeasible for consumers without developer knowledge.</li><li>Video summarisation is a newly feasible idea for developers by combining multiple APIs like the previously mentioned Video Intelligence API and <a href="https://developers.generativeai.google/">Googles PaLM API</a>.</li><li>Video clip generation is currently not feasible without research level knowledge however in the near future APIs to do this may be available.</li><li>Full movie generation would only be feasible with a lot of research time.</li></ul><h3>Where are your innovative ideas?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/902/1*2fNDQ5rsFEJsbdKzNscXoQ.png" /></figure><p>You can use the Technical Horizon Framework to identify which of your ideas are “innovative”. Simply by looking to the immediate inside edge of <strong>your</strong> Technical Horizon. If you are a consumer, Image Generation is an innovative idea that is good for consumers to explore.</p><p>As a developer; Video summarisation is currently an innovative space. The reason the space on the immediate inside of your Technical Horizons is innovative, is because in this space there are ideas that have never been done before; <strong>because they could never have been done before</strong> due to the enabling technology only recently becoming available.</p><h3>What about ideas outside of your Technical Horizon?</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/676/1*VOIioOr4stjIQnPGFJ1vDg.png" /></figure><h4><strong>Almost feasible ideas</strong></h4><p>Assume your a developer and you’ve come up with an idea were the APIs aren’t quite there yet (e.g Video clip generation), it’s just outside your Technical Horizon, or “<strong><em>almost feasible</em></strong>”<strong><em>. </em></strong>You have a few options:</p><ul><li>Park the idea and wait for the tools to become available, and spend your time on more feasible ideas.</li><li>Or, embark on building the idea anyway, but risk wasting your developer resources as at any time an API could be released that nudges the Developer Technical Horizon forward, potentially making the idea buildable with a few lines of code, and rendering your work potentially redundant. What’s especially dangerous in this scenario is you (or your team) are at risk of “<a href="https://www.google.co.uk/search?q=sunk-cost+fallacy">sunk-cost fallacy</a>” if an API does come out after you’ve already invested time into your own implementation when using someone else&#39;s solution might be better.</li></ul><h4>Technical Horizons are invisible</h4><p>The challenge here is the <strong>unknown</strong>. When will the developer tools be ready? A day, month, year? This is were having good channels for up to date news about new tools is valuable. Also being a part of early access programs and going to conferences lets you peek into the future of where your Technical Horizon is going.</p><h4>Very infeasible ideas (Moonshots)</h4><p>Keep track of them, they might be infeasible now, but Technical Horizons can leap forward suddenly, like it has with the surge of LLMs (Large Language Model) technology.</p><p>If waiting isn’t an option, a word for these high risk - high reward ideas is a “Moonshot”. Accept that it’s a bold undertaking so the pay off needs to be worth the risk.</p><h3>Watch out for “Shoehorn Innovation”</h3><p>“<em>My idea is innovative because it’s using the latest technology!</em>”, a<br>cliché when any new technology is released.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/839/1*4Mmyz0dZ-M3eNi-tyrvbHQ.jpeg" /></figure><p>The “innovativeness” of an idea is not defined by the technology it uses, but by the actual technical requirements of the problem it’s solving. An innovative idea is solving a problem that <strong>can only be solved</strong> with new technology, so ask the question “Can this problem already be solved with an existing technology?”, a hard but important pill to swallow sometimes.</p><h3>Wrap up</h3><p>When you find an idea that is genuinely newly feasible with a new technology, and it’s solving a problem that could never have been solve before, then you know you have an innovative idea.</p><h4>Key takeaways</h4><ol><li>A truly innovative idea is one that solves a problem that could not previously be solved.</li><li>Listen out for news about new technology that could push your Technical Horizon forward. “What can I do now do that I couldn’t before”, is a good question to be asking about any new technology you hear about.</li><li>Beware of “Shoehorn Innovation.” Just because a new technology is available doesn’t mean that you need to use it in your idea. Don’t just use new technology for the sake of using it.</li></ol><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f7c9c613682e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How I conquered writer's block with Google Assistant + Google Docs]]></title>
            <link>https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/e327d18b52b</guid>
            <category><![CDATA[writing]]></category>
            <category><![CDATA[google-docs]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[productivity]]></category>
            <category><![CDATA[google-assistant]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Fri, 30 Jul 2021 15:51:57 GMT</pubDate>
            <atom:updated>2021-07-30T18:26:55.370Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VRQzx-1P5Qu7uRzJ_dDorQ.png" /></figure><p>Over lockdown I built a voice app that would interview me about my projects and help write articles for me. What would usually take me weeks, now takes me an hour or two, and as a testament to that I wrote one article a day for a week and lived to tell the tale:</p><p>Day 1</p><p><a href="https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505">See what Video Intelligence API can do with this visualisation tool</a></p><p>Day 2</p><p><a href="https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533">🎥 🙈 Using AI to automate phobia safe film production</a></p><p>Day 3</p><p><a href="https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d">🎥 🏉 AppSheet + AppScript for sports videography highlights tracking</a></p><p>Day 4</p><p><a href="https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9">🚴 Safer cycling with ML and Edge TPU’s</a></p><p>Day 5 (<em>this article </em>🤯)</p><p><a href="https://medium.com/@zackakil/how-i-conquered-writers-block-with-google-assistant-google-docs-e327d18b52b">How I conquered writer&#39;s block with Google Assistant + Google Docs</a></p><h3>Background</h3><p>Writing is something I’ve always struggled with, and in my role as a Developer Advocate it can be an important part of the job to make and share content about what I’ve built in order to inspire and educate fellow developers. It wouldn’t be uncommon for me to take longer writing an article than it did for me to build the thing I was writing about.</p><p>One time I had built something and someone reached out to interview me about my project so that they could write an article on my behalf. Obviously I found that experience a lot easier. So that gave me an idea; what if I could build some voice AI that would interview me about my projects and generate the articles for me.</p><h3>Solution</h3><p>Using <a href="https://developers.google.com/assistant/console/builder">Google Action Builder</a> I built a Google Assistant app that would ask me a set of questions about a single project and then output what I said into a generated Google Doc.</p><p>I followed this tutorial to learn how to build an action:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FZ1hxvniJ18s%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DZ1hxvniJ18s&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FZ1hxvniJ18s%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/d61d324ae46d344aa227912458dc9e14/href">https://medium.com/media/d61d324ae46d344aa227912458dc9e14/href</a></iframe><p>Within the Assistant app I have a custom webhook that is a Google Cloud Function that takes what was said and writes it into a new Google Doc using the <a href="https://developers.google.com/docs/api/quickstart/python">Google Docs python client API</a>. See the code for that Cloud Function on my github:</p><p><a href="https://github.com/ZackAkil/project-reporter-voice-app">GitHub - ZackAkil/project-reporter-voice-app: A voice app that interview me about my projects in order to generate articles.</a></p><p>The first step is always the hardest so even though it’s essentially just a dictation app with some prompting questions, it makes that initial draft phrase of article writing virtually zero friction for me. I never have to see that horrible immovable empty document again when writing articles. Here it is in action:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FuRJAftUEuW8%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DuRJAftUEuW8&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FuRJAftUEuW8%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="640" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/href">https://medium.com/media/78867c0dcdbb19a370b8a6e4f516ded8/href</a></iframe><h3>Tricky challenges</h3><h4>Authentication! Authentication! Authentication!</h4><p>Getting Google Assistant to talk directly to Google Docs is especially tricky as you have to navigate a merade of authentication services in order to create and write docs on the users’ behalf. I created <a href="https://docs.google.com/presentation/d/11aLNFEqMWUiKYbeh3MdWrSMw-pEKQQ4PmUL0mhbC5x4/edit?usp=sharing\">this slide deck</a> to loosely document the process in case you’d like to do something similar, but be warned it is a lot of throwing around of api keys, client secrets, and refresh tokens. <strong>But it can be done!</strong></p><h4>Speak quick and concise (look to using Dialogflow)</h4><p>Google Assistant apps have a 30 second response limit so I have to speak quite quickly, and as soon as there is a pause in my voice it cuts the mic and takes that as my answer. For my first version of this app I’m OK with that, however for a more natural interview experience where you can take your time and collect your thoughts before you answer I’d look into using Dialogflow which allows for more granulare configuration and longer talking response times. Check out <a href="https://twitter.com/ladysign"><strong>Lee Boonstra</strong></a>’s I/O talk to learn more about Dialogflow:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FO7JfSF3CJ84%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DO7JfSF3CJ84&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FO7JfSF3CJ84%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/href">https://medium.com/media/89c0d9cfd112bc2503918cf6404108e3/href</a></iframe><h4>Remembering the Doc ID</h4><p>Because I was using a Cloud Function as my webhook, there was no guarantee that the exact same instance of that Cloud Function would be called during the same conversation (<em>one of the tricky things with serverless tech</em>). So I used Cloud Firestore as a storage medium to store which conversation IDs (<em>which are passed to every webhook call</em>) map to which generated Google Doc IDs.</p><h3>Future</h3><p>As an extra step in the article writing process it would be cool to have some machine learning text generation automatically generate tweets based on what I said about my project.</p><p>Also the interview process is currently just a static set list of questions. I could use some natural language ML tools like the <a href="https://cloud.google.com/natural-language">Natural Language API</a> or <a href="https://cloud.google.com/natural-language/automl/docs">AutoML Natural Language</a> to do some intelligent analysis into my responses in order to adapt the follow up questions like in a real interview.</p><h3>Takeaways</h3><p>Leverage your strengths (<em>for me it’s building things</em>) to overcome your weaknesses (<em>for me it’s writing things</em>).</p><p>You can get Google Assistant to talk directly to Google Docs (<em>and the rest of G Suite</em>), it’s just a fair bit of authentication loops you have to jump through.</p><p>It can be a lot easier to say something than to write something so be aware of voice technologies like <a href="https://developers.google.com/assistant">Google Assistant</a> and <a href="https://cloud.google.com/dialogflow">Dialogflow</a>.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e327d18b52b" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ Safer cycling with ML and Edge TPU’s]]></title>
            <link>https://medium.com/@zackakil/safer-cycling-with-ml-and-edge-tpus-c4883a82dae9?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/c4883a82dae9</guid>
            <category><![CDATA[raspberry-pi]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[cycling]]></category>
            <category><![CDATA[google]]></category>
            <category><![CDATA[edge-computing]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Thu, 29 Jul 2021 13:33:19 GMT</pubDate>
            <atom:updated>2021-07-29T13:39:54.411Z</atom:updated>
            <content:encoded><![CDATA[<p>A while ago I tried to make my bike smarter with machine learning to help keep me safe whilst commuting through busy London streets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9-QqkNPrP9lB7SQ3vzGZfA.png" /></figure><h3>Background</h3><p>If you live in a big city and cycle you probably have developed superhuman reflexes and awareness, especially in cities like London where a lot of the narrow roads were originally designed for horse and cart.</p><p>After enough close calls and some actual calls I figured it was time to try and solve this problem with a modern approach (<em>without mirrors</em>).</p><h3>Solution</h3><p>Using a small computer (<em>Raspberry Pi</em>) attached to the bike. It would run an object detection model on images being taken from a rear facing camera. The system would detect the different potential dangers behind and intuitively relay that information back to the rider (<em>via a handlebar mounted LED strip</em>) so that they can maintain their focus on what&#39;s ahead.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/907/1*J0IQG8Bx_h9TGEIUYk1bNA.png" /><figcaption>what an object detection model would detect</figcaption></figure><h4>Need for speed</h4><p>With just the Raspberry Pi running the object detection model it was able to make predictions at a rate of about 2 predictions per second, which for this use case is on the slow side. That is where the <a href="https://coral.ai/">Coral Edge TPU</a> comes in. It is able to run the same model at 30–60 predictions per second giving the system the super low latency it needs. You can find out more about Coral Edge TPU’s from this Google I/O session:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FnWExsl6iAxA&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DnWExsl6iAxA&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FnWExsl6iAxA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/7400903711405d60c51ba4a89e582f1c/href">https://medium.com/media/7400903711405d60c51ba4a89e582f1c/href</a></iframe><p>Specifically I used the <a href="https://coral.ai/products/accelerator">Edge TPU USB Accelerator</a> and had it and the Raspberry Pi both attached to the bike:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/980/1*j1woMnVtKe1-SD5Gjybuvg.png" /><figcaption>prototype showcasing the Coral Edge TPU usb accelerator</figcaption></figure><p>After the Raspberry Pi + Edge TPU made it’s prediction of what&#39;s behind the bike, it would then send commands to an RGB LED strip on the bikes’ handlebars to then indicate to the rider where the dangers from behind are, and what level of danger those things are (<em>because it’s able to distinguish between the different types of vehicle e.g car, bus, truck, bike, pedestrian</em>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4d4wVG7EaaO-aE9C.jpeg" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/677/0*RdcmfLmp9WXkJN-O" /><figcaption>prototype and real bike version</figcaption></figure><h4>The specific model</h4><p>Due to this being a pretty generic prediction task (<em>traffic object detection</em>), there were already pretrained models I could download and use that already detected different types of vehicles. In fact I ended up using the exact same model that Bill used in his 2019 Google I/O demo:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FJgm25QdF90A%3Fstart%3D1789%26feature%3Doembed%26start%3D1789&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJgm25QdF90A&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FJgm25QdF90A%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/href">https://medium.com/media/d34f85b39693e866b1d23d70b42874a5/href</a></iframe><p>You can explore different kinds of pretrained models on the Coral website:</p><p><a href="https://coral.ai/models/">Models | Coral</a></p><p>You can also find all the code for this project on my github:</p><p><a href="https://github.com/ZackAkil/edge-TPU-safe-bike">GitHub - ZackAkil/edge-TPU-safe-bike: An application of realtime object-detection running on an Edge TPU for making cycling in busy cities a little less terrifying.</a></p><h3>Interesting points</h3><h4>Australia has it own dangers</h4><p>When I presented this idea in Sydney there were discussions around if a specific version of this could be made for their specific dangers. At the time I assumed they might have been talking about some cliche dangers when you think of Australia… kangaroos, snakes, drop bears. But actually for cyclists in Australia it’s teritorial magpies that are <a href="https://www.google.com/search?q=australia+magpie+cyclist">constantly attacking</a>. So a custom version that included swooping birds in the model would be perfect for them.</p><h4>Pretrained models not designed for toy cars</h4><p>It’s important to test exactly how you’re going to use pretrained models. When I was demoing this idea on stage I hadn’t thoroughly tested how well it worked with toy cars and I got suboptimal results. This is where training custom models with tools like <a href="https://cloud.google.com/vision/automl/object-detection/docs">AutoML Object Detection</a> would shine. <em>Also interesting to note that my toy version of this demo worked a lot better when I included a printed out fake road for the toy cars to sit on, see in the previous gif above </em>👆.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FUmDeKDgFRj4%3Fstart%3D1699%26feature%3Doembed%26start%3D1699&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DUmDeKDgFRj4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FUmDeKDgFRj4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/href">https://medium.com/media/064b56e52cacdd823bf9643a6d49c391/href</a></iframe><h3>Future</h3><p>I want to train a custom model that would be trained specifically on London traffic (<em>including the distinctive taxis</em>) and then I could look into training other custom models for different regions such as an Australian magpie edition. Oh, and actually make this on a real bike that I can cycle (<em>not just a prototype</em>).</p><h3>Takeaway</h3><p>It’s good to be aware of technology like Edge TPU’s in case you have a problem that requires super low latency and mobility. Also always checking to see if there are pretrained models that could already solve your problem and save you a huge amount of development time.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c4883a82dae9" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[  AppSheet + AppScript for sports videography highlights tracking]]></title>
            <link>https://medium.com/@zackakil/appsheet-appscript-for-painless-sport-videography-1b40fdf9a47d?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/1b40fdf9a47d</guid>
            <category><![CDATA[productivity]]></category>
            <category><![CDATA[appsheet]]></category>
            <category><![CDATA[app-script]]></category>
            <category><![CDATA[sports]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Wed, 28 Jul 2021 14:59:12 GMT</pubDate>
            <atom:updated>2021-07-28T15:42:31.973Z</atom:updated>
            <content:encoded><![CDATA[<p>During the summer I was asked to help with some video recording for a rugby tournament, but rather than spending hours rewatching footage to find highlights, I quickly threw together some tech to make things a lot easier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Oh6iX2qwulAu63rcv3pejA.png" /></figure><h3>Background</h3><p>Sport videography is a hobby of mine (<em>specifically rugby</em>). It is however very tedious work rewatching full matches in order to find the highlights. That&#39;s why when I was asked to help a friend film an entire rugby tournament this summer I was hesitant. Rewatching a full day’s worth of rugby videos is not something I wanted either of us to have to do.</p><p>There are some cameras that have a “<strong>mark special moment</strong>” button that will let you set a marker of when something cool happens, but we were using multiple cameras that didn’t have that feature. We discussed a simple paper based solution where we just noted down the time of when cool things happened. This was the solution we settled on until the morning of the event which is when I discovered a technology that would make things a lot easier.</p><h3>Solution</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*c9NZx_vB5qs01clp3NFodg.png" /><figcaption>original spreadsheet</figcaption></figure><p>For the data capture/entry we used <a href="https://www.appsheet.com/">AppSheet</a>, which is a no code app builder that integrates seamlessly into Google Sheets (<em>as well as other things</em>). As long as you have your spreadsheet set up with the right data formats for your columns, AppSheet can generate a clean and robust data entry UI that feeds data directly back into the spreadsheet.</p><p>I found out about AppSheet from this video by <a href="https://twitter.com/carterthecomic"><strong>Carter Morgan</strong></a><strong> </strong>👇:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FDjAD81A9nYk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DDjAD81A9nYk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FDjAD81A9nYk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/href">https://medium.com/media/3853ff39e38ba0c6b4656165fa0fce6c/href</a></iframe><p>Thanks to Carter’s intro I was able to build the perfect data entry app on the same morning of the tournament that would fit my needs (and was even able to style the colour theme of it), and both my friend and I were able to access it from a <a href="https://play.google.com/store/apps/details?id=x1Trackmaster.x1Trackmaster&amp;hl=en_GB&amp;gl=US">native Android App</a> on the field!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mE0iDvtEh3sfes1fPpl2Vg.gif" /><figcaption>generated AppSheet app</figcaption></figure><p>Throughout the day, anytime something cool happened whilst the cameras were rolling I just put in a note using the app, and because of how I set it up; the app knew to use the current time as the default value.</p><h4>After the tournament was over</h4><p>When it came to finding the actual clips that contained the highlights after the tournament, this is where <a href="https://developers.google.com/apps-script">AppScript</a> came into play (<em>I know AppScript and AppSheet are annoying similar names</em>). AppScript is a technology that lets you build automation within GSuit e.g Google Drive, Sheets, Docs, etc. Check out this video for a quick overview 👇</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FAs3keJaPmmk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DAs3keJaPmmk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FAs3keJaPmmk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/72a41db491bd122711fb457c959c30e7/href">https://medium.com/media/72a41db491bd122711fb457c959c30e7/href</a></iframe><p>We first uploaded all of the video clips from the tournament into Google Drive. Then within the Google Sheet that contained all of our inputted highlight timestamps I wrote a AppScript that would loop through each row and:</p><ol><li>extract out the time data for that specific highlight</li><li>scan through the file metadata of all the videos in a specified Google Drive folder to find the video that overlapped with that time</li><li>automatically paste in a link to that video file back into the spreadsheet</li></ol><p>The AppScript that does this is attached to the Google Sheet so that I get a nice toolbar button to trigger the script. When the script is triggered it prompts me to specify the Google Drive folder url containing the tournament videos to search through. Watch it in action 👇</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/1*OzJQdhWJwhZkz_F_0IzHtQ.gif" /><figcaption>the automated clip finding process using my script</figcaption></figure><p>You can see all the AppScript code in my github:</p><p><a href="https://github.com/ZackAkil/gsuit-sports-videography-tool/blob/main/script.gs">gsuit-sports-videography-tool/script.gs at main · ZackAkil/gsuit-sports-videography-tool</a></p><p>and here’s a visual overview of the whole system:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G40tztSeU_YnkrWeFZY0CA.png" /><figcaption>high level diagram of the entire solution</figcaption></figure><h3>Tricky challenges</h3><h4>Camera clocks</h4><p>When I first ran the script to find the matching clips they were all wrong. Turns out the system clock of the camera was wrong 🤦‍♂️. So I added a quick and dirty bit of code to the script to offset the times.</p><h4>‘File created’ time vs ‘File updated’ time</h4><p>When I was writing the code to fetch the video metadata from Google Drive I thought that the file.getDateCreated() method from the <a href="https://developers.google.com/apps-script/reference/drive/file">AppScript Google Drive API</a> would tell me when the video was captured. Turns out that method just returns when the file was created/uploaded “<strong>in Google Drive</strong>”. This had me panicked for a while thinking the project was dead as I could not extract the lower level file metadata, but turns out file.getLastUpdated() will tell you the last time the file was edited which in our case was the time the video was captured on the camera.</p><h3>Future</h3><p>Now that we have timestamps of highlights along with the specific video clips we can look into training machine learning models that could automatically detect highlights in videos. This could be done with a tool like <a href="https://cloud.google.com/video-intelligence/automl/docs">AutoML Video Intelligence</a> which can train powerful video models without us writing any machine learning code.</p><h3>Takeaway</h3><p>The time it takes to build an app is <strong>not correlated</strong> to how useful that app can be. Tools like AppSheet take minutes to use and when the right moment comes for them they can save you literally days of work. Combine that with scripting tools like AppScript and you have a powerful problem solving duo.</p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1b40fdf9a47d" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[  Using AI to automate phobia safe film production]]></title>
            <link>https://medium.com/@zackakil/using-ai-to-automate-phobia-safe-film-production-d57a2051f533?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/d57a2051f533</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[machine-vision]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[video-production]]></category>
            <category><![CDATA[film]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Tue, 27 Jul 2021 12:58:37 GMT</pubDate>
            <atom:updated>2021-07-27T13:18:42.528Z</atom:updated>
            <content:encoded><![CDATA[<h3>🎥🙈 Using AI to automate phobia safe film production</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/429/1*4IJ3uqqnyf0hTp0zb6KMgA.png" /></figure><p>At Google I/O this year I used ML to build an automated video processing pipeline that detects what’s in a video and automatically hides the parts you would find scary.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FIWPclIcRCrM%3Fstart%3D23%26feature%3Doembed%26start%3D23&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DIWPclIcRCrM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FIWPclIcRCrM%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/href">https://medium.com/media/f09b58e89a28bb08562eac4e37fd4f27/href</a></iframe><h3>Background</h3><p>Whilst watching some movies with some friends, we discovered that one of our friends has a deathly fear snakes and so we had a tell them exactly when the snakes were going to appear , and sometimes we got this wrong 😬 (<em>watching</em> <em>Harry Potter was a poor choice in hindsight</em>).</p><h3>Solution</h3><p>I built an automated video processing pipeline using <a href="https://cloud.google.com/video-intelligence">Video Intelligence API</a> that would scan through a video to detect all the things within the video. Then if it detects something that matches with the phobia that we told the system about, it would use the <a href="https://cloud.google.com/transcoder/docs/concepts/overview">Transcoder API</a> to insert an overlay and hide it from view. So the solution was based on these two APIs:</p><h4>Video Intelligence API</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*JerXnfAbOCQNy_EX.gif" /><figcaption><a href="https://zackakil.github.io/video-intelligence-api-visualiser/">https://zackakil.github.io/video-intelligence-api-visualiser/</a></figcaption></figure><p>A powerful video analyse API on Google Cloud Platform. It uses machine learning to detect what is in a video e.g faces, people, objects, text etc.</p><p>If you want to see all that it can do you can check out <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">this interactive demo</a>. Also check out the docs <a href="https://cloud.google.com/video-intelligence">here</a>.</p><h4>Transcoder API</h4><p>A scalable cloud based video transcoding API on Google Cloud Platform that lets you perform complex video transcoding jobs. Some of its’ features include :</p><ul><li>generating different bit rates and formats</li><li>generating thumbnails</li><li>inserting overlays</li><li>inserting ad breaks</li></ul><p>see more details on the API in <a href="https://cloud.google.com/transcoder/docs/concepts/overview">the docs</a>.</p><h4>Connecting these together</h4><p>The system pipeline uses the <a href="https://cloud.google.com/video-intelligence/docs/analyze-labels">Label Detection feature</a> of the Video Intelligence API (see visualisation of this feature <a href="https://zackakil.github.io/video-intelligence-api-visualiser/#Label%20Detection">here</a>) to detect what’s in each scene of the video and return a list of labels with time segments. If any of those labels match the phobia we are scared of (e.g snakes, swans, bread, birds) we then use the Transcoder API to inject a <a href="https://cloud.google.com/transcoder/docs/how-to/create-overlays">full screen overlay</a> on the video for those time segments, hiding them from view.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FibGeuf-QOPQUwCfwrjtLw.png" /><figcaption>high level API pipeline</figcaption></figure><p>To get these API’s to run in a scalable way I used a chain of <a href="https://cloud.google.com/storage">Cloud Storage Buckets</a> and <a href="https://cloud.google.com/functions/docs/calling/storage">triggered Cloud Functions</a> that automatically run when a new video is uploaded.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*41kbfziVlWuSArvwB-6icw.png" /><figcaption>exact tech stack including storage and triggers</figcaption></figure><h3>Tricky challenge</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/408/1*hAJhh5bF3zzMVUOIPEUpZA.png" /></figure><p>In order to tell the system what my phobia/phobias were without some fancy UI I came up with the idea of just using the ‘<em>see no evil</em>’ emoji 🙈 in the uploaded file name followed by the phobia. Then get the cloud function code to read the filename and extract the phobia from that. See the specific code in the <a href="https://github.com/ZackAkil/phobia-safe-videos-with-ml">github repo</a>.</p><h3>Output</h3><p>Finally I would just drag and drop a scary video into the input storage bucket with a filename like <a href="https://console.cloud.google.com/storage/browser/_details/scary-videos/test_nature%F0%9F%99%88swan.mp4?project=phobia-safe-video-player&amp;supportedpurview=project">test_nature🙈swan.mp4</a> …</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*JAJTrpj1pYr6-F2Z" /><figcaption>original scary video (containing swans)</figcaption></figure><p>… and in less time that it would take to watch the video the Video Intelligence API has already scanned the video and the Transcoder API has already generated the new non-scary video. Check out the<a href="https://youtu.be/IWPclIcRCrM?t=23"> I/O video</a> ☝️ that demo segment was all realtime!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*KDPicz5wkVyKvD24" /><figcaption>generated phobia safe video (hiding swans)</figcaption></figure><h3>Future work</h3><p>In the future I want to use the same kind of pipeline to solve video processing problems surrounding sports analysis and sports video production e.g <em>highlights generation</em>. You can see the kind of things I do with sport in this episode of <a href="https://twitter.com/dalequark"><strong>Dale Markowitz</strong></a>’s Making with ML series:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FyLrOy2Xedgk%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyLrOy2Xedgk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FyLrOy2Xedgk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/25c04956d43e7829875470e071585770/href">https://medium.com/media/25c04956d43e7829875470e071585770/href</a></iframe><h3>Takeaways</h3><p>Building ML powered video production pipeline need only take you plugging together a couple of API’s.</p><p><a href="https://github.com/ZackAkil/phobia-safe-videos-with-ml">GitHub - ZackAkil/phobia-safe-videos-with-ml: How you can use ML to make watching videos safer for people with unique and/or serious phobias.</a></p><p>Follow and ask me questions on twitter <a href="https://twitter.com/ZackAkil">ZackAkil</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d57a2051f533" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[See what Video Intelligence API can do with this visualisation tool]]></title>
            <link>https://medium.com/@zackakil/see-what-video-intelligence-api-can-do-with-this-visualisation-tool-4303e371505?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/4303e371505</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[google-cloud]]></category>
            <category><![CDATA[machine-vision]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Mon, 26 Jul 2021 12:53:43 GMT</pubDate>
            <atom:updated>2021-07-26T12:55:28.042Z</atom:updated>
            <content:encoded><![CDATA[<p>I built a <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">visualiser</a> for the <a href="https://cloud.google.com/video-intelligence">Google Cloud Video intelligence API</a> that allows anybody to explore all of the features of the API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*wGtCPL_11IkYbect3il1RQ.gif" /><figcaption><a href="https://zackakil.github.io/video-intelligence-api-visualiser/">https://zackakil.github.io/video-intelligence-api-visualiser/</a></figcaption></figure><p>If you’re working with video, then the Video Intelligence API has a lot of powerful features for analysing and detecting what’s in your videos, for small pet projects or massive scale applications.</p><p>Here is a list of the features of the API along with some example use cases for those features (see <a href="https://cloud.google.com/video-intelligence/docs/features">the docs</a> for more info):</p><ul><li><strong>Label Detection</strong> <em>(</em><a href="https://youtu.be/IWPclIcRCrM?t=26"><em>phobia detection</em></a><em>)</em></li><li><strong>Shot Detection</strong> <em>(detect ad break segments)</em></li><li><strong>Object Tracking</strong><em> (traffic counting)</em></li><li><strong>Person Detection</strong> <em>(</em><a href="https://www.youtube.com/watch?v=yLrOy2Xedgk"><em>sports player analysis</em></a><em>)</em></li><li><strong>Face Detection</strong> <em>(analyse eye contact with camera)</em></li><li><strong>Logo Recognition</strong><em> (product placement detection)</em></li><li><strong>Speech Transcriptio</strong>n <em>(generate interactive subtitles)</em></li><li><strong>Text Detection</strong> <em>(search videos of presentations based on text in slides)</em></li><li><strong>Explicit Content Detection</strong> <em>(automated content monitoring)</em></li><li><strong>Celebrity Detection*</strong></li></ul><p><em>*restricted access and not shown in visualiser</em></p><h3>Analyse and visualise your own videos</h3><p>You can follow <a href="https://cloud.google.com/video-intelligence/docs/quickstart?utm_source=ext&amp;utm_medium=partner&amp;utm_campaign=CDR_zac_aiml_vid_intel_demo_interactive%20demo_060221&amp;utm_content=-">this tutorial</a> in order to analyse your own videos. You can also use this script I made to quickly analyse videos stored in Google Cloud Storage:</p><p><a href="https://github.com/ZackAkil/video-intelligence-util/blob/main/video_intel_util.py">video-intelligence-util/video_intel_util.py at main · ZackAkil/video-intelligence-util</a></p><p>After you have analysed your video you can drag the output <strong>.json </strong>along with the original video into the <a href="https://zackakil.github.io/video-intelligence-api-visualiser/">visualiser</a> to visualise the results.</p><p>Hope this tools helps you explore and work with the Video Intelligence API! Please reach out to me with any questions,</p><p>Happy video analysing!</p><p><a href="https://twitter.com/ZackAkil">JavaScript is not available.</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4303e371505" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zombies & Model Rot (with ML Engine + DataStore)]]></title>
            <link>https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/747b299526e9</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[cloud-computing]]></category>
            <category><![CDATA[zombies]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Wed, 24 Oct 2018 14:48:41 GMT</pubDate>
            <atom:updated>2018-10-24T14:48:41.431Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/592/1*uWwW72D4mHiZHOJYJa-Ctg.png" /><figcaption>Don’t leave your models to rot into obscurity</figcaption></figure><p>So you’ve deployed your machine learning model to the cloud and all of your apps and services are able to fetch predictions from it, nice! You can leave that model alone to do its thing forever… maybe not. Most machine learning models are modeling something about this world, and this world is constantly changing. Either change with it, or be left behind!</p><h3>What is model rot?</h3><p>Model rot, data rot, AI rot, whatever you want to call it, it’s not good! Let’s say we’ve built a model that predicts if a zombie is friendly or not. We deploy it to the cloud and now apps all over the world are using it to help the general public know which zombies they can befriend without getting bitten. Amazing, people seem super happy with your model, but after a couple of months you start getting angry emails from people who say that your model is terrible! Turns out that the zombie population mutated! Now your model is out of date, or <strong>rotten</strong>! You need to update your model, and even better, add in a way to keep track of your models state of rot so that this doesn’t happen again.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/696/1*AW5ZhXxEa8OV-pVc908C8w.png" /><figcaption>This is an example of a very sudden case of model rot!</figcaption></figure><h3>It’s not just zombies that change</h3><p>Sure, fictional creatures can change, but so can financial markets, residential environments, traffic patterns, weather patterns, the way people write tweets, the way cats look! Ok maybe cats will always look like cats (although give it a few million years and maybe not). The point is that depending on what your models are predicting will effect how fast they are going to rot.</p><p>It’s also important to note that the thing you are predicting doesn’t need to change for your model to rot. Maybe the sensor you are using to capture input data gets changed. Anything that negatively effects the performance of your model when it’s deployed is effectually causing model rot, either remove the thing causing the reduced performance or update the model (most likely going to be the latter choice).</p><h3>Let’s fight model rot (with ML Engine + DataStore)</h3><p>There’s a zombie outbreak, however it’s not as scary as the movies would lead you to believe. They are pretty slow moving creatures, and a lot of them are just looking for human friends, but some aren’t. To help people make the right choice of zombie friends we developed a model that predicts if a zombie is friendly or not based on a few characteristics:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/741/1*F8Dby00KywP6Z29Xj5x7vQ.png" /></figure><p>We used Scikit-Learn to build a Decision Tree Classifier model. See the <a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor.ipynb"><strong>notebook here</strong></a> to see the exact code to do this.</p><p>The plan is now to deploy our model to <a href="https://cloud.google.com/ml-engine/">ML Engine</a> which will host our model for us on the cloud. (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Deploying_a_Sk_Learn_model_to_ML_Engine.ipynb"><em>see how we do this using gcloud commands in a notebook</em></a>)</p><p>First we’ll throw our model into cloud storage:</p><pre>gsutil cp model.joblib gs://your-storage-bucket/v1/model.joblib</pre><p>Then create a new ML Engine model, which you can do using the Google Cloud Console UI or using <strong>gcloud</strong> commands (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Deploying_a_Sk_Learn_model_to_ML_Engine.ipynb"><em>which you can see here used in a notebook</em></a>):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZF3DVx-l5CL92Drwg0L9TQ.png" /><figcaption>ML Engine UI</figcaption></figure><p>Then we deploy our Decision Tree model as version 1:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0sHZ1Xwom_e3hImLqOhZyQ.png" /><figcaption>Creating a new version of your model using the Cloud Console UI</figcaption></figure><p>To make it easier for apps to fetch predictions from our model, we’ll create a public endpoint using Cloud Functions. You can read more about how to do this in <a href="https://cloud.google.com/blog/products/ai-machine-learning/simplifying-ml-predictions-with-google-cloud-functions"><strong>this blog post</strong></a> I wrote with my colleague <a href="https://twitter.com/SRobTweets"><strong>Sara Robinson</strong></a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1O-GzUlURbvYYyRcQ6XreA.png" /><figcaption>here’s the current architecture of our system</figcaption></figure><p>OK, our model is deployed and apps can easily fetch predictions from it! Now to monitor for model rot with DataStore!</p><h3>DataStore?</h3><p>Imagine a place in the cloud where you can store millions of python dictionaries quickly, and then query them (also quickly). That’s <a href="https://cloud.google.com/datastore/">DataStore</a>, a fully managed NoSQL database on Google Cloud Platform. If you have previous experience with databases you might be used to carefully planning out exactly what structure of data to store in tables, then experience the pain of creating migration scripts to update the structure of your database. Non of that nonsense with DataStore, want to store the following data:</p><pre>{<br>  &quot;name&quot;: &quot;Alex&quot;<br>  &quot;occupation&quot;: &quot;Zombr trainer&quot;<br>}</pre><p>then do it (<em>using the python client library</em>):</p><pre># create new entity/row<br>new_person = datastore.Entity(key=client.key(&#39;person&#39;))</pre><pre>new_person[&#39;name&#39;] = &#39;Alex&#39;<br>new_person[&#39;occupation&#39;] = &#39;Zombr trainer&#39;</pre><pre># save to datastore<br>client.put(new_person)</pre><p>Oh wait, you want to start storing peoples githubs and twitters? go for it:</p><pre># create new entity/row<br>new_person = datastore.Entity(key=client.key(&#39;person&#39;))</pre><pre>new_person[&#39;name&#39;] = &#39;Zack&#39;<br>new_person[&#39;occupation&#39;] = &#39;Zombr CEO&#39;<br>new_person[&#39;github&#39;] = &#39;<a href="https://github.com/zackakil">https://github.com/zackakil</a>&#39;<br>new_person[&#39;twitter&#39;] = &#39;<a href="https://twitter.com/zackakil">@zackakil</a>&#39;</pre><pre># save to datastore<br>client.put(new_person)</pre><p>and DataStore will say “thank you”:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*m2cUpreghQwk8Wl75Pjnrw.png" /><figcaption>DataStore’s UI</figcaption></figure><h3>Using DataStore to collect model feedback</h3><p>The feedback data that we are going to collect will look like the following:</p><pre>{<br>&quot;model&quot;: &quot;v1&quot;,<br>&quot;prediction input&quot;: [2.1, 1.4, 5.3, 8.0],<br>&quot;prediction output&quot;: 1,<br>&quot;was correct&quot;: False,<br>&quot;time&quot;: &quot;23-10-2018,14:45:23&quot;<br>}</pre><p>This data will tell us what version of our model on ML Engine was used to generate the prediction (<strong>model</strong>), what the input data for the prediction was (<strong>prediction input</strong>), what the prediction made by the model was (<strong>prediction output</strong>), if the prediction was correct (the actual feedback from the user) (<strong>was correct</strong>), and the time that the feedback was submitted (<strong>time</strong>).</p><p>We’ll use Cloud Functions again to make another web API endpoint, this time to receive the feedback data and store it in DataStore:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/href">https://medium.com/media/8509b887a7eafbcfefc2c580d4570252/href</a></iframe><p>Now our system architecture looks like the following:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LEYpFJCmZ3294kR_fMibqQ.png" /><figcaption>the new architecture of our system</figcaption></figure><p>The client apps just need to add in a intuitive way for the users to submit their feedback. In our case it could be a simple ‘thumbs up or thumbs down’ prompt after the user is presented with a prediction:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hW0dAlgij1W1fdR_J7WTg.png" /><figcaption>You may have come across feedback prompts like this before</figcaption></figure><h3>Get creative with how you collect feedback</h3><p>Often times you can infer feedback about your model, rather than explicitly requesting it from the users like I’ve done in the <strong>Zombr </strong>interface<strong>. </strong>For example if we see that a user stops using the app immediately after a prediction, we could use that data to indicate a wrong prediction 😬.</p><p>Back in reality, a dog adoption agency might have a recommender system for new owners. The rate of successful adoptions made by the model is it’s own performance feedback. If the agency suddenly sees that the system is making a lot fewer successful matches than usual, then they can use that as the indication that the model is rotten and may need updating.</p><h3>Feedback is collected, now what?</h3><p>Now we can analyse the feedback data. For any data analyse work I default to using Jupyter Notebooks.</p><p><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb">Click here for the full notebook of how I fetch data from DataStore and analyse the feedback</a>.</p><p>The important bits of fetching data from DataStore are, first installing the DataStore python client library:</p><pre>pip install google-cloud-datastore</pre><p>then you can import it and connect to DataStore:</p><pre><strong>from</strong> google.cloud <strong>import</strong> datastore</pre><pre><em># connect to DataStore</em><br><strong>client = datastore.Client(&#39;your project id&#39;)</strong></pre><pre><em># query for all prediction-feedback items</em> <br><strong>query = client.query(kind=&#39;prediction-feedback&#39;) <br> </strong><br><em># order by the time field</em> <br><strong>query.order = [&#39;time&#39;] </strong><br> <br><em># fetch the items <br># (returns an iterator so we will empty it into a list)</em> <br><strong>data = list(query.fetch())</strong></pre><p>The library with automatically convert all of the data into python dictionaries:</p><pre>print(data[0][&#39;was correct&#39;])<br>print(data[0][&#39;model&#39;])<br>print(data[0][&#39;time&#39;])<br>print(data[0][&#39;input data&#39;])</pre><pre>&gt;&gt;&gt; True<br>&gt;&gt;&gt; v1<br>&gt;&gt;&gt; 2018-10-22 14:21:02.199917+00:00<br>&gt;&gt;&gt; [-0.8300105114555543, 0.3990742221560673, 1.9084475908892906, 0.3804372006233603]</pre><p>Thanks to us saving a “was correct” boolean in our feedback data we can easily calculate the accuracy of our model from the feedback by looking at the ratio of ‘<strong>Trues’ </strong>for this field:</p><pre>number_of_items = len(data)<br>number_of_was_correct = len([d <strong>for</strong> d <strong>in</strong> data <strong>if</strong> d[&#39;was correct&#39;]])</pre><pre>print(number_of_was_correct / number_of_items)</pre><pre>&gt;&gt;&gt; 0.84</pre><p>0.84 is not much rot since we <a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor.ipynb">first trained our model</a> which scored ~0.9 accuracy, but that’s calculated using all of the feedback data together. What if we do this same accuracy calculation on a sliding window across our data and plot it? (<em>you can see the code for doing this in the </em><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb"><em>analysis notebook</em></a>)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/501/1*fsLb8wHZohYfFdzwRytfyA.png" /></figure><p>That’s a big drop in performance for the most recent feedback.</p><p>We should investigate further. Let’s compare the input data (i.e the zombie characteristic data) from the times of high accuracy to the times of low accuracy. Good thing we also collected that in our feedback:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/479/1*epRqaSvi6ZFhjgflekeGSw.png" /><figcaption>blue = correct prediction, red = incorrect prediction</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/479/1*u0LnadB81REpsSRmGjdjCw.png" /><figcaption>blue = correct prediction, red = incorrect prediction</figcaption></figure><p>Ah, the data looks completely different. I guess the zombie population has mutated! We need to retrain our model ASAP with new data. Good thing we collected the input data in the feedback, we can use that as the new training data (saves us having to manually collect new data). We can use information about the prediction the model made (“prediction” field) and the users’ feedback (“was correct” field) to infer the correct prediction label for the new training data:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/href">https://medium.com/media/c0d5987b55fdc47466ffd84670f4c456/href</a></iframe><p><a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Feedback_analyse_from_DataStore.ipynb">See how this code is used in the bottom of the feedback analysis notebook</a>.</p><p>With this new data set we can train a new version of our model. This is an identical process to training the initial model but using a different data set (<a href="https://github.com/ZackAkil/rotting-zombie-model/blob/master/Friendly_zombie_predictor_re_train.ipynb">see the notebook</a>), and then uploading it to ML Engine as a new version of the model.</p><p>Once it’s on ML Engine you can either set it as the new default version of the zombies model so that all of your clients will automatically start having their prediction requests sent to the new model, or you can instruct your clients to specify the version name in their prediction requests:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PjAjfJd5k4S6-JCfYFNHTw.png" /><figcaption>setting v2 as the default model</figcaption></figure><p>If you set the default model to v2 then all prediction request to “zombies” will go to the v2 version:</p><pre>PREDICTION REQUEST BODY:<br>{<br>&quot;instances&quot;:[[2.0, 3.4, 5.1, 1.0]],<br>&quot;model&quot;:&quot;zombies&quot;<br>}</pre><p>or your clients can just be more specific:</p><pre>PREDICTION REQUEST BODY:<br>{<br>&quot;instances&quot;:[[2.0, 3.4, 5.1, 1.0]],<br>&quot;model&quot;:&quot;zombies/versions/v2&quot;<br>}</pre><p>After all that you can sit back and just run the same analysis after some more feedback has been collected:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/688/1*paMZjeQqTQUgzXJekmXWXA.png" /><figcaption>seems like people find our v2 model helpful</figcaption></figure><p>Hopefully this has given you a few ideas on how you can monitor your deployed models for model rot. All of the code used can be found in the <a href="http://ZackAkil/rotting-zombie-model">github repo</a>:</p><p><a href="https://github.com/ZackAkil/rotting-zombie-model">ZackAkil/rotting-zombie-model</a></p><p>Reach out to me <a href="https://twitter.com/zackakil">@ZackAkil</a> with any thoughts/questions on monitoring model rot.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=747b299526e9" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/zombies-model-rot-with-ml-engine-datastore-747b299526e9">Zombies &amp; Model Rot (with ML Engine + DataStore)</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TensorFlow & reflective tape : why I’m bad at basketball ]]></title>
            <link>https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/a30a923332de</guid>
            <category><![CDATA[basketball]]></category>
            <category><![CDATA[tensorflow]]></category>
            <category><![CDATA[sports]]></category>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Fri, 05 Oct 2018 14:00:27 GMT</pubDate>
            <atom:updated>2018-10-05T21:22:21.016Z</atom:updated>
            <content:encoded><![CDATA[<h3>TensorFlow &amp; reflective tape 🏀 (am I bad at basketball?)</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J1rVK_jZ_Lodt7fimRJ3FQ.jpeg" /></figure><p>Recently a friend got me into basketball. Turns out, it’s a lot harder than it looks. No matter, I can over engineer a solution using machine learning. If your into ML and shooting hoops then there’s also <a href="https://medium.com/tensorflow/tf-jam-shooting-hoops-with-machine-learning-7a96e1236c32">this article</a> that combined TensorFlow and basketball in a simulation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*V1eZtAdA1sUfqX7WNNIClg.gif" /><figcaption>nothing but net… if there was a net</figcaption></figure><p>The task is to find the exact angle of my shots. Then I can hopefully use that information in a proactive way to get better.</p><p><em>psst! the code for all of this is on my </em><a href="https://github.com/ZackAkil/optimising-basketball"><em>Github</em></a></p><h3>Task 1: collecting data</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w3-DOoutfsTxgGyxyF36tA.jpeg" /><figcaption>I didn’t need to follow the seems of the ball, but it looks cool</figcaption></figure><p>I don’t have access to 3D tracking studios fitted with 200 cameras, but I do have Ebay. It’s quite easy to buy <a href="https://www.google.com/search?q=reflective+tape">reflective tape online</a> and stick it to the ball. Then (thanks to the lack of lighting at my local court) I can record some footage of me practicing in the evening and capture the balls movements.</p><p>The torch build into my phone provides the perfect light source to bounce off the reflective tap of the ball.</p><p>As a result the footage captured shows a sparkly object flying through a mostly dark scene, perfect for doing some image manipulation in python.</p><h3>Task 2: Getting our video into python</h3><p>Firstly I do everything in Python, and a really easy way to import video into Python is to use a library called ‘<a href="http://www.scikit-video.org">scikit-video</a>’, so I installed that:</p><pre>pip install scikit-video</pre><p>and then used it to load in my video as a matrix:</p><pre>from skvideo.io import vread</pre><pre>video_data = vread(&#39;VID_20180930_193148_2.mp4&#39;)</pre><p>The shape (that you can find by running <strong>video_data.shape</strong>) of this data is (220, 1080, 1920, 3). Which means 220 frames of 1080x1920 pixels of 3 channels of colour (red, green, blue):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*1tn3tKyuCMNidLyQillYZw.gif" /><figcaption>raw video data</figcaption></figure><h3>Task 3: Extracting the shot (image processing)</h3><p>So I want to get the data on just the balls movement. Fortunately, it’s one of the only things moving in the video. So I can do my favourite video processing trick: <strong>delta frame extraction</strong>! (<em>that’s what I call it, but there’s probably another name for it</em>).</p><p>By subtracting all of the pixel values in one frame from all of the pixel values in the next frame you will be left with non-zero values in just the pixels that have changed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/417/1*KeA9dwEEeuXqNE7AVKb8uw.png" /><figcaption>calculating delta frame in order to isolate moving pixels</figcaption></figure><p>Cool cool cool, now I do that for each frame in the video and combine the result into one image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/490/1*tYmopdxUS4b0iiJ5QwQJ8w.png" /><figcaption>adding together all of the delta frames from the video sequence</figcaption></figure><p>Next is extracting the shot data into a usable format. So we’ll covert the pixel values that are lite up into a list of <strong><em>x</em></strong> and <strong><em>y</em></strong> points. The code to do this is a numpy function called <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.where.html"><strong>numpy.where</strong></a> which will find all of the values that are <strong>True</strong> in an array and return their indices (i.e their positions in the matrix).</p><p>But before we do that, we’ll quickly crop out just the balls trajectory and flip the data so that it starts at the origin (the bottom left of the scene):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/850/1*wlWVcZEObJePb1zEzeNN1Q.png" /></figure><p>and the resulting image:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1009/1*OxS6pId6J1aMXM7rlthu5w.png" /></figure><p>Notice how it still seems upside down? That’s only because images tend to be drawn starting at the <strong>top</strong> left corner (note the axis numbering). When we convert the pixels to data points and draw them on a normal graph they will get drawn starting at the <strong>bottom</strong> left corner.</p><p>now we run our <strong>numpy.where</strong> code to get the pixels as data points:</p><pre>pixels_that_are_not_black = cropped_trail.sum(axis=2) &gt; 0</pre><pre>y_data, x_data = numpy.where(pixels_that_are_not_black)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/673/1*JFCUAmNTHhqWUB3lUyXDCg.png" /><figcaption>awesome! our relatively clean ball trajectory data</figcaption></figure><h3>Task 4: Build TensorFlow model</h3><p>This is where TensorFlow shines. You may be used to hearing about using TensorFlow for building neural networks, but you can define almost any mathematical formula and tell it to optimise whatever parts of it you want. In our case we will use the formula for a trajectory which we know from primary school to be:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/569/1*5VB-_Rbf_mxycVAzxtY2fA.png" /><figcaption>extremely mathematical equation of trajectory that I found online</figcaption></figure><p><strong>θ</strong> (theta) is the angle of the shot (<em>the value we really care about</em>)</p><p><strong><em>v</em></strong> is the initial velocity</p><p><strong><em>g</em> </strong>is gravity (9.8m/s)</p><p><strong><em>x </em></strong>is the horizontal position (<em>data we have already</em>)</p><p><strong><em>y</em></strong> is the vertical position (<em>data we have already</em>)</p><p>A far more interesting way to see the equation in action is to play around with <a href="https://www.desmos.com/calculator/gjnco6mzjo">this trajectory tool</a>.</p><p>We can use the ball trail of my shot as the <strong><em>x</em></strong> and <strong><em>y </em></strong>of the<strong><em> </em></strong>equation and task TensorFlow with finding the correct angle (<strong>θ</strong>) and initial velocity (<strong><em>v</em></strong>) that fits my shots <strong><em>x</em></strong> and <strong><em>y</em></strong> data:</p><p>We’ll start be recreating our trajectory equation in TensorFlow:</p><p>first tell it what data we will feed it when we run the optimisation:</p><pre>x = tf.placeholder(tf.float32, [<strong>None</strong>, 1])<br>y = tf.placeholder(tf.float32, [<strong>None</strong>, 1])</pre><p>next tell it what variables we want it to tweak and tune in order to fit the trajectory curve to our data:</p><pre>angle_variable = tf.Variable(40.0, name=&#39;angle_variable&#39;)<br>force_variable = tf.Variable(100.0, name=&#39;force_variable&#39;)<br><br>gravity_constant = tf.constant(9.8, name=&#39;gravity_constant&#39;)</pre><p>and join all of these together (<strong>warning</strong>: it’s going to look quite messy, but it’s just the maths equation seen before written in TensorFlow syntax):</p><pre>left_hand_side = x * tf.tan(deg2rad(angle_variable))<br>top = gravity_constant * x ** 2<br>bottom = (2*(force_variable)**2) * <br>            (tf.cos(deg2rad(angle_variable))**2)</pre><pre>output = left_hand_side - (top / bottom)</pre><p>then tell TensorFlow how to tell if it’s doing a good job or not at fitting the trajectory function to our data:</p><pre># the lower this score, the better<br>error_score = tf.losses.mean_squared_error(y, output)</pre><p>create an optimiser that will do the actual tweaking of variables (angle_variable and force_variable) in order to reduce the error_score:</p><pre>optimiser = tf.train.AdamOptimizer(learning_rate=5) <br>optimiser_op = optimiser.minimize(error_score)</pre><h3>Task 5 : Magic</h3><p>We can now run the optimisation task to find the <strong>angle_variable</strong> and <strong>force_variable</strong> values that fit my shot.</p><pre>sess = tf.Session()<br>sess.run(tf.global_variables_initializer())</pre><pre># do 150 steps of optimisation<br>for i in range(150):</pre><pre>    sess.run([optimiser_op], <br>             feed_dict={x: np.array(x_data).reshape(-1, 1), <br>                        y: np.array(y_data).reshape(-1, 1)})</pre><pre>found_angle = sess.run(angle_constant.value())</pre><pre>print(found_angle)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/504/1*4DTRp298z0G0LHxFp3WWGA.gif" /><figcaption>TensorFlow finding the angle of my shot</figcaption></figure><p>At the end of that optimisation we find out that the trajectory function that best fits my shot data has an angle of ~61°… not sure what to do with that information… I guess I could look at what professional shooting angles are for comparison… to be continued.</p><blockquote>The lesson to take away: you can always distract your-self with completely unnecessary (but fun) machine learning.</blockquote><p>All of the code I used is available on my Github:</p><p><a href="https://github.com/ZackAkil/optimising-basketball">ZackAkil/optimising-basketball</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a30a923332de" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/tensorflow-reflective-tape-why-im-bad-at-basketball-a30a923332de">TensorFlow &amp; reflective tape : why I’m bad at basketball 🏀</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Finally learn how to use command line apps… by making one!]]></title>
            <link>https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd?source=rss-5b7ea6e5016a------2</link>
            <guid isPermaLink="false">https://medium.com/p/bd5cf21a15cd</guid>
            <category><![CDATA[beginner]]></category>
            <category><![CDATA[command-line]]></category>
            <category><![CDATA[linux]]></category>
            <category><![CDATA[bash]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Zack Akil]]></dc:creator>
            <pubDate>Mon, 21 May 2018 21:07:45 GMT</pubDate>
            <atom:updated>2018-05-22T23:15:06.221Z</atom:updated>
            <content:encoded><![CDATA[<p>At the risk of alienating a lot of readers… I grew up with GUI’s, so I never needed to learn the ways of the terminal! This is socially acceptable in todays society of friendly user interfaces, except maybe if you’re in software engineering… whoops!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/455/1*cyhNkd0jd7Zd3zBDLaKpSg.png" /><figcaption>Manager — “Oh this is easy, just use this command line app”, Me — ”Yes…the command line… I’ll use that…”</figcaption></figure><h3>Getting back my software engineering street cred’</h3><p>I’ve gotten pretty far just through copy and pasting full command-line operations from Stack-Overflow but have never become comfortable enough to use a command line app “properly”. What finally caused it to click for me was when I stumbled across a very handy python library called <a href="https://docs.python.org/3/library/argparse.html"><strong>argparse</strong></a><strong> </strong>that allows you to build a nice robust command line interface for your python scripts.</p><p><a href="https://docs.python.org/3.5/howto/argparse.html">This tutorial</a> is great in-depth explanation about how to use <strong>argparse, </strong>but I’ll go over the key eye openers<strong>:</strong></p><blockquote><a href="https://docs.python.org/3/library/argparse.html">argparse</a> is part of the standard python library so pop open your code editor and follow along (you don’t need to install anything)!</blockquote><h3><strong>HELP</strong></h3><p>The most useful part of every command line app!</p><pre># inside a file called my_app.py</pre><pre><strong>import</strong> <strong>argparse</strong><br>parser = argparse.ArgumentParser(description=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.parse_args()</pre><p>The code above will do nothing, except by default you have the <strong>help </strong>flag!</p><p>In the command line you can run:</p><pre>python my_app.py --help</pre><p>or</p><pre>python my_app.py -h</pre><p>and you’ll get an output like this:</p><pre>usage: my_app.py [-h]<br></pre><pre><strong>Nice little CL app!</strong><br></pre><pre>optional arguments:</pre><pre>-h, --help  <strong>show this help message and exit</strong></pre><p>Seems pretty cool right? but wait, when you use <strong>argparse </strong>to<strong> </strong>add more functions (see below) this <strong>help</strong> output will automatically fill up with all of the instructions of how you can use your app!</p><h3>REQUIRED ARGUMENTS</h3><p>Lets say you want to have your app take in some variable, we just use the parser.add_argument() function and give our argument some label (in this case “name”):</p><pre>import argparse<br>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br></pre><pre>parser.add_argument(&quot;<strong><em>name</em></strong>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)<br></pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what?</em> &quot; + args<strong>.name</strong>)</pre><p>Notice how we added the <strong>help </strong>text! Now when we run python my_app.y -h we get all of the app details:</p><pre>usage: my_app.py [-h] name<br></pre><pre><strong>Nice little CL app!</strong><br></pre><pre>positional arguments:</pre><pre>name        <strong>Just your name, nothing special</strong><br></pre><pre>optional arguments:</pre><pre>-h, --help  <strong>show this help message and exit</strong></pre><p>Pretty cool, but let’s run our app with python my_app.py</p><pre>usage: my_app.py [-h] name</pre><pre>my_app.py: error: too few arguments</pre><p>That’s right! Automatic input checking!</p><p>Now lets run python my_app.py &quot;Slim Shady&quot;</p><pre>Your name is what? Slim Shady</pre><p>Pretty slick!</p><h3>OPTIONAL ARGUMENTS</h3><p>Maybe you want to give someone the option of telling you little more about themselves? Adding a double dash when using parser.add_argument() function will make that argument optional!</p><pre>import argparse<br>parser = argparse.ArgumentParser(<strong>description</strong>=”<em>Nice little CL app!</em>”)<br>parser.add_argument(“<em>name</em>”, <strong>help</strong>=”<em>Just your name, nothing special</em>”)<br></pre><pre>parser.add_argument(&quot;<strong><em>--</em></strong><em>profession</em>”, <strong>help</strong>=”Y<em>our nobel profession</em>”)<br></pre><pre>args = parser.parse_args()<br>print(“<em>Your name is what? </em>“ + args.name)</pre><pre>if args.profession:<br>    print(“<em>What is your profession!? a </em>“ + args.profession)</pre><p>If you want to pass in a variable for that argument you just need to specify the double-dashed argument name before the variable you want to pass:</p><pre>python my_app.py &quot;Slim Shady&quot; <strong>--profession &quot;gift wrapper&quot;</strong></pre><p>which gives you :</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper</pre><p>Or you don’t have to, it is optional after all!</p><pre>python my_app.py &quot;Slim Shady&quot;</pre><p>still gives you:</p><pre>Your name is what? Slim Shady</pre><p>and the magic once again when running python my_app.py -h :</p><pre>usage: my_app.py [-h] [--profession PROFESSION] name<br></pre><pre><strong>Nice little CL app!<br></strong></pre><pre>positional arguments:</pre><pre>name                      <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                <strong>show this help message and exit</strong></pre><pre>--profession PROFESSION   <strong>Your nobel profession</strong></pre><h3>FLAGS</h3><p>Maybe you just want to enable something cool to happen. Add action=&quot;<em>store_true</em>&quot; to your parser.add_argument() function and you have yourself a <strong>flag</strong> argument :</p><pre>import argparse</pre><pre>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.add_argument(&quot;<em>name</em>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)<br>parser.add_argument(&quot;<em>--profession</em>&quot;, <strong>help</strong>=&quot;<em>Your nobel profession</em>&quot;)<br></pre><pre>parser.add_argument(&quot;<em>--cool</em>&quot;, <strong>action</strong>=&quot;<em>store_true</em>&quot;, <strong>help</strong>=&quot;<em>Add a little cool</em>&quot;)<br></pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what? </em>&quot; + args.name)</pre><pre>cool_addition = &quot;<em> and dragon tamer</em>&quot; if args.cool else &quot;&quot;</pre><pre>if args.profession:<br>    print(&quot;<em>What is your profession!? a </em>&quot; + args.profession + cool_addition)</pre><p>It’s pretty neat, you just plop the <strong>flag</strong> name into your command like so:</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot; <strong>--cool</strong></pre><p>and presto!</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper and dragon tamer</pre><p>and remember, you don’t have to use it, it’s just a flag:</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot;</pre><p>will still give:</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper</pre><p>look though at the <strong>help </strong>command python my_app.py -h :</p><pre>usage: my_app.py [-h] [--profession PROFESSION] [--cool] name<br></pre><pre><strong>Nice little CL app!<br></strong></pre><pre>positional arguments:</pre><pre>name                      <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                <strong>show this help message and exit</strong></pre><pre>--profession PROFESSION   <strong>Your nobel profession</strong></pre><pre>--cool                    <strong>Add a little cool</strong></pre><p>I’m just going to assume you’re as satisfied with that as I am from now on.</p><h3>SHORT FORM</h3><p>Unveiling the mysterious one character arguments that confused me for so long. Just by adding a single letter prefixed with a single dash to your parser.add_argument() functions you have super short versions of the same arguments:</p><pre>import argparse</pre><pre>parser = argparse.ArgumentParser(<strong>description</strong>=&quot;<em>Nice little CL app!</em>&quot;)<br>parser.add_argument(&quot;<em>name</em>&quot;, <strong>help</strong>=&quot;<em>Just your name, nothing special</em>&quot;)</pre><pre>parser.add_argument(&quot;-p&quot;, &quot;<em>--profession</em>&quot;, <strong>help</strong>=&quot;<em>Your nobel profession</em>&quot;)</pre><pre>parser.add_argument(&quot;-c&quot;, &quot;<em>--cool</em>&quot;, <strong>action</strong>=&quot;<em>store_true</em>&quot;, <strong>help</strong>=&quot;<em>Add a little cool</em>&quot;)</pre><pre>args = parser.parse_args()<br>print(&quot;<em>Your name is what? </em>&quot; + args.name)<br>cool_addition = &quot;<em> and dragon tamer</em>&quot; if args.cool else &quot;&quot;<br>if args.profession:<br>    print(&quot;<em>What is your profession!? a </em>&quot; + args.profession + cool_addition)</pre><p>So that instead of typing in :</p><pre>python my_app.py &quot;Slim Shady&quot; --profession &quot;gift wrapper&quot; --cool</pre><p>you can just type:</p><pre>python my_app.py &quot;Slim Shady&quot; -p &quot;gift wrapper&quot; -c</pre><p>and you’ll get the same output:</p><pre>Your name is what? Slim Shady</pre><pre>What is your profession!? a gift wrapper and dragon tamer</pre><p>And this is reflected in the help text (python my_app.py -h ):</p><pre>usage: my_app.py [-h] [--profession PROFESSION] [--cool] name</pre><pre><strong>Nice little CL app!</strong></pre><pre>positional arguments:</pre><pre>name                           <strong>Just your name, nothing special</strong></pre><pre>optional arguments:</pre><pre>-h, --help                      <strong>show this help message and exit</strong></pre><pre>-p PROFESSION, --profession PROFESSION   <strong>Your nobel profession</strong></pre><pre>-c, --cool                      <strong>Add a little cool</strong></pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/400/1*OYyJ-v1Ao09QEhv71MW_Gg.jpeg" /><figcaption>a perfect little command line app!</figcaption></figure><p>You now have a perfect little command line app and are hopefully more comfortable with finding your way around command line apps!</p><p>Just remember --help !</p><p><a href="https://github.com/ZackAkil/super-simple-command-line-app">ZackAkil/super-simple-command-line-app</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd5cf21a15cd" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/finally-learn-how-to-use-command-line-apps-by-making-one-bd5cf21a15cd">Finally learn how to use command line apps… by making one!</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>