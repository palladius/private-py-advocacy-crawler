<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Mete Atamel on Medium]]></title>
        <description><![CDATA[Stories by Mete Atamel on Medium]]></description>
        <link>https://medium.com/@meteatamel?source=rss-6444b44155fa------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/2*0D3MqBpzg9laGKlwvZJ7Uw.jpeg</url>
            <title>Stories by Mete Atamel on Medium</title>
            <link>https://medium.com/@meteatamel?source=rss-6444b44155fa------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 15:29:36 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@meteatamel/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Grounding LLMs with your own data using Vertex AI Search]]></title>
            <link>https://medium.com/google-cloud/grounding-llms-with-your-own-data-using-vertex-ai-search-eb7a008b025f?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/eb7a008b025f</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 01 Jul 2024 12:48:53 GMT</pubDate>
            <atom:updated>2024-07-01T12:48:53.652Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>In my previous <a href="https://cloud.google.com/blog/products/ai-machine-learning/using-vertex-ai-grounding-with-google-search">Vertex AI’s Grounding with Google Search: how to use it and why</a> post, I explained why you need grounding with large language models (LLMs) and how Vertex AI’s grounding with Google Search can help to ground LLMs with public up-to-date data.</p><p>That’s great but you sometimes need to ground LLMs with your own private data. How can you do that? There are many ways but Vertex AI Search is the easiest way and that’s what I want to talk about today with a simple use case.</p><h3>Cymbal Starlight 2024</h3><p>For the use case, imagine you own the 2024 model of a fictitious vehicle called Cymbal Starlight. It has a user’s manual in PDF format ( <a href="https://github.com/meteatamel/genai-beyond-basics/blob/main/samples/grounding/vertexai-search/cymbal-starlight-2024.pdf">cymbal-starlight-2024.pdf</a>) and you want to ask LLM questions about this vehicle from the manual.</p><p>Since LLMs are not trained with this user manual, they’ll likely not be able to answer any questions about the vehicle but let’s see.</p><h3>Without Grounding</h3><p>First, let’s ask a question to the LLM about the vehicle without any grounding. You can do this in the <a href="https://console.cloud.google.com/vertex-ai/generative/language/create/text">Vertex AI section of Google Cloud Console</a> against any model.</p><pre>Prompt: What is the cargo capacity of Cymbal Starlight?<br><br>Response: Unfortunately, I do not have access to real-time information,<br>including specific details about ships like &quot;Cymbal Starlight&quot; <br>...</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*f4NRx61P-aHrGggT.png" /></figure><p>As you can see, the model thinks we’re talking about a ship and cannot really answer the question.</p><p>There’s also a <a href="https://github.com/meteatamel/genai-beyond-basics/blob/main/samples/grounding/vertexai-search/main.py">main.py</a> sample code that you can run:</p><pre>python main.py --project_id your-project-id</pre><p>You get a similar response:</p><pre>Prompt: What is the cargo capacity of Cymbal Starlight?<br>Response text: I do not have access to real-time information,<br>including specific details about ships like the &quot;Cymbal Starlight.&quot;</pre><p>This is not surprising. Let’s now ground the LLM with this private PDF.</p><h3>Create datastore with PDF</h3><p>To use a PDF for grounding, you need to upload the PDF to a Cloud Storage bucket and set up a datastore to import from that bucket.</p><p>Create a Cloud Storage bucket with uniform bucket-level access and upload the PDF file:</p><pre>gsutil mb -b on gs://cymbal-starlight-bucket <br>gsutil cp cymbal-starlight-2024.pdf gs://cymbal-starlight-bucket</pre><p>Go to the <a href="https://console.cloud.google.com/gen-app-builder">Agent Builder</a> section of the Google Cloud Console. Click on Data Stores on the left and Create Data Store button. Select Cloud Storage.</p><p>Point to the PDF file in the bucket and continue:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/0*LMBldj6v2RHdjLsF.png" /></figure><p>Give your datastore a name and click Create:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/349/0*Nuiji-VPRRTuAF90.png" /></figure><p>You need to wait until the import is complete which can take up to 10–15 minutes:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/482/0*keVLRiNSEq8S1Itr.png" /></figure><h3>Create a search app</h3><p>Before you can use grounding, we need to create a search app and point to the datastore you just created.</p><p>Go to <a href="https://console.cloud.google.com/gen-app-builder">Agent Builder</a> section of the Google Cloud Console and click on Create App button:</p><ol><li>Select Search app type.</li><li>For Content, select Generic.</li><li>Make sure Enterprise edition features is enabled</li><li>Give your app a name and enter company info.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/611/0*njlkgE6xnPgH5MYh.png" /></figure><p>In the next page, choose the data store you created earlier and click Create:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/437/0*-PXV-6g-Q3XowoWC.png" /></figure><h3>Setup grounding with Vertex AI Search</h3><p><strong>Caution</strong>: <em>Make sure the datastore import is complete before you continue. If you get an error message: “Cannot use enterprise edition features” or something similar, you might need to wait a little before trying again.</em></p><p>Let’s set up grounding with Vertex AI Search now.</p><p>Go back to <a href="https://console.cloud.google.com/vertex-ai/generative/language/create/text">Vertex AI section of Google Cloud Console</a> and in the Advanced section, select Enable grounding:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/329/0*Z53h5aPi5ymt9S_N.png" /></figure><p>Customize grounding and point to the datastore you just created:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/494/0*EarK5Siu_PTKfrSG.png" /></figure><p>This is the format of the datastore string:</p><p>projects/{}/locations/{}/collections/{}/dataStores/{}</p><p>In my case, it’s as follows:</p><p>projects/genai-atamel/locations/global/collections/default_collection/dataStores/cymbal-datastore_1718710076267</p><h3>With grounding</h3><p>Finally, we’re ready to ask questions about the vehicle with grounding enabled.</p><p>Let’s start with the previous question:</p><pre>Prompt: What is the cargo capacity of Cymbal Starlight?<br>Response: The Cymbal Starlight 2024 has a cargo capacity of 13.5 cubic feet.</pre><p>Let’s ask another question:</p><pre>Prompt: What&#39;s the emergency roadside assistance phone number?<br>Response: 1-800-555-1212</pre><p>Also run <a href="https://github.com/meteatamel/genai-beyond-basics/blob/main/samples/grounding/vertexai-search/main.py">main.py</a> Python sample with grounding:</p><pre>python main.py --project_id your-project-id \<br>  --data_store_path projects/your-project-id/locations/global/collections/default_collection/dataStores/your-datastore-id</pre><p>You get a similar response:</p><pre>Prompt: What is the cargo capacity of Cymbal Starlight? <br>Response text: The Cymbal Starlight 2024 has a cargo capacity of 13.5 cubic feet.</pre><p>It works!</p><h3>Conclusion</h3><p>In this blog post, I explored Vertex AI’s grounding with Vertex AI Search. It’s the easiest way of grounding LLMs with your own private data. If you want to learn more, here are some more resources:</p><ul><li><a href="https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search">Get started with generic search docs</a></li><li><a href="https://cloud.google.com/docs/ai-ml/generative-ai#grounding">Grounding decision flowchart</a></li><li><a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb">Getting Started with Grounding with Gemini in Vertex AI notebook</a></li><li><a href="https://youtu.be/v4s5eU2tfd4">Grounding for Gemini with Vertex AI Search and DIY RAG talk</a></li><li><a href="https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/sample-apps/fixmycar">Fix My Car sample app</a></li></ul><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/07-01_grounding_with_own_data_vertexai_search/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb7a008b025f" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/grounding-llms-with-your-own-data-using-vertex-ai-search-eb7a008b025f">Grounding LLMs with your own data using Vertex AI Search</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Give your LLM a quick lie detector test]]></title>
            <link>https://medium.com/google-cloud/give-your-llm-a-quick-lie-detector-test-97922d247c81?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/97922d247c81</guid>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Thu, 06 Jun 2024 14:03:20 GMT</pubDate>
            <atom:updated>2024-06-06T14:03:20.384Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/507/1*1-soPSwI75UIlIihhoySBQ.png" /></figure><h3>Introduction</h3><p>It’s no secret that LLMs sometimes lie and they do so in a very confident kind of way. This might be OK for some applications but it can be a real problem if your application requires high levels of accuracy.</p><p>I remember when the first LLMs emerged back in early 2023. I tried some of the early models and it felt like they were hallucinating half of the time. More recently, it started feeling like LLMs are getting better at giving more factual answers. But it’s just a feeling and you can’t base application decisions (or any decision?) on feelings, can you?</p><p><strong>How can I tell if my LLM is lying to me and how much?</strong></p><p>The rigorous answer probably requires using something like Vertex AI’s <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview">Generative AI evaluation service</a> which lets you evaluate model performance across specific use cases. It’s something I’ll look into and report back on in a future article.</p><p>For now, I’m interested in a quick lie detector test for my LLM and for that, you need:</p><ol><li>A high quality, labeled <strong>dataset</strong> that you can trust</li><li>A good <strong>test question</strong> that you can ask the LLM about the dataset</li><li>A way to <strong>measure</strong> the accuracy of the LLM response</li></ol><p>Let’s talk about these in more detail.</p><h3>Dataset</h3><p>For a dataset, you have a lot of options and you probably want to use something that’s close to your application domain to make it a realistic test. However, in this quick &amp; dirty lie detector test, we can use any dataset that you trust to be correct.</p><p>I was introduced to the <a href="https://opentdb.com/">Open Trivia database</a> by a coworker last year. It is a free to use, user-contributed trivia question database with 4100+ verified questions and answers.</p><p>It also has an API where you can request random questions in different categories and answers with a URL like this: <a href="https://opentdb.com/api.php?amount=10">https://opentdb.com/api.php?amount=10</a></p><p>And you get a JSON response like this:</p><pre>{<br>  response_code: 0,<br>  results: [<br>    {<br>      type: &quot;multiple&quot;,<br>      difficulty: &quot;medium&quot;,<br>      category: &quot;Sports&quot;,<br>      question: &quot;How many games did Arsenal FC go unbeaten during the 2003-2004<br>        season of the English Premier League&quot;,<br>      correct_answer: &quot;38&quot;,<br>      incorrect_answers: [<br>        &quot;51&quot;,<br>        &quot;49&quot;,<br>        &quot;22&quot;<br>      ]<br>    },<br>    ...<br>}</pre><h3>Test</h3><p>We have the dataset but what should the test question be for the LLM?</p><p>Again, there are different approaches you can take here but the simplest approach is to ask the LLM: <strong>Given this question and answers, find the correct answer</strong>. This is a good test because it’s a very specific task, the LLM has all the info it needs, and the lie is easy to detect.</p><p>We need to be careful about not biasing LLM with the correct answer. We also don’t want to burden the LLM with more info then it needs. This means, the previous JSON file can be transformed into this simplified JSON with all correct and incorrect answers shuffled in the same list:</p><pre>[<br>  {<br>    question: &quot;How many games did Arsenal FC go unbeaten during the 2003-2004<br>        season of the English Premier League&quot;,<br>    answers: [<br>      &quot;51&quot;,<br>      &quot;49&quot;,<br>      &quot;38&quot;,<br>      &quot;22&quot;<br>    ]<br>  },<br>  ...<br>]</pre><p>Now, we can ask the LLM: Given these questions and answers, find the correct answer for each question.</p><h3>Measure</h3><p>Measuring accuracy is basically comparing the correct answers in the original dataset and the correct answers given by the LLM. If they all match, you get 100% accuracy but we know this won’t be the case. <strong>The question is: How close will we get to 100% accuracy with different models?</strong></p><h3>Code</h3><p>The full code is in my <a href="https://github.com/meteatamel/opentrivia-llm-testing/">opentrivia-llm-testing</a> repo. You can check out for details but to give you an overview:</p><ol><li><a href="https://github.com/meteatamel/opentrivia-llm-testing/blob/main/main.py#L16">get_questions</a> retrieves questions from Open Trivia and filters out the unnecessary fields.</li><li><a href="https://github.com/meteatamel/opentrivia-llm-testing/blob/main/main.py#L50">transform_questions</a> combines correct and incorrect answers into a single field.</li><li><a href="https://github.com/meteatamel/opentrivia-llm-testing/blob/main/main.py#L63">ask_llm</a> asks the LLM to find the correct answer and return a JSON back with the same format as Open Trivia.</li><li><a href="https://github.com/meteatamel/opentrivia-llm-testing/blob/main/main.py#L112">compare_question_lists</a> compares the lists and keeps track of how many correct answers the LLM returned.</li><li><a href="https://github.com/meteatamel/opentrivia-llm-testing/blob/main/main.py#L162">run_tests</a> runs multiple tests in an iteration, keeping track of accuracy and also time taken.</li></ol><p>A few notes about the code:</p><ul><li>When prompting the LLM, I set the temperature to 0 for more consistency. Temperature can be thought of as a measure of how random or creative you want the response to be. Using a value of zero means we want the most repeatable results possible.</li><li>I turned off safety settings to avoid censored output.</li><li>Despite specifying the desired JSON output in our prompt, I occasionally still got malformed JSON so I had to add some post-processing to catch and correct that case.</li><li>I observed quite a bit of variance in the results when running these tests, so I run multiple iterations and averaged the results.</li><li>I tried adding the Google Search grounding option to see if it helps improve accuracy.</li></ul><p>You can run the tests as follows with 4 test iterations and 25 questions in each iteration and no Google Search grounding:</p><pre>python main.py your-project-id model-id</pre><p>If you wanted to customize the test runs, you could do so as follows:</p><pre>python main.py your-project-id model-id \ <br>  --num_iterations= 4 --no_questions= 25 --google_search_grounding</pre><p>As a side note, I used <a href="https://cloud.google.com/products/gemini/code-assist">Gemini Code Assist</a> to help me write the Python code and it was amazing! It really sped up my development process and saved me quite a bit of time as a non-expert Python developer.</p><h3>Results</h3><p><strong>Disclaimer:</strong> My tests are hardly scientific. For each model, I only ran 4 iterations with 25 questions in each iteration. 100 questions is not enough. I need to run against more questions, more iterations, and use more sophisticated <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview#determine-eval">evaluation methods</a> beyond accuracy and time. Also, I don’t know if correctness against Open Trivia generalizes to other use cases.</p><p>Let’s look at the results with the disclaimer in mind. You can see the detailed test runs in the <a href="https://github.com/meteatamel/opentrivia-llm-testing/tree/main/runs">runs</a> folder with a results table.</p><p>Surprisingly, Google Search grounding did not help in my tests. It made the execution time longer with slightly less correctness percentage. My guess is that the dataset is already public and possibly models are already trained on it and that’s why grounding with Google Search does not help.</p><p>Here are the results without grounding and with average correctness and execution seconds:</p><h4>Average results without grounding</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/588/1*O_6tgCzQphZoHt29O8Wsgg.png" /></figure><p>Most models achieved more than 90% correctness and that was good to see. There was a big variation in execution times, ranging from 14 seconds to almost 45 seconds. If speed is not important, I’d probably go with gemini-1.5-pro with its best accuracy. Otherwise, I&#39;d probably choose gemini-1.0-pro as it seems to be quick but also has a good accuracy at the same time. Another consideration is probably cost but it&#39;s not something I looked into.</p><h3>Conclusion</h3><p>In this blog post, I tried to see if I can figure out how much LLMs lie to me and came up with a simple test. As I said, my tests are hardly scientific and not sure if they generalize beyond the Open Trivia dataset. The main point is that if accuracy is important for your use case, you can also come up with a test. It was nice to see accuracy above 90% with most models in my case. It’s not 100% but a good start and with other techniques like RAG and grounding, it can probably be improved depending on the dataset and questions asked.</p><p>In a future post, I want to look into Vertex AI’s <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview">Generative AI evaluation service</a> and see if I can do a more rigorous, scientific evaluation.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/06-06_llm_lie_detector_test/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=97922d247c81" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/give-your-llm-a-quick-lie-detector-test-97922d247c81">Give your LLM a quick lie detector test</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AsyncAPI gets a new version 3.0 and new operations]]></title>
            <link>https://medium.com/google-cloud/asyncapi-gets-a-new-version-3-0-and-new-operations-013dd1d6265b?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/013dd1d6265b</guid>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[software-engineering]]></category>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[event-driven-architecture]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 13 May 2024 08:32:53 GMT</pubDate>
            <atom:updated>2024-05-13T12:07:34.632Z</atom:updated>
            <content:encoded><![CDATA[<p>Almost one year ago, I talked about <a href="https://www.asyncapi.com/">AsyncAPI</a> 2.6 and how confusing its publish and subscribe operations can be in my <a href="https://atamel.dev/posts/2023/05-18_asyncapi_publishsubscribe_refactor">Understanding AsyncAPI&#39;s publish &amp; subscribe semantics with an example</a> post.</p><p>Since then, a new 3.0 version of AsyncAPI has been released with breaking changes and a totally new send and receive operations.</p><p>In this blog post, I want to revisit the example from last year and show how to rewrite it for AsyncAPI 3.0 with the new send and receive operations.</p><h3>AsyncAPI 3.0</h3><p>AsyncAPI 3.0 was released in December 2023. Since it&#39;s a major version, it has some breaking changes. These two pages does a good job explaining the changes and the rationale behind them:</p><ul><li><a href="https://www.asyncapi.com/blog/release-notes-3.0.0">AsyncAPI 3.0.0 Release Notes</a>.</li><li><a href="https://www.asyncapi.com/docs/migration/migrating-to-v3">Migrating to v3</a>.</li></ul><p>I won’t go through all the changes. For me, the biggest change is the separation of operations and channels, and changing publish and subscribe operations to send and receive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/693/0*JRRNdhOa9QSlg9u_.png" /></figure><h3>Recap: Publish and subscribe operations in AsyncAPI 2.6</h3><p>As a recap, AsyncAPI 2.6 has the following publish and subscribe operations with these semantics:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/783/1*qt82k4Q2nNRNfxRTiEz9xA.png" /></figure><p>In 2.6, publish and subscribe operations are from <strong>user&#39;s perspective</strong>.</p><h3>New: Send and receive operations in AsyncAPI 3.0</h3><p>In 3.0, the publish and subscribe operations are replaced with send and receive operations. In <a href="https://www.asyncapi.com/docs/migration/migrating-to-v3">Migrating to v3</a> page, the rationale is given as follows:</p><blockquote>In v2, the publish and subscribe operations consistently caused confusion, even among those familiar with the intricacies.</blockquote><blockquote>When you specified publish, it implied that others could publish to this channel since your application subscribed to it. Conversely, subscribe meant that others could subscribe because your application was the one publishing.</blockquote><blockquote>In v3, these operations have been entirely replaced with an action property that clearly indicates what your application does. That eliminates ambiguities related to other parties or differing perspectives.</blockquote><p>While I agree that publish and subscribe were confusing, I&#39;m not sure if send and receive are less confusing. You still need to talk about whose perspective when defining these operations. AsyncAPI docs talk about <em>application</em> but that&#39;s not clear either. Does application refer to the code sending the message (user) or the code receiving the message (server)?</p><p>In Async 3.0, send and receive operations are from <strong>server&#39;s perspective</strong>. An example will clarify.</p><h3>Account and Email Services</h3><p>Let’s revisit our example from last year. You have two microservices: Account Service emits an userSignedUp event and Email Service receives that event:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/720/0*9Ydxd0aMTSxeLya1" /></figure><p>How do you define such an architecture in AsyncAPI 2.6 vs 3.0?</p><h3>Account Service</h3><p>For Account Service, in 2.6, you had to define a channel with subscribe operation because the <strong>user</strong> has to subscribe to receive messages:</p><pre>asyncapi: 2.6.0<br><br>channels:<br>  user/signedup:<br>    subscribe:<br>      operationId: publishUserSignedUp<br>      message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/account-service-2.6.yaml">account-service-2.6.yaml</a></p><p>In 3.0, the channel does not have an operation anymore. Instead, there&#39;s a new publishUserSignedUp operation with send action that refers to the channel. This is because the <strong>server</strong> sends the message:</p><pre>asyncapi: 3.0.0<br><br>channels:<br>  user/signedup:<br>    address: user/signedup<br>    messages:<br>      publishUserSignedUp.message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;<br><br>operations:<br>  publishUserSignedUp:<br>    action: send<br>    channel:<br>      $ref: &#39;#/channels/user~1signedup&#39;<br>    messages:<br>      - $ref: &#39;#/channels/user~1signedup/messages/publishUserSignedUp.message&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/account-service-3.0.yaml">account-service-3.0.yaml</a></p><h3>Email Service</h3><p>Similarly, in Email Service, in 2.6, you had to define a publish operation because the <strong>user</strong> had to publish a message to the server:</p><pre>asyncapi: 2.6.0<br><br>channels:<br>  user/signedup:<br>    publish:<br>      operationId: receiveUserSignedUp<br>      message:<br>        $ref : &#39;#/components/messages/userSignedUp&#39;</pre><p><a href="https://github.com/meteatamel/asyncapi-basics/blob/main/samples/account-email-services/email-service-2.6.yaml">email-service-2.6.yaml</a></p><p>However, in 3.0, the <strong>server</strong> receives a message, so the operation has receive action:</p><pre>asyncapi: 3.0.0<br><br>channels:<br>  user/signedup:<br>    address: user/signedup<br>    messages:<br>      receiveUserSignedUp.message:<br>        $ref: &#39;#/components/messages/userSignedUp&#39;<br><br>operations:<br>  receiveUserSignedUp:<br>    action: receive<br>    channel:<br>      $ref: &#39;#/channels/user~1signedup&#39;<br>    messages:<br>      - $ref: &#39;#/channels/user~1signedup/messages/receiveUserSignedUp.message&#39;</pre><p>In this blog post, I explored a small part of AsyncAPI and explained the differences between 2.6 operations publish and subscribe and 3.0 operations send and receive. In a nutshell, in 2.6, you need to think from user&#39;s perspective and in 3.0 in server&#39;s perspective when you define these operations.</p><p>If you’re interested in learning more, I have a talk on CloudEvents and AsyncAPI and a repo with some samples:</p><ul><li><a href="https://speakerdeck.com/meteatamel/open-standards-for-building-event-driven-applications-in-the-cloud">Open standards for building event-driven applications in the cloud</a>.</li><li><a href="https://github.com/meteatamel/asyncapi-basics/">asyncapi-basics</a></li></ul><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/05-13_asyncapi_30_send_receive/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=013dd1d6265b" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/asyncapi-gets-a-new-version-3-0-and-new-operations-013dd1d6265b">AsyncAPI gets a new version 3.0 and new operations</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A tour of Gemini 1.5 Pro samples]]></title>
            <link>https://medium.com/google-cloud/a-tour-of-gemini-1-5-pro-samples-da48e3c907b3?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/da48e3c907b3</guid>
            <category><![CDATA[software-engineering]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[genai]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Tue, 07 May 2024 10:37:56 GMT</pubDate>
            <atom:updated>2024-05-07T19:26:56.365Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>Back in February, Google <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024">announced</a> Gemini 1.5 Pro with its impressive 1 million token context window.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/960/0*8L8ozw7hNmwpJHwY.gif" /></figure><p>Larger context size means that Gemini 1.5 Pro can process vast amounts of information in one go — 1 hour of video, 11 hours of audio, 30,000 lines of code or over 700,000 words and the good news is that there’s good language support.</p><p>In this blog post, I will point out some samples utilizing Gemini 1.5 Pro in Google Cloud’s Vertex AI in different use cases and languages (Python, Node.js, Java, C#, Go).</p><h3>Audio</h3><p>Gemini 1.5 Pro can understand audio. For example, listen this <a href="https://storage.googleapis.com/cloud-samples-data/generative-ai/audio/pixel.mp3">audio file</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/304/0*3uUUIOiOV7-U89Ve.png" /></figure><p>It’s 10:28 long but maybe you don’t have time or patience to listen it fully.</p><p>You can use use Gemini to summarize it with Python in <a href="https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_audio.py">gemini_audio.py</a>:</p><pre>def summarize_audio(project_id: str) -&gt; str:<br><br>    import vertexai<br>    from vertexai.generative_models import GenerativeModel, Part<br><br>    vertexai.init(project=project_id, location=&quot;us-central1&quot;)<br><br>    model = GenerativeModel(model_name=&quot;gemini-1.5-pro-preview-0409&quot;)<br><br>    prompt = &quot;&quot;&quot;<br>    Please provide a summary for the audio.<br>    Provide chapter titles with timestamps, be concise and short, no need to provide chapter summaries.<br>    Do not make up any information that is not part of the audio and do not be verbose.<br>    &quot;&quot;&quot;<br><br>    audio_file_uri = &quot;gs://cloud-samples-data/generative-ai/audio/pixel.mp3&quot;<br>    audio_file = Part.from_uri(audio_file_uri, mime_type=&quot;audio/mpeg&quot;)<br><br>    contents = [audio_file, prompt]<br><br>    response = model.generate_content(contents)<br>    print(response.text)<br><br>    return response.text</pre><p>You get a nice summary from Gemini:</p><pre>This episode of the Made by Google podcast discusses the Pixel feature drops,<br>which are software updates that bring new features and improvements to Pixel<br>devices. The hosts, Aisha Sharif and DeCarlos Love, who are both product<br>managers for various Pixel devices, talk about the importance of feature drops<br>in keeping Pixel devices up-to-date and how they use user feedback to decide<br>which features to include in each drop. They also highlight some of their<br>favorite features from past feature drops, such as call screening, direct my<br>call, and clear calling.<br><br>Chapter Titles with Timestamps:<br><br>00:00 Intro<br>00:14 Made by Google Podcast Intro<br>00:35 Transformative Pixel Features<br>01:49 Why Feature Drops Are Important<br>02:28 January Feature Drop Highlights<br>02:58 March Feature Drop: Pixel Watch<br>03:41 March Feature Drop: Pixel Phone <br>05:34 More Portfolio Updates<br>06:09 Pixel Superfans Question<br>07:32 Importance of User Feedback<br>08:07 Feature Drop Release Date<br>08:23 Favorite Feature Drop Features<br>10:17 Outro<br>10:18 Podcast Outro</pre><p>Maybe you want to transcribe the whole audio file instead. Here’s how you can do it with Node.js in <a href="https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/main/generative-ai/snippets/gemini-audio-transcription.js">gemini-audio-transcription.js</a>:</p><pre>async function transcript_audio(projectId = &#39;PROJECT_ID&#39;) {<br>  const vertexAI = new VertexAI({project: projectId, location: &#39;us-central1&#39;});<br><br>  const generativeModel = vertexAI.getGenerativeModel({<br>    model: &#39;gemini-1.5-pro-preview-0409&#39;,<br>  });<br><br>  const filePart = {<br>    file_data: {<br>      file_uri: &#39;gs://cloud-samples-data/generative-ai/audio/pixel.mp3&#39;,<br>      mime_type: &#39;audio/mpeg&#39;,<br>    },<br>  };<br>  const textPart = {<br>    text: `<br>    Can you transcribe this interview, in the format of timecode, speaker, caption?<br>    Use speaker A, speaker B, etc. to identify speakers.`,<br>  };<br><br>  const request = {<br>    contents: [{role: &#39;user&#39;, parts: [filePart, textPart]}],<br>  };<br><br>  const resp = await generativeModel.generateContent(request);<br>  const contentResponse = await resp.response;<br>  console.log(JSON.stringify(contentResponse));<br>}<br><br>transcript_audio(...process.argv.slice(2)).catch(err =&gt; {<br>  console.error(err.message);<br>  process.exitCode = 1;<br>});</pre><p>You get a full transcription (cropped here to keep it short):</p><pre>## Interview Transcription<br><br>**00:00** Speaker A: Your devices are getting better over time, and so we think<br>about it across the entire portfolio, from phones to watch, to buds, to tablet.<br>We get really excited about how we can tell a joint narrative across everything. <br><br>**00:14** Speaker B: Welcome to the Made by Google Podcast, where we meet the<br>people who work on the Google products you love. Here&#39;s your host, Rasheed<br>Finch.<br><br>...<br><br>**10:19** Speaker C: Don’t miss out on new episodes. Subscribe now wherever you<br>get your podcasts to be the first to listen. </pre><h3>Video with audio</h3><p>So far so good, but how about videos?</p><p><a href="https://storage.googleapis.com/cloud-samples-data/generative-ai/video/pixel8.mp4">Take this 57 seconds long video</a> for example:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/633/0*IvClRqCr7mE1oCyY.png" /></figure><p>You can describe the video and everything people said in the video in Java with <a href="https://github.com/GoogleCloudPlatform/java-docs-samples/blob/main/vertexai/snippets/src/main/java/vertexai/gemini/VideoInputWithAudio.java">VideoInputWithAudio.java</a>:</p><pre>  public static String videoAudioInput(String projectId, String location, String modelName)<br>      throws IOException {<br>    try (VertexAI vertexAI = new VertexAI(projectId, location)) {<br>      String videoUri = &quot;gs://cloud-samples-data/generative-ai/video/pixel8.mp4&quot;;<br><br>      GenerativeModel model = new GenerativeModel(modelName, vertexAI);<br>      GenerateContentResponse response = model.generateContent(<br>          ContentMaker.fromMultiModalData(<br>              &quot;Provide a description of the video.\n The description should also &quot;<br>                  + &quot;contain anything important which people say in the video.&quot;,<br>              PartMaker.fromMimeTypeAndData(&quot;video/mp4&quot;, videoUri)<br>          ));<br><br>      String output = ResponseHandler.getText(response);<br>      System.out.println(output);<br><br>      return output;<br>    }<br>  }</pre><p>You get a pretty impressive output:</p><pre>The video is an advertisement for the new Google Pixel phone. It features a<br>photographer in Tokyo who is using the phone to take pictures and  videos of the<br>city at night. The video highlights the phone&#39;s &quot;Night Sight&quot; feature, which<br>allows users to take clear and bright pictures and videos in low-light<br>conditions. The photographer also mentions that the phone&#39;s &quot;Video Boost&quot;<br>feature helps to improve the quality of videos taken in low light. The video<br>shows the photographer taking pictures and videos of various scenes in Tokyo,<br>including the city streets, a bar, and a puddle. The video ends with the<br>photographer saying that the new Pixel phone is &quot;amazing&quot; and that she &quot;loves<br>it.&quot;</pre><h3>All modalities</h3><p>You can go even further and process images, video, audio, and text at the same time. Here’s how to do it in C# with <a href="https://github.com/GoogleCloudPlatform/dotnet-docs-samples/blob/main/aiplatform/api/AIPlatform.Samples/MultimodalAllInput.cs">MultimodalAllInput.cs</a>:</p><pre> public async Task&lt;string&gt; AnswerFromMultimodalInput(<br>        string projectId = &quot;your-project-id&quot;,<br>        string location = &quot;us-central1&quot;,<br>        string publisher = &quot;google&quot;,<br>        string model = &quot;gemini-1.5-pro-preview-0409&quot;)<br>    {<br><br>        var predictionServiceClient = new PredictionServiceClientBuilder<br>        {<br>            Endpoint = $&quot;{location}-aiplatform.googleapis.com&quot;<br>        }.Build();<br><br>        string prompt = &quot;Watch each frame in the video carefully and answer the questions.\n&quot;<br>                  + &quot;Only base your answers strictly on what information is available in &quot;<br>                  + &quot;the video attached. Do not make up any information that is not part &quot;<br>                  + &quot;of the video and do not be too verbose, be to the point.\n\n&quot;<br>                  + &quot;Questions:\n&quot;<br>                  + &quot;- When is the moment in the image happening in the video? &quot;<br>                  + &quot;Provide a timestamp.\n&quot;<br>                  + &quot;- What is the context of the moment and what does the narrator say about it?&quot;;<br><br>        var content = new Content<br>        {<br>            Role = &quot;USER&quot;<br>        };<br>        content.Parts.AddRange(new List&lt;Part&gt;()<br>        {<br>            new() {<br>                Text = prompt<br>            },<br>            new() {<br>                FileData = new() {<br>                    MimeType = &quot;video/mp4&quot;,<br>                    FileUri = &quot;gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4&quot;<br>                }<br>            },<br>            new() {<br>                FileData = new() {<br>                    MimeType = &quot;image/png&quot;,<br>                    FileUri = &quot;gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png&quot;<br>                }<br>            }<br>        });<br><br>        var generateContentRequest = new GenerateContentRequest<br>        {<br>            Model = $&quot;projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}&quot;<br>        };<br>        generateContentRequest.Contents.Add(content);<br><br>        GenerateContentResponse response = await predictionServiceClient.GenerateContentAsync(generateContentRequest);<br><br>        string responseText = response.Candidates[0].Content.Parts[0].Text;<br>        Console.WriteLine(responseText);<br><br>        return responseText;<br>    }</pre><p>The output:</p><pre>- The timestamp of the image is 00:49.<br>- The context is that the narrator, a blind filmmaker, is talking about the<br>  story of his film. The story is about a blind man and his girlfriend, and the<br>  film follows them on their journey together.</pre><h3>Pdf files</h3><p>Gemini 1.5 can even handle Pdf files. Here’s a Go example in <a href="https://github.com/GoogleCloudPlatform/golang-samples/blob/main/vertexai/multimodal-pdf/pdf.go">pdf.go</a> that summarizes a given PDF with the help of Gemini:</p><pre>type pdfPrompt struct {<br> pdfPath string<br> question string<br>}<br><br>func generateContentFromPDF(w io.Writer, prompt pdfPrompt, projectID, location, modelName string) error {<br> // prompt := pdfPrompt{<br> //  pdfPath: &quot;gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf&quot;,<br> //  question: `<br> //   You are a very professional document summarization specialist.<br> //   Please summarize the given document.<br> //  `,<br> // }<br> // location := &quot;us-central1&quot;<br> // modelName := &quot;gemini-1.5-pro-preview-0409&quot;<br> ctx := context.Background()<br><br> client, err := genai.NewClient(ctx, projectID, location)<br> if err != nil {<br>  return fmt.Errorf(&quot;unable to create client: %w&quot;, err)<br> }<br> defer client.Close()<br><br> model := client.GenerativeModel(modelName)<br><br> part := genai.FileData{<br>  MIMEType: &quot;application/pdf&quot;,<br>  FileURI:  prompt.pdfPath,<br> }<br><br> res, err := model.GenerateContent(ctx, part, genai.Text(prompt.question))<br> if err != nil {<br>  return fmt.Errorf(&quot;unable to generate contents: %w&quot;, err)<br> }<br><br> if len(res.Candidates) == 0 ||<br>  len(res.Candidates[0].Content.Parts) == 0 {<br>  return errors.New(&quot;empty response from model&quot;)<br> }<br><br> fmt.Fprintf(w, &quot;generated response: %s\n&quot;, res.Candidates[0].Content.Parts[0])<br> return nil<br>}</pre><p>You get the summary back (cropped here to keep it short):</p><pre>## Gemini 1.5 Pro: A Summary of its Multimodal, Long-Context Capabilities<br><br>**Gemini 1.5 Pro** is a cutting-edge multimodal, large language model (LLM)<br>developed by Google DeepMind. Its most significant advancement lies in its<br>ability to process and understand extremely long contexts of information,<br>spanning millions of tokens across various modalities like text, audio, and<br>video. This represents a substantial leap forward from previous LLMs, which were<br>typically limited to processing hundreds of thousands of tokens.<br><br>Here are the key takeaways from the document:<br>...</pre><h3>System instructions</h3><p>Last but not least, Gemini 1.5 supports <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions">system instructions</a>. System instructions enable users to direct the behavior of the model based on their specific needs and use cases. It’s an additional context to understand the task over the full user interaction with the model.</p><p>For example, here’s a Python example in <a href="https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/generative_ai/gemini_system_instruction.py">gemini_system_instruction.py</a> on how to set system instructions:</p><pre>def set_system_instruction(project_id: str) -&gt; str:<br>    import vertexai<br><br>    from vertexai.generative_models import GenerativeModel<br><br>    vertexai.init(project=project_id, location=&quot;us-central1&quot;)<br><br>    model = GenerativeModel(<br>        model_name=&quot;gemini-1.5-pro-preview-0409&quot;,<br>        system_instruction=[<br>            &quot;You are a helpful language translator.&quot;,<br>            &quot;Your mission is to translate text in English to French.&quot;,<br>        ],<br>    )<br><br>    prompt = &quot;&quot;&quot;<br>    User input: I like bagels.<br>    Answer:<br>    &quot;&quot;&quot;<br><br>    contents = [prompt]<br><br>    response = model.generate_content(contents)<br>    print(response.text)<br><br>    return response.text</pre><p>And with that instruction, the model answers in French:</p><pre>J&#39;aime les bagels.</pre><h3>Conclusion</h3><p>Gemini 1.5 Pro is quite impressive with its multimodal nature and large context size. In this blog post, I provided you pointers to samples for different use cases in different languages. If you want to learn more, here’s a list of further resources:</p><ul><li><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview">Overview of multimodal models</a> documentation page.</li><li><a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_1_5_pro.ipynb">Getting started with the Vertex AI Gemini 1.5 Pro notebook</a></li><li>Python Gemini samples in <a href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/generative_ai">python-docs-samples</a></li><li>Node.js Gemini samples in <a href="https://github.com/GoogleCloudPlatform/nodejs-docs-samples/tree/main/generative-ai/snippets">nodejs-docs-samples</a></li><li>Java Gemini samples in <a href="https://github.com/GoogleCloudPlatform/java-docs-samples/tree/main/vertexai/snippets/src/main/java/vertexai/gemini">java-docs-samples</a></li><li>Go Gemini samples in <a href="https://github.com/GoogleCloudPlatform/golang-samples/tree/main/vertexai">golang-samples</a></li><li>C# Gemini samples in <a href="https://github.com/GoogleCloudPlatform/dotnet-docs-samples/tree/main/aiplatform/api/AIPlatform.Samples">dotnet-docs-samples</a></li></ul><p>As always, for any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/05-07_gemini_15_pro_samples/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=da48e3c907b3" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/a-tour-of-gemini-1-5-pro-samples-da48e3c907b3">A tour of Gemini 1.5 Pro samples</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[C# and Vertex AI Gemini streaming API bug and workaround]]></title>
            <link>https://medium.com/google-cloud/c-and-vertex-ai-gemini-streaming-api-bug-and-workaround-dd5eb61f1f8e?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/dd5eb61f1f8e</guid>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[gemini]]></category>
            <category><![CDATA[dotnet]]></category>
            <category><![CDATA[grpc]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Wed, 01 May 2024 10:55:28 GMT</pubDate>
            <atom:updated>2024-05-01T10:55:28.339Z</atom:updated>
            <content:encoded><![CDATA[<p>A user recently <a href="https://github.com/GoogleCloudPlatform/dotnet-docs-samples/issues/2609">reported</a> an intermittent error with C# and Gemini 1.5 model on Vertex AI’s streaming API. In this blog post, I want to outline what the error is, what causes it, and how to avoid it with the hopes of saving some frustration for someone out there.</p><h3>Error</h3><p>The user reported using Google.Cloud.AIPlatform.V1 library with version 2.27.0 to use Gemini 1.5 via Vertex AI&#39;s streaming API and running into an intermittent System.IO.IOException.</p><p>As a test, I took our <a href="https://github.com/GoogleCloudPlatform/dotnet-docs-samples/blob/main/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs">GeminiQuickstart.cs</a>, change the model from gemini-1.0-pro-vision to gemini-1.5-pro-preview-0409 and ran into the problem after running the sample a few times:</p><pre>[xUnit.net 00:00:08.11]     GeminiQuickstartTest.TestGenerateContentAsync [FAIL]<br>  Failed GeminiQuickstartTest.TestGenerateContentAsync [7 s]<br>  Error Message:<br>   Grpc.Core.RpcException : Status(StatusCode=&quot;Unavailable&quot;, Detail=&quot;Error reading next message. IOException: The request was aborted. IOException: The response ended prematurely while waiting for the next frame from the server.&quot;, DebugException=&quot;System.IO.IOException: The request was aborted.&quot;)<br>---- System.IO.IOException : The request was aborted.<br>-------- System.IO.IOException : The response ended prematurely while waiting for the next frame from the server.<br>  Stack Trace:<br>     at Grpc.Net.Client.Internal.HttpContentClientStreamReader`2.MoveNextCore(CancellationToken cancellationToken)<br>   at Google.Api.Gax.Grpc.AsyncResponseStream`1.MoveNextAsync(CancellationToken cancellationToken)<br>   at GeminiQuickstart.GenerateContent(String projectId, String location, String publisher, String model) in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs:line 82<br>   at GeminiQuickstart.GenerateContent(String projectId, String location, String publisher, String model) in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs:line 82<br>   at GeminiQuickstartTest.TestGenerateContentAsync() in /Users/atamel/dev/github/meteatamel/dotnet-docs-samples/aiplatform/api/AIPlatform.Samples.Tests/GeminiQuickstartTest.cs:line 35<br>--- End of stack trace from previous location ---</pre><h3>Root cause</h3><p>I wasn’t sure what was causing the issue but thankfully, we have the awesome <a href="https://twitter.com/jonskeet">Jon Skeet</a> in our team and after some debugging, he pointed out issues <a href="https://github.com/grpc/grpc-dotnet/issues/2358">2358</a> and <a href="https://github.com/grpc/grpc-dotnet/issues/2361">2361</a> from <a href="https://github.com/grpc/grpc-dotnet">grpc-dotnet</a> project. Basically, there’s a bug in the interaction between .NET gRPC client + the Google L7 load balancer that causes the failure.</p><p>To summarize:</p><ol><li>The issue happens only when the streaming API is used.</li><li>The issue manifests itself intermittently in Gemini 1.5 but it could technically happen in other Gemini versions too.</li></ol><h3>Fix and workarounds</h3><p>The permanent fix is on the way on the .NET side: <a href="https://github.com/dotnet/runtime/pull/97881">dotnet/runtime#9788</a> and it looks like it’ll be available in .NET 9, .NET 8, and backported to previous versions .NET 7, and .NET 6.</p><p>That’s great but what do you do in the meantime? There are a couple of options.</p><p>First, if you don’t require streaming, you can use the non-streaming API. In the <a href="https://github.com/GoogleCloudPlatform/dotnet-docs-samples/blob/main/aiplatform/api/AIPlatform.Samples/GeminiQuickstart.cs">GeminiQuickstart.cs</a> sample, instead of streaming responses like this:</p><pre>using PredictionServiceClient.StreamGenerateContentStream response = predictionServiceClient.StreamGenerateContent(generateContentRequest);<br><br>StringBuilder fullText = new();<br><br>AsyncResponseStream&lt;GenerateContentResponse&gt; responseStream = response.GetResponseStream();<br>await foreach (GenerateContentResponse responseItem in responseStream)<br>{<br>    fullText.Append(responseItem.Candidates[0].Content.Parts[0].Text);<br>}<br><br>return fullText.ToString();</pre><p>You can do a non-streaming call like this:</p><pre>GenerateContentResponse response = await _predictionServiceClient.GenerateContentAsync(generateContentRequest);</pre><p>Of course, this might not be feasible. If you require streaming, thankfully, there are a couple more workarounds.</p><ol><li>You can specify an app switch to disable dynamic window sizing:</li></ol><pre>AppContext.SetSwitch(&quot;System.Net.SocketsHttpHandler.Http2FlowControl.DisableDynamicWindowSizing&quot;, true);</pre><p>2. You can use Grpc.Core instead of Grpc.Net.Client:</p><ul><li>Add a dependency to Grpc.Core version 2.46.6</li><li>Add a using directive for Google.Api.Gax.Grpc</li><li>In the PredictionServiceClientBuilder object initializer, add GrpcAdapter = GrpcCoreAdapter.Instance</li></ul><p>Since the first option is much easier, I tried that and it works great.</p><p>Hopefully this blog post saved some frustration for someone out there and in the worst case, it’ll serve me as a reminder to remove the AppContext workaround once the permanent fix makes it to the .NET runtime 😀</p><p>As always, for any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/05-01_csharp_vertex_gemini_streaming_bug/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dd5eb61f1f8e" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/c-and-vertex-ai-gemini-streaming-api-bug-and-workaround-dd5eb61f1f8e">C# and Vertex AI Gemini streaming API bug and workaround</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Tour of Gemini Code Assist — Slides and Demos]]></title>
            <link>https://medium.com/google-cloud/a-tour-of-gemini-code-assist-slides-and-demos-dbe00fc109c7?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/dbe00fc109c7</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[genai]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Wed, 24 Apr 2024 08:23:17 GMT</pubDate>
            <atom:updated>2024-04-24T08:23:17.323Z</atom:updated>
            <content:encoded><![CDATA[<h3>A Tour of Gemini Code Assist — Slides and Demos</h3><p>This week, I’m speaking at 3 meetups on <a href="https://cloud.google.com/products/gemini/code-assist">Gemini Code Assist</a>. My talk has a little introduction to GenAI and Gemini, followed by a series of hands-on demos that showcase different features of Gemini Code Assist.</p><p>In the demos, I setup Gemini Code Assist in <a href="https://cloud.google.com/code">Cloud Code</a> IDE plugin in Visual Studio Code. Then, I show how to design and create an application, explain, run, generate, test, transform code, and finish with understanding logs with the help of Gemini.</p><p>Here is the <a href="https://speakerdeck.com/meteatamel/improve-your-development-workflow-with-gemini-code-assist">presentation</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qtLefIjXlZBksA4N" /></figure><p>Here’s the <a href="https://github.com/meteatamel/gemini-code-assist-tour/tree/main">gemini-code-assist-tour</a> repository that contains a guided tour of demos.</p><p>As always, for any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/04_24_tour_of_gemini_code_assist/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dbe00fc109c7" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/a-tour-of-gemini-code-assist-slides-and-demos-dbe00fc109c7">A Tour of Gemini Code Assist — Slides and Demos</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AI Gemini generateContent (non-streaming) API]]></title>
            <link>https://medium.com/google-cloud/vertex-ai-gemini-generatecontent-non-streaming-api-3c4965f33cf8?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/3c4965f33cf8</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[vertex-ai]]></category>
            <category><![CDATA[gemini]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 26 Feb 2024 16:58:57 GMT</pubDate>
            <atom:updated>2024-02-26T16:58:57.983Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>In my recent blog post, I’ve been exploring Vertex AI’s Gemini REST API and mainly talked about the <a href="https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.publishers.models/streamGenerateContent">streamGenerateContent</a> method which is a streaming API.</p><p>Recently, a new method appeared in Vertex AI docs: <a href="https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.publishers.models/generateContent">generateContent</a> which is the <strong>non-streaming</strong> (unary) version of the API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/909/1*MBzUYnrVH5WPjkE1nnrHXw.png" /></figure><p>In this short blog post, I take a closer look at the new non-streaming generateContent API and explain why it makes sense to use as a simpler API when the latency is not super critical.</p><h3>Recap: streamGenerateContent method</h3><p>As a recap, this is how you can use the streamGenerateContent method:</p><pre>PROJECT_ID=&quot;genai-atamel&quot;<br>LOCATION=&quot;us-central1&quot;<br>API_ENDPOINT=${LOCATION}-aiplatform.googleapis.com<br>MODEL_ID=&quot;gemini-pro&quot;<br><br>curl -X POST -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    --no-buffer -H &quot;Content-Type: application/json&quot;  \<br>    https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent -d \<br>    $&#39;{<br>      &quot;contents&quot;: {<br>        &quot;role&quot;: &quot;USER&quot;,<br>        &quot;parts&quot;: { &quot;text&quot;: &quot;Why is the sky blue?&quot; }<br>      },<br>      &quot;generation_config&quot;:{<br>        &quot;temperature&quot;: 0.4,<br>        &quot;top_p&quot;: 1,<br>        &quot;top_k&quot;: 32,<br>        &quot;max_output_tokens&quot;: 2048<br>      }<br>  }&#39;</pre><p>And this is the sort of response you’d get:</p><pre>[{<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot;The sky appears blue due to a phenomenon called Rayleigh scattering. This occurs when sunlight, which is composed of all colors of the visible spectrum, passes through the Earth&quot;<br>            }<br>          ]<br>        },<br>        &quot;safetyRatings&quot;: [<br>          ...<br>      }<br>    ]<br>  }<br>  ,<br>  {<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot;&#39;s atmosphere. The atmosphere is made up of tiny particles, such as molecules and dust, that are much smaller than the wavelength of visible light.\n\nAs&quot;<br>            }<br>          ]<br>        },<br>        &quot;safetyRatings&quot;: [<br>          ...<br>        ]<br>      }<br>    ]<br>  }<br>  ,<br>  {<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot; sunlight passes through the atmosphere, these particles scatter the light in all directions. However, the amount of scattering depends on the wavelength of the light. Shorter wavelengths, such as blue light, are scattered more than longer wavelengths, such as red light. This is because the shorter wavelengths have a higher frequency and therefore interact more with&quot;<br>            }<br>          ]<br>        },<br>        &quot;safetyRatings&quot;: [<br>          ...<br>        ]<br>      }<br>    ]<br>  }<br>  ,<br>  {<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot; the particles in the atmosphere.\n\nAs a result, more blue light is scattered in all directions, which means that when we look up at the sky, we see more blue light than any other color. This is why the sky appears blue during the day.\n\nAt sunset and sunrise, the sunlight has to travel through&quot;<br>            }<br>          ]<br>        },<br>        &quot;safetyRatings&quot;: [<br>          ...<br>        ]<br>      }<br>    ]<br>  }<br>  ,<br>  {<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot; more of the atmosphere to reach our eyes. This means that more of the blue light is scattered away, and we see more of the longer wavelengths, such as red and orange. This is why the sky appears red or orange at these times of day.&quot;<br>            }<br>          ]<br>        },<br>        &quot;finishReason&quot;: &quot;STOP&quot;,<br>        &quot;safetyRatings&quot;: [<br>          ...<br>        ]<br>      }<br>    ],<br>    &quot;usageMetadata&quot;: {<br>      ...<br>    }<br>  }<br>  ]</pre><p>Notice how the text is split in multiple chunks and the last chunk has finishReason: STOP to indicate that it&#39;s the last one. Since this is a streaming API, you&#39;d receive these chunks as they become available.</p><p>This is useful if you have a latency sensitive application such as a chat application. But it makes processing the response more complicated, as you need to combine the text in each chunk into a final text.</p><h3>New: generateContent method</h3><p>If you have an application where latency is not that important and you’d rather wait for the whole response before displaying anything to the user, then <a href="https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.publishers.models/generateContent">generateContent</a> method is more appropriate. It is the <strong>non-streaming</strong> (unary) version of the API.</p><p>Usage of the generateContent is very similar to before:</p><pre>PROJECT_ID=&quot;genai-atamel&quot;<br>LOCATION=&quot;us-central1&quot;<br>API_ENDPOINT=${LOCATION}-aiplatform.googleapis.com<br>MODEL_ID=&quot;gemini-pro&quot;<br><br>curl -X POST -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \<br>    -H &quot;Content-Type: application/json&quot;  \<br>    https://${API_ENDPOINT}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent -d \<br>    $&#39;{<br>      &quot;contents&quot;: {<br>        &quot;role&quot;: &quot;USER&quot;,<br>        &quot;parts&quot;: { &quot;text&quot;: &quot;Why is the sky blue?&quot; }<br>      },<br>      &quot;generation_config&quot;:{<br>        &quot;temperature&quot;: 0.4,<br>        &quot;top_p&quot;: 1,<br>        &quot;top_k&quot;: 32,<br>        &quot;max_output_tokens&quot;: 2048<br>      }<br>  }&#39;</pre><p>But the response is different:</p><pre>{<br>    &quot;candidates&quot;: [<br>      {<br>        &quot;content&quot;: {<br>          &quot;role&quot;: &quot;model&quot;,<br>          &quot;parts&quot;: [<br>            {<br>              &quot;text&quot;: &quot;The sky appears blue because of a phenomenon called Rayleigh scattering. This occurs when sunlight passes through the Earth&#39;s atmosphere and interacts with molecules of nitrogen and oxygen. These molecules are much smaller than the wavelength of visible light, so they scatter the light in all directions. However, blue light is scattered more than other colors because it has a shorter wavelength. This means that more blue light reaches our eyes from all directions, making the sky appear blue.&quot;<br>            }<br>          ]<br>        },<br>        &quot;finishReason&quot;: &quot;STOP&quot;,<br>        &quot;safetyRatings&quot;: [<br>          ...<br>        ],<br>        &quot;citationMetadata&quot;: {<br>          ...<br>        }<br>      }<br>    ],<br>    &quot;usageMetadata&quot;: {<br>      ...<br>    }<br>  }</pre><p>As you can see, we got back a single chunk with the full text and finishReason: STOP. This is certainly much simpler to process and a better choice for applications where you don&#39;t need to display responses right away.</p><h3>Summary</h3><p>In this short blog post, I showed you how to use the non-streaming generateContent API and explain why it makes sense to use as a simpler API when the latency is not super critical.</p><p>If you want to run these samples yourself, you can check out my GenAI repo on GitHub:</p><ul><li><a href="https://github.com/meteatamel/genai-samples/tree/main/vertexai/gemini/console/curl">https://github.com/meteatamel/genai-samples/tree/main/vertexai/gemini/console/curl</a></li></ul><p>As always, for any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/02-26_vertexai_gemini_generate_content_api/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3c4965f33cf8" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/vertex-ai-gemini-generatecontent-non-streaming-api-3c4965f33cf8">Vertex AI Gemini generateContent (non-streaming) API</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Vertex AI Gemini from GAPIC libraries (C#)]]></title>
            <link>https://medium.com/google-cloud/using-vertex-ai-gemini-from-gapic-libraries-c-390ee4066002?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/390ee4066002</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[software-architecture]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Wed, 14 Feb 2024 12:11:12 GMT</pubDate>
            <atom:updated>2024-02-14T12:11:12.367Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KrbRuu5wgOHtgL0P.png" /></figure><h3>Introduction</h3><p>In my previous <a href="https://atamel.dev/posts/2024/02-05_vertexai_gemini_restapi_csharp_rust/">Using Vertex AI Gemini REST API</a> post, I showed how to use the Gemini REST API from languages without SDK support yet such as C# and Rust.</p><p>There’s actually another way to use Gemini from languages without SDK support: <strong>GAPIC libraries</strong>. In this post, I show you how to use Vertex AI Gemini from GAPIC libraries, using C# as an example.</p><h3>What is GAPIC?</h3><p><strong>At this point, you might be wondering: What’s GAPIC?</strong> GAPIC stands for Google API CodeGen. In Google Cloud, all services have auto-generated libraries from Google’s service proto files. Since these libraries are auto-generated, they’re not the easiest and most intuitive way of calling a service. Because of that, some services also have hand-written SDKs/libraries on top of GAPIC libraries.</p><p>Specifically for Vertex AI and Gemini, there are hand-written SDKs for Python, Java, Node.js, and Go. You’d prefer these hand-written SDKs over GAPIC versions.</p><p>For other languages, you can either rely on the REST API or you can use the GAPIC library for that language, if it exists.</p><h3>REST vs. GAPIC</h3><p>As an example, in C#, there is a GAPIC library for Vertex AI. It’s mostly through the <a href="https://cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1.PredictionService.PredictionServiceClient">PredictionServiceClient</a> of<a href="https://cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest/Google.Cloud.AIPlatform.V1">Google.Cloud.AIPlatform.V1</a> library. For other languages, you&#39;d search for similarly named classes and libraries to see if there&#39;s GAPIC support.</p><p>When you have the choice to use the REST API or the GAPIC library, which one should you use? It’s really up to you but keep in mind:</p><ul><li>The REST API is probably more intuitive but you need to handle authentication and you need to parse the returned JSON to strongly typed objects on your own.</li><li>The GAPIC library provides authentication and strongly typed classes but you need to figure out how to construct the right protobuf classes, so it can be a little unintuitive.</li></ul><h3>Gemini REST API from C#</h3><p>As a reminder, in my <a href="https://atamel.dev/posts/2024/02-05_vertexai_gemini_restapi_csharp_rust/">Using Vertex AI Gemini REST API</a> post, I showed how to use the Gemini REST API from C# to describe an image of a cat. The full sample is in my GitHub repo in <a href="https://github.com/meteatamel/genai-samples/blob/main/vertexai/gemini/console/csharp/rest/GenerateTextFromImageGcs.cs">GenerateTextFromImageGcs.cs</a>.</p><h3>Gemini GAPIC library from C#</h3><p>Now, let’s build the same sample but this time using the GAPIC library.</p><h4>Define prompt and image</h4><p>Define the prompt and the image stored on Cloud Storage:</p><pre>string text = &quot;Describe this image in detail in a few paragraphs&quot;;<br>string imageUrl = &quot;gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg&quot;;<br>Console.WriteLine($&quot;Text: {text}&quot;);<br>Console.WriteLine($&quot;ImageUrl: {imageUrl}&quot;);</pre><p>It’s an image of a cat 🙂</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/320/0*Llaq3u_20w_4A6eV.png" /></figure><p>While we’re at it, also define some constants we’ll need later:</p><pre>const string ProjectId = &quot;genai-atamel&quot;;<br>const string Location = &quot;us-central1&quot;;<br>const string Publisher = &quot;google&quot;;<br>const string Model = &quot;gemini-pro-vision&quot;;</pre><h3>Create a prediction service client</h3><p>Create a PredictionServiceClient that you&#39;ll use to make requests to Vertex AI:</p><pre>// Create client<br>var predictionServiceClient = new PredictionServiceClientBuilder<br>{<br>    Endpoint = $&quot;{Location}-aiplatform.googleapis.com&quot;<br>}.Build();</pre><h3>Construct the request payload</h3><p>Next, construct the request payload with the right parameters. This can be tricky to get right:</p><pre>// Initialize request argument(s)<br>var content = new Content<br>{<br>    Role = &quot;USER&quot;<br>};<br>content.Parts.AddRange(<br>[<br>    new() {<br>        Text = prompt<br>    },<br>    new() {<br>        FileData = new() {<br>            MimeType = &quot;image/png&quot;,<br>            FileUri = imageUrl<br>        }<br>    }<br>]);<br><br>var generateContentRequest = new GenerateContentRequest<br>{<br>    Model = $&quot;projects/{ProjectId}/locations/{Location}/publishers/{Publisher}/models/{Model}&quot;,<br>    GenerationConfig = new GenerationConfig<br>    {<br>        Temperature = 0.4f,<br>        TopP = 1,<br>        TopK = 32,<br>        MaxOutputTokens = 2048<br>    }<br>};<br>generateContentRequest.Contents.Add(content);</pre><h3>Send the request and parse the streaming response</h3><p>Sending the request is a simple call:</p><pre>// Make the request, returning a streaming response<br>using PredictionServiceClient.StreamGenerateContentStream response = <br>    predictionServiceClient.StreamGenerateContent(generateContentRequest);</pre><p>Note that the authentication is handled for us, so we don’t need to do anything special to pass the auth token or anything like that.</p><p>The response is streaming, you can print it out as it comes back:</p><pre>Console.Write($&quot;Response: &quot;);<br><br>// Read streaming responses from server until complete<br>AsyncResponseStream&lt;GenerateContentResponse&gt; responseStream = response.GetResponseStream();<br>while (await responseStream.MoveNextAsync())<br>{<br>    GenerateContentResponse responseItem = responseStream.Current;<br>    Console.WriteLine(responseItem.Candidates[0].Content.Parts[0].Text);<br>}</pre><p>You can see the full sample in my GitHub repo in <a href="https://github.com/meteatamel/genai-samples/blob/main/vertexai/gemini/console/csharp/sdk/GenerateTextFromImageGcs.cs">GenerateTextFromImageGcs.cs</a>.</p><h3>Run the sample</h3><p>Run the sample:</p><pre>dotnet run<br><br>Prompt: Describe this image in detail in a few paragraphs<br>ImageUrl: gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg<br>Response:  This is an image of a cat in the snow. The cat is standing on a<br>snow-covered field, and it appears to be looking off to the side. It is a<br>short-haired tabby cat with a white belly and paws. The cat&#39;s fur is brown and<br>black, with some white patches on its chest and belly. The cat&#39;s eyes are<br>yellow, and its nose is pink. The cat&#39;s tail is long and fluffy, and it is<br>curled up at the end. The cat&#39;s body is slightly hunched, and its ears are<br>perked up. The cat&#39;s fur is matted and dirty, and it appears to be in poor<br>condition. The cat&#39;s surroundings are covered in snow, and there are no other<br>animals or people visible in the image.</pre><p>You’ll realize that the response will stream as it’s available. That’s nice.</p><h3>Summary</h3><p>Ultimately, the language of your choice determines what SDK/library to use to talk to Gemini on Vertex AI:</p><ul><li>Hand-written SDKs for Python, Java, Node.js, Go.</li><li>GAPIC libraries for other Google Cloud supported languages like C#.</li><li>REST API for everything else.</li></ul><p>As always, if you have any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/02-14_vertexai_gemini_gapic_libraries/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=390ee4066002" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-vertex-ai-gemini-from-gapic-libraries-c-390ee4066002">Using Vertex AI Gemini from GAPIC libraries (C#)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Vertex AI Gemini REST API (C# and Rust)]]></title>
            <link>https://medium.com/google-cloud/using-vertex-ai-gemini-rest-api-c-and-rust-cf6a488e19b1?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/cf6a488e19b1</guid>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 05 Feb 2024 09:31:04 GMT</pubDate>
            <atom:updated>2024-02-05T09:32:02.240Z</atom:updated>
            <content:encoded><![CDATA[<h3>Introduction</h3><p>Back in December, Google announced <a href="https://blog.google/technology/ai/google-gemini-ai/">Gemini</a>, its most capable and general model so far available from <a href="https://makersuite.google.com/">Google AI Studio</a> and<a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal"> Google Cloud Vertex AI</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3cRr0AR_jwaCu0Da.png" /></figure><p>The <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-multimodal">Try the Vertex AI Gemini API</a> documentation page shows instructions on how to use the Gemini API from <strong>Python</strong>, <strong>Node.js</strong>, <strong>Java</strong>, and <strong>Go</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/787/0*kRWZtoqMeU7YXpuO.png" /></figure><p>That’s great but what about other languages?</p><p>Even though there are no official SDKs/libraries for other languages yet, you can use the <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini">Gemini REST API</a> to access the same functionality with a little bit more work on your part.</p><p>In this blog post, I want to take a look at an example on how to use the Gemini REST API from languages without SDK support yet: C# and Rust in this case.</p><h3>Gemini REST API</h3><p>There are currently two models available in the Gemini API:</p><ul><li><strong>Gemini Pro</strong> model (gemini-pro): Fine-tuned model to handle natural language tasks such as classification, summarization, extraction, and writing.</li><li><strong>Gemini Pro Vision</strong> model (gemini-pro-vision): Multimodal model that supports adding image and video prompts for a text response.</li></ul><p><a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini">Gemini API</a> page is a great resource on learning how to make the right HTTP requests with the right parameters.</p><p>For example, to send a multi-modal (text + image) request to Gemini, you’d make an HTTP POST to gemini-pro-vision model with the right parameters in the request body:</p><pre>POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-pro-vision:streamGenerateContent<br><br>{<br>  &quot;contents&quot;: {<br>    &quot;role&quot;: &quot;user&quot;,<br>    &quot;parts&quot;: [<br>      {<br>        &quot;fileData&quot;: {<br>          &quot;mimeType&quot;: &quot;image/png&quot;,<br>          &quot;fileUri&quot;: &quot;gs://cloud-samples-data/ai-platform/flowers/daisy/10559679065_50d2b16f6d.jpg&quot;<br>        }<br>      },<br>      {<br>        &quot;text&quot;: &quot;Describe this picture.&quot;<br>      }<br>    ]<br>  },<br>  &quot;safety_settings&quot;: {<br>    &quot;category&quot;: &quot;HARM_CATEGORY_SEXUALLY_EXPLICIT&quot;,<br>    &quot;threshold&quot;: &quot;BLOCK_LOW_AND_ABOVE&quot;<br>  },<br>  &quot;generation_config&quot;: {<br>    &quot;temperature&quot;: 0.4,<br>    &quot;topP&quot;: 1.0,<br>    &quot;topK&quot;: 32,<br>    &quot;maxOutputTokens&quot;: 2048<br>  }<br>}</pre><p>A couple of things to watch out for:</p><ol><li>You need to get and set an authentication token with your request.</li><li>The responses come in batches (see <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#sample_responses">sample responses</a>), so you need to extracttext from each batch and combine them to get the full text.</li></ol><p>Now, let’s take a look at how to make these requests from actual code.</p><h3>Gemini REST API from C#</h3><p>Let’s say we want Gemini to describe an image in Cloud Storage for us from my default language: C#.</p><h3>Define prompt and image</h3><p>Let’s define the prompt and the image stored on Cloud Storage:</p><pre>string text = &quot;Describe this image&quot;;<br>string imageUrl = &quot;gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg&quot;;<br>Console.WriteLine($&quot;Text: {text}&quot;);<br>Console.WriteLine($&quot;ImageUrl: {imageUrl}&quot;);</pre><p>It’s an image of a cat 🙂</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/320/0*F9LQx8aBMOQT_koF.png" /></figure><h3>Construct the request payload</h3><p>Construct the right request payload with prompt and image url:</p><pre>private static string GeneratePayload(string text, string imageUrl)<br>{<br>    var payload = new<br>    {<br>        contents = new<br>        {<br>            role = &quot;USER&quot;,<br>            parts = new object[] {<br>                new {text = text},<br>                new {file_data = new {<br>                        mime_type = &quot;image/png&quot;,<br>                        file_uri = imageUrl<br>                    }<br>                }<br>            }<br>        },<br>        generation_config = new<br>        {<br>            temperature = 0.4,<br>            top_p = 1,<br>            top_k = 32,<br>            max_output_tokens = 2048<br>        }<br>    };<br>    return JsonConvert.SerializeObject(payload);<br>}</pre><h3>Send the request with auth token</h3><p>Get an authentication token and send the HTTP request:</p><pre>private async static Task&lt;string&gt; SendRequest(string payload)<br>{<br>    GoogleCredential credential = GoogleCredential.GetApplicationDefault();<br>    var handler = credential.ToDelegatingHandler(new HttpClientHandler());<br>    using HttpClient httpClient = new(handler);<br><br>    httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(&quot;application/json&quot;));<br><br>    HttpResponseMessage response = await httpClient.PostAsync(EndpointUrl,<br>        new StringContent(payload, Encoding.UTF8, &quot;application/json&quot;));<br><br>    response.EnsureSuccessStatusCode();<br><br>    return await response.Content.ReadAsStringAsync();<br>}</pre><h3>Parse the response</h3><p>Receive the HTTP response from Gemini and deserialize the JSON response body. JSON response body will have a list of candidates to parse through (see <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini#response_body">response body</a>). It makes sense to create a<a href="https://github.com/meteatamel/genai-samples/blob/main/vertexai/gemini/console/csharp/rest/GeminiResponse.cs">GeminiResponse.cs</a> class to capture this to make our lives easier in JSON deserialization:</p><pre>string payload = GeneratePayload(text, imageUrl);<br>string response = await SendRequest(payload);<br>var geminiResponses = JsonConvert.DeserializeObject&lt;List&lt;GeminiResponse&gt;&gt;(response);</pre><p>Finally, use some LINQ magic to combine the text in each batch into a final text:</p><pre>string fullText = string.Join(&quot;&quot;, geminiResponses<br>    .SelectMany(response =&gt; response.Candidates)<br>    .SelectMany(candidates =&gt; candidates.Content.Parts)<br>    .Select(part =&gt; part.Text));<br><br>Console.WriteLine($&quot;Response: {fullText}&quot;);</pre><p>You can see the full sample in my GitHub repo in <a href="https://github.com/meteatamel/genai-samples/blob/main/vertexai/gemini/console/csharp/rest/GenerateTextFromImageGcs.cs">GenerateTextFromImageGcs.cs</a>.</p><h3>Run the sample</h3><p>Run the sample:</p><pre>dotnet run<br><br>Text: Describe this image<br>ImageUrl: gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg<br>Response:  A cat is walking in the snow. The cat is gray and white, and it has a long tail. The cat is looking at the camera. The snow is white and it is covering the ground.</pre><p>That’s a nice and pretty good description of the image from Gemini!</p><h3>Gemini REST API from Rust</h3><p>Of course, you can use the REST API from any language. My colleague from the Chrome DevRel team, <a href="https://twitter.com/andreban">André Bandarra</a>, rewrote my sample in Rust. It follows the same pattern of getting an auth token, generating the request with the right parameters and combining the text from the response:</p><pre>let authentication_manager = AuthenticationManager::new().await?;<br>let scopes = &amp;[&quot;https://www.googleapis.com/auth/cloud-platform&quot;];<br>let token = authentication_manager.get_token(scopes).await?;<br><br>let prompt = &quot;Describe this image&quot;;<br>let image_url = &quot;gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg&quot;;<br><br>let payload = GenerateContentRequest {<br>    contents: vec![Content {<br>        role: &quot;user&quot;.to_string(),<br>        parts: vec![<br>            Part::Text(prompt.to_string()),<br>            Part::FileData {<br>                mime_type: &quot;image/jpeg&quot;.to_string(),<br>                file_uri: image_url.to_string(),<br>            },<br>        ],<br>    }],<br>    generation_config: Some(GenerationConfig {<br>        max_output_tokens: Some(2048),<br>        temperature: Some(0.4),<br>        top_p: Some(1.0),<br>        top_k: Some(32),<br>        ..Default::default()<br>    }),<br>    tools: None,<br>};<br><br>let resp = reqwest::Client::new()<br>    .post(&amp;endpoint_url)<br>    .bearer_auth(token.as_str())<br>    .json(&amp;payload)<br>    .send()<br>    .await?;<br><br>let response = resp.json::&lt;GenerateContentResponse&gt;().await?;<br>response.0.iter().for_each(|chunk| {<br>    chunk.candidates.iter().for_each(|candidate| {<br>        candidate.content.parts.iter().for_each(|part| {<br>            if let Part::Text(text) = part {<br>                print!(&quot;{}&quot;, text);<br>            }<br>        });<br>    });<br>});</pre><p>You can check out the full sample in his repo in <a href="https://github.com/andreban/gemini-rust/blob/main/src/bin/generate-text-from-image-gcs.rs">generate-text-from-image-gcs.rs</a>.</p><h3>Summary</h3><p>Admittedly, using the <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini">Gemini REST API</a> is not easy as it can be as other languages that have Gemini SDK support. However, with a little bit of work in making the right request and parsing the response, it’s straightforward to talk to Gemini with the REST API from any other language. To see more Gemini samples, you can check out our repos on GitHub:</p><ul><li><a href="https://github.com/meteatamel/genai-samples">https://github.com/meteatamel/genai-samples</a></li><li><a href="https://github.com/andreban/gemini-rust">https://github.com/andreban/gemini-rust</a></li></ul><p>As always, if you have any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/02-05_vertexai_gemini_restapi_csharp_rust/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cf6a488e19b1" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/using-vertex-ai-gemini-rest-api-c-and-rust-cf6a488e19b1">Using Vertex AI Gemini REST API (C# and Rust)</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Test and change an existing web app with Duet AI]]></title>
            <link>https://medium.com/google-cloud/test-and-change-an-existing-web-app-with-duet-ai-863380c9a9d6?source=rss-6444b44155fa------2</link>
            <guid isPermaLink="false">https://medium.com/p/863380c9a9d6</guid>
            <category><![CDATA[google-cloud-platform]]></category>
            <category><![CDATA[generative-ai-tools]]></category>
            <category><![CDATA[software-engineering]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Mete Atamel]]></dc:creator>
            <pubDate>Mon, 29 Jan 2024 14:40:51 GMT</pubDate>
            <atom:updated>2024-01-29T14:40:51.525Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/229/0*Od1BS44Yh75OVjQx.png" /></figure><p>In the <a href="https://atamel.dev/posts/2024/01-23_duetai_create_deploy_webapp_clourun/">Create and deploy a new web app to Cloud Run with Duet AI</a> post, I created a simple web application and deployed to Cloud Run using <a href="https://cloud.google.com/duet-ai">Duet AI</a>’s help. Duet AI has been great to get a new and simple app up and running. But does it help for existing apps? Let’s figure it out.</p><p>In this blog post, I take an existing web app, explore it, test it, add a unit test, add new functionality, and add more unit tests all with the help of Duet AI. Again, I captured some lessons learned along the way to get the most out of Duet AI.</p><h3>Before you start</h3><p>I’m using Duet AI with Cloud Code in Visual Studio Code but you can use Duet AI from other IDEs supported by Cloud Code such as IntelliJ. You can also use it in the Cloud Shell Editor or Cloud Workstations from Google Cloud Console. Check out <a href="https://cloud.google.com/duet-ai/docs/discover/developers">Duet AI for Developers</a> page for more details on how to get setup with Duet AI.</p><h3>Todo list app</h3><p>For the application, I went through the <a href="https://learn.microsoft.com/en-us/aspnet/core/tutorials/first-web-api?view=aspnetcore-8.0&amp;WT.mc_id=dotnet-35129-website&amp;tabs=visual-studio-code">Tutorial: Create a web API with ASP.NET Core</a> to create a Todo list app with MVC architecture and a web API.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/870/0*h7hZkuI-xpbRVlUh.png" /></figure><p>Or, you can start with the finished tutorial app in <a href="https://github.com/meteatamel/duetai-aspnet-webapi/tree/main/before/TodoApi">before</a> folder in my GitHub repo.</p><p>Check that you can run the application:</p><pre>cd before/TodoApi <br>dotnet run</pre><p>You should see the app running:</p><pre>Building...<br>info: Microsoft.Hosting.Lifetime[14]<br>      Now listening on: http://localhost:5112<br>info: Microsoft.Hosting.Lifetime[0]<br>      Application started. Press Ctrl+C to shut down.</pre><p>You can also interact with the app with Swagger/OpenAPI endpoint at <a href="http://localhost:5112/swagger/:">http://localhost:5112/swagger/:</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/552/0*J95wn0JV95r0-Js2.png" /></figure><p>We’re ready to use Duet AI’s help.</p><h3>Explain code with Duet AI</h3><p>First, let’s see if Duet AI can help us to understand the code.</p><p>Open <a href="https://github.com/meteatamel/duetai-aspnet-webapi/blob/main/before/TodoApi/Controllers/TodoItemsController.cs">TodoItemsController.cs</a> in your IDE and ask the following to Duet AI using the Duet AI chat functionality:</p><p><strong>Prompt</strong></p><blockquote><em>Can you explain what this class does?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>The TodoItemsController class is a RESTful API controller that provides CRUD operations for the TodoItem entity. The controller inherits from the ControllerBase class, which provides a number of common methods for handling HTTP requests.</em></blockquote><blockquote><em>The GetTodoItems() method returns a list of all TodoItems in the database. The GetTodoItem() method returns a single TodoItem by its id. The PutTodoItem() method updates a TodoItem in the database. The PostTodoItem() method creates a new TodoItem in the database. The DeleteTodoItem() method deletes a TodoItem from the database.</em></blockquote><blockquote><em>The controller also implements the IActionFilter interface, which allows it to perform custom actions before and after each request. In this case, the controller uses the IActionFilter interface to validate the input parameters for each request and to ensure that the user is authenticated.</em></blockquote><blockquote><em>The TodoItemsController class is a good example of how to create a RESTful API using ASP.NET Core. The controller is well-organized and easy to use, and it provides a number of features that make it a good choice for building APIs.</em></blockquote><p>That was a pretty good explanation and gave me a good overview of the code!</p><h3>Test code with Duet AI</h3><p>Next, let’s see if Duet AI can help us to test the code.</p><p>Highlight the <a href="https://github.com/meteatamel/duetai-aspnet-webapi/blob/main/before/TodoApi/Controllers/TodoItemsController.cs#L73">PostTodoItem()</a> method in TodoItemsController.cs:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/760/0*cy4qSeZvX2cC-z-s" /></figure><p>Ask the following:</p><p><strong>Prompt</strong></p><blockquote><em>Can you help me test this highlighted code with curl? The server is running on port 5112</em></blockquote><p><strong>Answer</strong></p><blockquote><em>Sure. To test the highlighted code with curl, you can use the following command:</em></blockquote><blockquote>curl -X POST \<br> -H “Content-Type: application/json” \<br> -d ‘{<br> “name”: “Learn ASP.NET Core”,<br> “description”: “Build a web API with ASP.NET Core”,<br> “priority”: 4,<br> “done”: false<br> }’ \<br> <a href="http://localhost:5112/api/TodoItems">http://localhost:5112/api/TodoItems</a></blockquote><blockquote><em>This command will send a POST request to the api/TodoItems endpoint on the server, with the body of the request containing the JSON representation of a new TodoItem object. The server will then create the new TodoItem and return a response with the status code 201 Created.</em></blockquote><blockquote><em>You can then use the curl command to get the newly created TodoItem by using the following command:</em></blockquote><blockquote>curl <a href="http://localhost:5112/api/TodoItems/1">http://localhost:5112/api/TodoItems/1</a></blockquote><blockquote><em>This command will send a GET request to the api/TodoItems/1 endpoint on the server, and the server will return a response with the status code 200 OK and the body of the response containing the JSON representation of the newly created TodoItem.</em></blockquote><p>As you can see, the command works but it isn’t correct. ToDoItem.cs contains id, name, and isComplete fields. Duet AI got the name correctly but the rest was not correct.</p><p>Let’s try a more specific prompt.</p><p><strong>Prompt</strong></p><blockquote><em>Can you try generating the curl command, taking </em><em>ToDoItem.cs into account?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>Sure. The following command will send a POST request to the api/TodoItems endpoint on the server, with the body of the request containing the JSON representation of a new TodoItem object:</em></blockquote><pre>curl -X POST \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{ &quot;name&quot;: &quot;Learn ASP.NET Core&quot;, &quot;isComplete&quot;: false }&#39; \ http://localhost:5112/api/TodoItems</pre><blockquote><em>This command will create a new TodoItem with the name “Learn ASP.NET Core” and the value of IsComplete set to false.</em></blockquote><p>This time the answer was correct! Duet AI needs good context to provide good answers.</p><h3>Add unit tests with Duet AI</h3><p>Next, let’s see if Duet AI can help us to add unit tests.</p><p>First, let’s refresh our memories about the unit test frameworks in C#.</p><p><strong>Prompt</strong></p><blockquote><em>What unit testing frameworks exist for ASP.NET apps?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>There are a number of unit testing frameworks available for ASP.NET apps. Some of the most popular include:</em></blockquote><blockquote><em>xUnit: xUnit is a free and open source unit testing framework for .NET. It is based on the xUnit unit testing framework for Java. xUnit is a popular choice for ASP.NET apps because it is easy to use and has a large community of users and contributors.</em></blockquote><blockquote><em>NUnit: NUnit is a free and open source unit testing framework for .NET. It is based on the JUnit unit testing framework for Java. NUnit is a popular choice for ASP.NET apps because it is powerful and has a large community of users and contributors.</em></blockquote><blockquote><em>MSTest: MSTest is a built-in unit testing framework for Visual Studio. It is a good choice for ASP.NET apps because it is easy to use and integrates well with Visual Studio.</em></blockquote><blockquote><em>In addition to these three frameworks, there are a number of other unit testing frameworks available for ASP.NET apps. Some of these include FluentAssertions, Shouldly, and AssertJ.</em></blockquote><blockquote><em>The best unit testing framework for your ASP.NET app will depend on your specific needs and preferences. If you are new to unit testing, xUnit or NUnit are a good place to start. If you are more experienced with unit testing, MSTest or one of the other more powerful frameworks may be a better choice.</em></blockquote><p>This is a pretty good answer! I remember using xUnit before, so let’s continue with more specific xUnit questions.</p><blockquote><em>How can I create a new xUnit project for unit tests for </em><em>TodoItemsController.cs from the command line? I want the project in </em><em>ToDoApiTest folder</em></blockquote><p><strong>Answer</strong></p><blockquote><em>To create a new xUnit project for unit tests for </em><em>TodoItemsController.cs from the command line, you can use the following steps:</em></blockquote><ul><li>Open a terminal window.</li><li>Navigate to the ToDoApiTest folder.</li><li>Run the following command:</li></ul><blockquote><em>This will create a new xUnit project with the name UnitTest1.</em></blockquote><blockquote><em>Open the</em><em> UnitTest1.csproj file in a text editor. Add the following code to the file:</em></blockquote><pre>using TodoApi.Controllers;<br>using Xunit;<br><br>namespace TodoApi.Tests;<br><br>public class TodoItemsControllerTests<br>{<br>     [Fact]<br>     public void GetTodoItems_ShouldReturnAllTodoItems()<br>     {<br>         // Arrange<br>         var controller = new TodoItemsController();<br><br>         // Act<br>         var result = controller.GetTodoItems();<br><br>         // Assert<br>         Assert.Equal(3, result.Count());<br>     }<br>}</pre><p>This answer is pretty good for generating a new test project but it has some issues:</p><ol><li>Duet AI is telling us to add code to the project file, UnitTest1.csproj instead of UnitTest1.cs. You an go ahead and rename UnitTest1.cs to ToDoItemsControllerTests.cs and add the suggested code.</li><li>ToDoItemsController is another project, so we need to refer to the other project from the test project.</li></ol><p>Here comes another question to Duet AI:</p><p><strong>Prompt</strong></p><blockquote><em>How can I change the project file </em><em>ToDoApiTest.csproj to refer to the </em><em>TodoItemsController class in the other project </em><em>ToDoApi.csproj?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>To change the project file </em><em>ToDoApiTest.csproj to refer to the </em><em>TodoItemsController class in the other project ToDoApi.csproj, you can use the following steps: [1]</em></blockquote><ul><li>Open the ToDoApiTest.csproj file in a text editor.</li><li>Find the line that references the TodoItemsController class.</li><li>Change the path to the TodoItemsController class to the path in the ToDoApi.csproj file.</li><li>Save the ToDoApiTest.csproj file.</li><li>Here is an example of how you would change the project file ToDoApiTest.csproj to refer to the TodoItemsController class in the other project ToDoApi.csproj:</li></ul><p>While the answer is not perfect ( Controller Include is not needed), it gives you what you need, just copy the ProjectReference Include to your testing project (see <a href="https://github.com/meteatamel/duetai-aspnet-webapi/blob/main/after/TodoApiTest/TodoApiTest.csproj">TodoApiTest.csproj</a>)</p><p>Now, the generated testing code is still not compiling:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/554/0*UCouCIepE4_FYvSG.png" /></figure><p>We need a mocked DB context for the controller.</p><p><strong>Prompt</strong></p><blockquote><em>How can I have a mocked </em><em>TodoContext for xunit testing?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>To have a mocked </em><em>TodoContext for xunit testing, you can use the following steps:</em></blockquote><ul><li>Create a new class that inherits from the TodoContext class.</li><li>In the new class, override the OnConfiguring method and provide a fake connection string.</li><li>In the new class, override the OnModelCreating method and provide a fake model builder.</li><li>Create a new instance of the new class and use it in your xunit tests.</li><li>Here is an example of how you would create a mocked TodoContext for xunit testing:</li></ul><p>The answer is pretty good and actually works! You can now go back to the <a href="https://github.com/meteatamel/duetai-aspnet-webapi/blob/main/after/TodoApiTest/TodoItemsControllerTests.cs">TodoItemsControllerTests.cs</a> and change it to have the basic unit test:</p><p>We got our unit test project setup with our first unit test done! As you observed, we had to nudge Duet AI with the right context to get a good answer. It’s important to not expect perfection from Duet AI or GenAI in general.</p><h3>Add new functionality with Duet AI</h3><p>Now that we have basics in place, let’s use Duet AI to add a new endpoint to check in an item exists or not.</p><p><strong>Prompt</strong></p><blockquote><em>Can you modify </em><em>TodoItemsController.cs to add a method to check if a </em><em>ToDoItem exists? The method should handle the HTTP HEAD method and it should use the id of ToDoItem to check for existence</em></blockquote><p><strong>Answer</strong></p><pre>using Microsoft.AspNetCore.Mvc;<br>using Microsoft.EntityFrameworkCore;<br>using TodoApi.Models;<br><br>namespace TodoApi.Controllers<br>{<br>    [Route(&quot;api/[controller]&quot;)]<br>    [ApiController]<br>    public class TodoItemsController : ControllerBase<br>    {<br>...<br><br>        // HEAD: api/TodoItems/5<br>        [HttpHead(&quot;{id}&quot;)]<br>        public async Task&lt;IActionResult&gt; HeadTodoItem(long id)<br>        {<br>            var todoItem = await _context.TodoItems.FindAsync(id);<br>            if (todoItem == null)<br>            {<br>                return NotFound();<br>            }<br><br>            return NoContent();<br>        }<br>    }<br>}</pre><p>The answer is very good and compiles out of the box!</p><p>You can test by adding an item:</p><pre>curl -X POST \<br>    -H &quot;Content-Type: application/json&quot; \<br>    -d &#39;{<br>        &quot;name&quot;: &quot;Learn ASP.NET Core&quot;,<br>        &quot;isComplete&quot;: false<br>    }&#39; \<br>    http://localhost:5112/api/TodoItems<br><br>{&quot;id&quot;:1,&quot;name&quot;:&quot;Learn ASP.NET Core&quot;,&quot;isComplete&quot;:false}%</pre><p>Verify that the new endpoint works:</p><pre>curl -I -X HEAD http://localhost:5112/api/ToDoItems/1<br>HTTP/1.1 204 No Content<br>Date: Mon, 15 Jan 2024 13:42:24 GMT<br>Server: Kestrel</pre><p>Duet AI inline code generation seems to work pretty well, presumably, Duet AI has good context when it’s generating code.</p><h3>Add more unit tests to test new functionality with Duet AI</h3><p>Let’s see if Duet AI can generate us some unit tests for the new functionality.</p><p><strong>Prompt</strong></p><blockquote><em>Can you add a new unit test in </em><em>TodoItemsControllerTests.cs to test the </em><em>HeadTodoItem method in </em><em>TodoItemsController.cs?</em></blockquote><p><strong>Answer</strong></p><pre>using Microsoft.AspNetCore.Mvc;<br>using Microsoft.EntityFrameworkCore;<br>using System.Threading.Tasks;<br>using TodoApi.Controllers;<br>using TodoApi.Models;<br>using Xunit;<br><br>namespace TodoApi.Tests;<br><br>public class TodoItemsControllerTests<br>{<br>    [Fact]<br>    public async Task HeadTodoItem_ReturnsNoContent_WhenItemExists()<br>    {<br>        // Arrange<br>        var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()<br>            .UseInMemoryDatabase(databaseName: &quot;TodoItems&quot;)<br>            .Options;<br>        var context = new TodoContext(options);<br>        var controller = new TodoItemsController(context);<br>        var id = 1;<br><br>        // Act<br>        var result = await controller.HeadTodoItem(id);<br><br>        // Assert<br>        Assert.IsType&lt;NoContentResult&gt;(result);<br>    }<br><br>    [Fact]<br>    public async Task HeadTodoItem_ReturnsNotFound_WhenItemDoesNotExist()<br>    {<br>        // Arrange<br>        var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()<br>            .UseInMemoryDatabase(databaseName: &quot;TodoItems&quot;)<br>            .Options;<br>        var context = new TodoContext(options);<br>        var controller = new TodoItemsController(context);<br>        var id = 1;<br><br>        // Act<br>        var result = await controller.HeadTodoItem(id);<br><br>        // Assert<br>        Assert.IsType&lt;NotFoundResult&gt;(result);<br>    }<br>}</pre><p>The good news is that both tests compile fine! The first test actually fails but the second test works. The reason why the first test fails is that the in-memory database does not have any items really, so I asked Duet AI to refine the test.</p><p><strong>Prompt</strong></p><blockquote><em>HeadTodoItem_ReturnsNoContent_WhenItemExists method does not work because the item is not in the list. Can you change the code to make sure the item actually exists?</em></blockquote><p><strong>Answer</strong></p><blockquote><em>Sure. Here is the updated code for the HeadTodoItem_ReturnsNoContent_WhenItemExists method:</em></blockquote><pre><br>[Fact]<br>public async Task HeadTodoItem_ReturnsNoContent_WhenItemExists()<br>{<br>    // Arrange<br>    var options = new DbContextOptionsBuilder&lt;TodoContext&gt;()<br>        .UseInMemoryDatabase(databaseName: &quot;TodoItems&quot;)<br>        .Options;<br>    var context = new TodoContext(options);<br>    var controller = new TodoItemsController(context);<br>    var todoItem = new TodoItem { Id = 1, Name = &quot;Item 1&quot;, IsComplete = false };<br>    context.TodoItems.Add(todoItem);<br>    await context.SaveChangesAsync();<br>    var id = 1;<br><br>    // Act<br>    var result = await controller.HeadTodoItem(id);<br><br>    // Assert<br>    Assert.IsType&lt;NoContentResult&gt;(result);<br>}</pre><p>That’s perfect!</p><p>The only thing you need to change is to make sure the second test refers to a non-existent item. You can check the final tests <a href="https://github.com/meteatamel/duetai-aspnet-webapi/blob/main/after/TodoApiTest/TodoItemsControllerTests.cs">TodoItemsControllerTests.cs</a>:</p><p>Duet AI has been very useful in adding new functionality and adding unit tests for it!</p><h3>Lessons learned</h3><p>Let’s recap some lessons learned as I played with different prompts:</p><ul><li><strong>Direct Duet AI with your knowledge</strong>. You need to direct Duet AI to the right answer with your knowledge of the code. E.g. Don’t say “Test the highlighted code with curl” but instead say “Test the highlighted code takingToDoItem.cs into account&quot;.</li><li><strong>Be specific</strong>. The more specific you get, the better will be the answers. E.g. Don’t say “How can I test?” but rather say “How can I create axUnit project to unit testTodoItemsController.cs class from command line&quot;</li><li><strong>Don’t expect perfection</strong>. Duet AI will tell you to generate code in wrong files sometimes or generate unnecessary configuration. You need to know your way around in your project to know what’s needed where, rather than expecting perfect answers that you can copy/paste.</li><li>The generated tests are good starting points. They won’t be perfect but will save a lot of time. You then need to nudge Duet AI to generate the right tests for what you want to exactly test.</li><li>Inline code/test generation seems to pretty good in Duet AI, presumably, due to Duet AI having good context with existing code and files in your project.</li></ul><p>In this blog post, I took an existing web app, explored it, tested it, added unit tests and added new functionality with the help of Duet AI. With specific prompts, right context, and some nudging, it gives good answers and overall, it saved me a lot of time and effort.</p><p>As always, if you have any questions or feedback, feel free to reach out to me on Twitter @ <a href="https://twitter.com/meteatamel">meteatamel</a>.</p><p><em>Originally published at </em><a href="https://atamel.dev/posts/2024/01-29_duetai_test_change_existing_webapp/"><em>https://atamel.dev</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=863380c9a9d6" width="1" height="1" alt=""><hr><p><a href="https://medium.com/google-cloud/test-and-change-an-existing-web-app-with-duet-ai-863380c9a9d6">Test and change an existing web app with Duet AI</a> was originally published in <a href="https://medium.com/google-cloud">Google Cloud - Community</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>