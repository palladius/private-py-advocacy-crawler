<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Stories by Kelsey Hightower on Medium]]></title>
        <description><![CDATA[Stories by Kelsey Hightower on Medium]]></description>
        <link>https://medium.com/@kelseyhightower?source=rss-9e783a6f12f6------2</link>
        <image>
            <url>https://cdn-images-1.medium.com/fit/c/150/150/0*wIGdSIC6duW9jbWz.jpg</url>
            <title>Stories by Kelsey Hightower on Medium</title>
            <link>https://medium.com/@kelseyhightower?source=rss-9e783a6f12f6------2</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Thu, 04 Jul 2024 15:28:21 GMT</lastBuildDate>
        <atom:link href="https://medium.com/@kelseyhightower/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Tech Conferences]]></title>
            <link>https://medium.com/@kelseyhightower/tech-conferences-84d67cf994f6?source=rss-9e783a6f12f6------2</link>
            <guid isPermaLink="false">https://medium.com/p/84d67cf994f6</guid>
            <category><![CDATA[public-speaking]]></category>
            <category><![CDATA[conference]]></category>
            <dc:creator><![CDATA[Kelsey Hightower]]></dc:creator>
            <pubDate>Mon, 23 Jan 2017 05:29:59 GMT</pubDate>
            <atom:updated>2017-01-23T05:29:59.255Z</atom:updated>
            <content:encoded><![CDATA[<h3>The Attendee</h3><p>I’ve gone to my fair share of conferences as an attendee, and nothing is more exciting than spending time with people who share your passion for technology. These are our tribes, and when we’re brought together, it’s amazing.</p><p>But for me it did not start out that way. I can remember going to conferences alone, hoping to join the party, only to find that conferences can be a bit cliquish, composed of subgroups that roll in tight circles. My first taste of feeling like an outcast came during a conference lunch; I felt like the kid who’s just transferred to a new school halfway into the school year. I was without my support system, no one knew who I was, and all the cool kids seemed to be having tons of fun without me.</p><p>Over time I learned to just jump in, work the room, listen in on conversations I found interesting, and then break the ice with a question. Turns out, most conference goers love to talk, and if you ask a somewhat relevant question, people will talk to you for hours. I took the power of <a href="https://xkcd.com/356/">nerd sniping</a> and used it for good.</p><p>After a few events, I came to understand that most people are nice, welcoming, and willing to talk to you — but it was up to me to jump in. With a little experience under my belt, I now seek out people who look like my formerly shy self and make them feel welcome, mainly by asking questions and listening. I don’t wait for my chance to respond; I actually listen, and I learn from them. It’s amazing how much people at tech conferences have to share, even if they have less experience in the conference topic than you.</p><h3>The Speaker</h3><p>I started speaking at conferences about five years ago. I got my start speaking at a local Python meetup in Atlanta, where I gave a talk comparing the implementation of list comprehensions in Python and Haskell. I chose such a complex topic because I felt I had to look smart. This is something I would later learn is unnecessary; some of the best presentations I’ve seen come from beginners sharing something they’re excited about.</p><p>After a few more meetups, I built up the courage to submit my first talk proposal to PuppetConf 2011, “<a href="https://www.youtube.com/watch?v=WUlYEJ-fpfU">Streamlining Workflows With Puppet Faces</a>”. The talk was a success! I overcame my fear of public speaking, despite sweaty palms and butterflies, and got a few laughs along the way. Shortly after that talk, I landed a job at Puppet Labs. I still consider that talk one of the best interviews I’ve ever given.</p><p>After PuppetConf, I went on to speak at a few more meetups and small conferences. As a speaker, I felt I had finally made it. But in some cases, I felt I had landed talks as the token black guy. To be honest, I didn’t care — opportunity is opportunity — though I do worry about the speakers that were rejected in the name of inclusion. Nevertheless, I figured I owed it to the community, my community and to the rejected speakers, to not only accept the speaking slot, but to go out there and earn it.</p><p>I’ve largely overcome my fears around submitting a CFP (Call for Papers), but I’ll never forget what it’s like to feel unqualified, or believe that an event won’t accept an outsider from an underrepresented group. Even to this day, when I see a CFP pop up, the first thing I do is check out the speakers from previous years. If I don’t see someone who looks like me, I immediately assume I’ll be fighting an uphill battle, and start questioning what makes me so special that I’ll be the first black person given an opportunity to speak at their conference. Representation matters, and it’s the main reason I accept so many speaking opportunities: I want to be the reflection some people are looking for.</p><h3>The Conference Chair</h3><p>Two years ago I was approached by <a href="http://www.oreilly.com">O’Reilly</a>, and was offered an opportunity to become a co-chair for OSCON 2016. After disconnecting from the initial call with Gina Blaber, the vice president of conferences, I did a few victory laps around my apartment, thinking,“I have just received one of the highest honors a member of the tech community could ever receive!” I’d been entrusted to help shape what continues to be a legendary event, not just for open source, but for all the communities born from it.</p><p>Rachel Roumeliotis, who I consider to be the Queen of OSCON, showed me the ropes and taught me everything I know about being a conference chair. Not only did Rachel teach me how to sort and rank talk submissions, she taught me how to think about each talk in relation to the entire conference. She taught me how to be a professional conference contributor.</p><p>I’ve also had the pleasure of working with Scott Hanselman as my OSCON co-chair. In short, Scott is extra dope, and is the biggest advocate for inclusion I know. Scott doesn’t go around giving out speaking slots, but goes above and beyond to ensure no one gets overlooked. Scott taught me how to dig deeper and find those hidden gems that reviewers often miss. Not only did we end up giving new speakers a chance, we gave attendees something new, something refreshing, and new faces to attach it to.</p><p>I’m also fortunate to co-chair <a href="http://events.linuxfoundation.org/events/cloudnativecon-and-kubecon-europe">KubeCon</a> this year, and to get a chance to incorporate what I’ve learned from OSCON. So far, the most challenging aspect of being a conference chair is rejecting talk submissions. I know how much time it takes to put together a CFP, and how much courage it can take to actually submit it, so it pains me to have to reject even a single talk. As a conference chair, you get to analyze every CFP, and you quickly realize that many submissions overlap, so great proposals often have to compete with each other. This is actually a good thing, but then you need to determine how to break ties. Do you optimize for new speakers, inclusion, and balance, or do the big names always win?</p><p>Some may suggest a blind CFP review and selection process. I can’t see how that works, unless your CFP attracts a balanced set of submissions from all groups. One way to foster inclusion is to encourage underrepresented groups to submit a CFP, and more importantly, offer assistance for putting together the best proposal possible. It also helps to keep in mind that gender is not the only form of inclusion. Some claim success because the number of female speakers at their event reaches some percentage slightly greater than terrible; this is a bit short-sighted, and leaves out many other underrepresented groups. But beyond inclusion, remember that content is king. Most people go to conferences to learn something, so the talks must meet that expectation, even if your goals for inclusion fall short.</p><p>I’m still pretty new to this whole conference chair thing, and so far I have learned there’s no one-size-fits-all when it comes to conferences. Fairness requires hard work from the organizers, rooted in respect for both the speaker and attendee — and putting together a tech conference give us the opportunity to do just that.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=84d67cf994f6" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[12 Fractured Apps]]></title>
            <link>https://medium.com/@kelseyhightower/12-fractured-apps-1080c73d481c?source=rss-9e783a6f12f6------2</link>
            <guid isPermaLink="false">https://medium.com/p/1080c73d481c</guid>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[devops]]></category>
            <dc:creator><![CDATA[Kelsey Hightower]]></dc:creator>
            <pubDate>Sun, 13 Dec 2015 22:33:00 GMT</pubDate>
            <atom:updated>2015-12-14T03:04:52.589Z</atom:updated>
            <content:encoded><![CDATA[<p>Over the years I’ve witnessed more and more people discover the <a href="http://12factor.net/">12 Fact0r App</a> manifesto and start implementing many of the suggestions outlined there. This has led to applications that are far easier to deploy and manage. However practical examples of 12 Factor were a rare sight to see in the wild.</p><p>Once Docker hit the scene the benefits of the 12 Factor App (12FA) really started to shine. For example, 12FA recommends that logging should be done to stdout and be treated as an event stream. Ever run the <em>docker logs </em>command? That’s 12FA in action!</p><p>12FA also suggests applications should use environment variables for configuration. Again Docker makes this trivial by providing the ability to set env vars programmatically when creating containers.</p><blockquote>Docker and 12 factor apps are a killer combo and offer a peek into the future of application design and deployment.</blockquote><p>Docker also makes it somewhat easy to “lift and shift” legacy applications to containers. I say “somewhat” because what most people end up doing is treating Docker containers like VMs, resulting in 2GB container images built on top of full blown Linux distros.</p><p>Unfortunately legacy applications, including the soon-to-be-legacy application you are working on right now, have many shortcomings, especially around the startup process. Applications, even modern ones, make too many assumptions and do very little to ensure a clean startup. Applications that require an external database will normally initialize the database connection during startup. However, if that database is unreachable, even temporarily, many applications will simply exit. If you’re lucky you might get an error message and non-zero exit code to aid in troubleshooting.</p><blockquote>Many of the applications that are being packaged for Docker are broken in subtle ways. So subtle people would not call them broken, it’s more like a hairline fracture — it works but hurts like hell when you use them.</blockquote><p>This kind of application behavior has forced many organizations into complex deployment processes and contributed to the rise of configuration management tools like Puppet or Ansible. Configuration management tools can solve the “missing” database problem by ensuring the database is started before the applications that depend on it. This is nothing more then a band-aid covering up the larger problem. The application should simply retry the database connection, using some sort of backoff, and log errors along the way. At some point either the database will come online, or your company will be out of business.</p><p>Another challenge for applications moving to Docker is around configuration. Many applications, even modern ones, still rely on local, on-disk, configuration files. It’s often suggested to simply build new “deployment” containers that bundle the configuration files in the container image.</p><p><strong>Don’t do this.</strong></p><p>If you go down this road you will end up with an endless number of container images named something like this:</p><ul><li>application-v2–prod-01022015</li><li>application-v2-dev-02272015</li></ul><p>You’ll soon be in the market for a container image management tool.</p><p>The move to Docker has given people the false notion they no longer need any form of configuration management. I tend to agree, there is no need to use Puppet, Chef, or Ansible to build container images, but there is still a need to manage runtime configuration settings.</p><p>The same logic used to do away with configuration management is often used to avoid all init systems in favor of the <em>docker run</em> command.</p><p>To compensate for the lack of configuration management tools and robust init systems, Docker users have turned to shell scripts to mask application shortcomings around initial bootstrapping and the startup process.</p><blockquote>Once you go all in on Docker and refuse to use tools that don’t bear the Docker logo you paint yourself into a corner and start abusing Docker.</blockquote><h3>Example Application</h3><p>The remainder of this post will utilize an example program to demonstrate a few common startup tasks preformed by a typical application. The example application performs the following tasks during startup:</p><ul><li>Load configuration settings from a JSON encoded config file</li><li>Access a working data directory</li><li>Establish a connection to an external mysql database</li></ul><pre>package main<br><br>import (<br>    &quot;database/sql&quot;<br>    &quot;encoding/json&quot;<br>    &quot;fmt&quot;<br>    &quot;io/ioutil&quot;<br>    &quot;log&quot;<br>    &quot;net&quot;<br>    &quot;os&quot;<br><br>    _ &quot;github.com/go-sql-driver/mysql&quot;<br>)<br><br>var (<br>    config Config<br>    db     *sql.DB<br>)<br><br>type Config struct {<br>    DataDir string `json:&quot;datadir&quot;`<br><br>    // Database settings.<br>    Host     string `json:&quot;host&quot;`<br>    Port     string `json:&quot;port&quot;`<br>    Username string `json:&quot;username&quot;`<br>    Password string `json:&quot;password&quot;`<br>    Database string `json:&quot;database&quot;`<br>}<br><br>func main() {<br>    log.Println(&quot;Starting application...&quot;)</pre><pre>    // Load configuration settings.<br>    data, err := ioutil.ReadFile(&quot;/etc/config.json&quot;)<br>    if err != nil {<br>        log.Fatal(err)<br>    }<br>    if err := json.Unmarshal(data, &amp;config); err != nil {<br>        log.Fatal(err)<br>    }<br><br>    // Use working directory.<br>    _, err = os.Stat(config.DataDir)<br>    if err != nil {<br>        log.Fatal(err)<br>    }</pre><pre>    // Connect to database.<br>    hostPort := net.JoinHostPort(config.Host, config.Port)<br>    dsn := fmt.Sprintf(&quot;%s:%s@tcp(%s)/%s?timeout=30s&quot;,<br>        config.Username, config.Password, hostPort, config.Database)<br><br>    db, err = sql.Open(&quot;mysql&quot;, dsn)<br>    if err != nil {<br>        log.Fatal(err)<br>    }<br><br>    if err := db.Ping(); err != nil {<br>        log.Fatal(err)<br>    }<br>}</pre><blockquote>The complete source code of the example program is available on <a href="https://github.com/kelseyhightower/12-fractured-apps">GitHub</a>.</blockquote><p>As you can see there’s nothing special here, but if you look closely you can see this application will only startup under specific conditions, which we’ll call the happy path. If the configuration file or working directory is missing, or the database is not available during startup, the above application will fail to start. Let’s deploy the example application via Docker and examine this first hand.</p><p>Build the application using the <em>go build</em> command:</p><pre>$ GOOS=linux go build -o app .</pre><p>Create a Docker image using the following Dockerfile:</p><pre>FROM scratch<br>MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;<br>COPY app /app<br>ENTRYPOINT [&quot;/app&quot;]</pre><p>All I’m doing here is copying the application binary into place. This container image will use the scratch base image, resulting in a minimal Docker image suitable for deploying our application. Remember, ship artifacts not build environments.</p><p>Create the Docker image using the <em>docker build</em> command:</p><pre>$ docker build -t app:v1 .</pre><p>Finally, create a Docker container from the <em>app:v1 </em>Docker image using the <em>docker run</em> command:</p><pre>$ docker run --rm app:v1<br>2015/12/13 04:00:34 Starting application...<br>2015/12/13 04:00:34 open /etc/config.json: no such file or directory</pre><p>Let the pain begin! Right out of the gate I hit the first startup problem. Notice the application fails to start because of the missing <em>/etc/config.json</em> configuration file. I can fix this by bind mounting the configuration file at runtime:</p><pre>$ docker run --rm \<br>  -v /etc/config.json:/etc/config.json \<br>  app:v1</pre><pre>2015/12/13 07:36:27 Starting application...<br>2015/12/13 07:36:27 stat /var/lib/data: no such file or directory</pre><p>Another error! This time the application fails to start because the <em>/var/lib/data</em> directory does not exist. I can easily work around the missing data directory by bind mounting another host dir into the container:</p><pre>$ docker run --rm \<br>  -v /etc/config.json:/etc/config.json \<br>  -v /var/lib/data:/var/lib/data \<br>  app:v1</pre><pre>2015/12/13 07:44:18 Starting application...<br>2015/12/13 07:44:48 dial tcp 203.0.113.10:3306: i/o timeout</pre><p>Now we are making progress, but I forgot to configure access to the database for this Docker instance.</p><p>This is the point where some people start suggesting that configuration management tools should be used to ensure that all these dependencies are in place before starting the application. While that works, it’s pretty much overkill and often the wrong approach for application-level concerns.</p><blockquote>I can hear the silent cheers from hipster “sysadmins” sipping on a cup of Docker Kool-Aid eagerly waiting to suggest using a custom Docker entrypoint to solve our bootstrapping problems.</blockquote><h3>Custom Docker entrypoints to the rescue</h3><p>One way to address our startup problems is to create a shell script and use it as the Docker entrypoint in place of the actual application. Here’s a short list of things we can accomplish using a shell script as the Docker entrypoint:</p><ul><li>Generate the required /etc/config.json configuration file</li><li>Create the required /var/lib/data directory</li><li>Test the database connection and block until it’s available</li></ul><p>The following shell script tackles the first two items by adding the ability to use environment variables in-place of the <em>/etc/config.json</em> configuration file and creating the missing <em>/var/lib/data</em> directory during the startup process. The script executes the example application as the final step, preserving the original behavior of starting the application by default.</p><pre>#!/bin/sh<br>set -e</pre><pre>datadir=${APP_DATADIR:=&quot;/var/lib/data&quot;}<br>host=${APP_HOST:=&quot;127.0.0.1&quot;}<br>port=${APP_PORT:=&quot;3306&quot;}<br>username=${APP_USERNAME:=&quot;&quot;}<br>password=${APP_PASSWORD:=&quot;&quot;}<br>database=${APP_DATABASE:=&quot;&quot;}</pre><pre>cat &lt;&lt;EOF &gt; /etc/config.json<br>{<br>  &quot;datadir&quot;: &quot;${datadir}&quot;,<br>  &quot;host&quot;: &quot;${host}&quot;,<br>  &quot;port&quot;: &quot;${port}&quot;,<br>  &quot;username&quot;: &quot;${username}&quot;,<br>  &quot;password&quot;: &quot;${password}&quot;,<br>  &quot;database&quot;: &quot;${database}&quot;<br>}<br>EOF</pre><pre>mkdir -p ${APP_DATADIR}</pre><pre>exec &quot;/app&quot;</pre><p>The Docker image can now be rebuilt using the following Dockerfile:</p><pre>FROM alpine:3.1<br>MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;<br>COPY app /app<br>COPY docker-entrypoint.sh /entrypoint.sh<br>ENTRYPOINT [&quot;/entrypoint.sh&quot;]</pre><blockquote>Notice the custom shell script is copied into the Docker image and used as the entrypoint in place of the application binary.</blockquote><p>Build the <em>app:v2</em> Docker image using the <em>docker build</em> command:</p><pre>$ docker build -t app:v2 .</pre><p>Now run it:</p><pre>$ docker run --rm \<br>  -e &quot;APP_DATADIR=/var/lib/data&quot; \<br>  -e &quot;APP_HOST=203.0.113.10&quot; \<br>  -e &quot;APP_PORT=3306&quot; \<br>  -e &quot;APP_USERNAME=user&quot; \<br>  -e &quot;APP_PASSWORD=password&quot; \<br>  -e &quot;APP_DATABASE=test&quot; \<br>  app:v2</pre><pre>2015/12/13 04:44:29 Starting application...</pre><p>The custom entrypoint is working. Using only environment variables we are now able to configure and run our application.</p><p>But why are we doing this?</p><p>Why do we need to use such a complex wrapper script? Some will say it’s much easier to write this functionality in shell then doing it in the app. But the cost is not only in managing shell scripts. Notice the other difference between the v1 and v2 Dockerfiles?</p><pre>FROM alpine:3.1</pre><p>The v2 Dockerfile uses the alpine base image to provide a scripting environment, while small, it does double the size of our Docker image:</p><pre>$ docker images<br>REPOSITORY  TAG  IMAGE ID      CREATED      VIRTUAL SIZE<br>app         v2   1b47f1fbc7dd  2 hours ago  10.99 MB<br>app         v1   42273e8664d5  2 hours ago  5.952 MB</pre><p>The other drawback to this approach is the inability to use a configuration file with the image. We can continue scripting and add support for both the configuration file and env vars, but this is just going down the wrong path, and it will come back to bite us at some point when the wrapper script gets out of sync with the application.</p><p>There is another way to fix this problem.</p><h3>Programming to the rescue</h3><p>Yep, good old fashion programming. Each of the issues being addressed in the <em>docker-entrypoint.sh</em> script can be handled directly by the application.</p><p>Don’t get me wrong, using an entrypoint script is ok for applications you don’t have control over, but when you rely on custom entrypoint scripts for applications you write, you add another layer of complexity to the deployment process for no good reason.</p><h4>Config files should be optional</h4><p>There is absolutely no reason to require a configuration file after the 90s. I would suggest loading the configuration file if it exists, and falling back to sane defaults. The following code snippet does just that.</p><pre>// Load configuration settings.<br>data, err := ioutil.ReadFile(&quot;/etc/config.json&quot;)</pre><pre>// Fallback to default values.<br>switch {<br>    case os.IsNotExist(err):<br>        log.Println(&quot;Config file missing using defaults&quot;)<br>        config = Config{<br>            DataDir: &quot;/var/lib/data&quot;,<br>            Host: &quot;127.0.0.1&quot;,<br>            Port: &quot;3306&quot;,<br>            Database: &quot;test&quot;,<br>        }<br>    case err == nil:<br>        if err := json.Unmarshal(data, &amp;config); err != nil {<br>            log.Fatal(err)<br>        }<br>    default:<br>        log.Println(err)<br>}</pre><h4>Using env vars for config</h4><p>This is one of the easiest things you can do directly in your application. In the following code snippet env vars are used to override configuration settings.</p><pre>log.Println(&quot;Overriding configuration from env vars.&quot;)</pre><pre>if os.Getenv(&quot;APP_DATADIR&quot;) != &quot;&quot; {<br>    config.DataDir = os.Getenv(&quot;APP_DATADIR&quot;)<br>}<br>if os.Getenv(&quot;APP_HOST&quot;) != &quot;&quot; {<br>    config.Host = os.Getenv(&quot;APP_HOST&quot;)<br>}<br>if os.Getenv(&quot;APP_PORT&quot;) != &quot;&quot; {<br>    config.Port = os.Getenv(&quot;APP_PORT&quot;)<br>}<br>if os.Getenv(&quot;APP_USERNAME&quot;) != &quot;&quot; {<br>    config.Username = os.Getenv(&quot;APP_USERNAME&quot;)<br>}<br>if os.Getenv(&quot;APP_PASSWORD&quot;) != &quot;&quot; {<br>    config.Password = os.Getenv(&quot;APP_PASSWORD&quot;)<br>}<br>if os.Getenv(&quot;APP_DATABASE&quot;) != &quot;&quot; {<br>    config.Database = os.Getenv(&quot;APP_DATABASE&quot;)<br>}</pre><h4>Manage the application working directories</h4><p>Instead of punting the responsibility of creating working directories to external tools or custom entrypoint scripts your application should manage them directly. If they are missing create them. If that fails be sure to log an error with the details:</p><pre>// Use working directory.<br>_, err = os.Stat(config.DataDir)<br>if os.IsNotExist(err) {<br>    log.Println(&quot;Creating missing data directory&quot;, config.DataDir)<br>    err = os.MkdirAll(config.DataDir, 0755)<br>}<br>if err != nil {<br>    log.Fatal(err)<br>}</pre><h4>Eliminate the need to deploy services in a specific order</h4><p>Do not require anyone to start your application in a specific order. I’ve seen too many deployment guides warn users to deploy an application after the database because the application would fail to start.</p><p>Stop doing this. Here’s how:</p><pre>$ docker run --rm \<br>  -e &quot;APP_DATADIR=/var/lib/data&quot; \<br>  -e &quot;APP_HOST=203.0.113.10&quot; \<br>  -e &quot;APP_PORT=3306&quot; \<br>  -e &quot;APP_USERNAME=user&quot; \<br>  -e &quot;APP_PASSWORD=password&quot; \<br>  -e &quot;APP_DATABASE=test&quot; \<br>  app:v3</pre><pre>2015/12/13 05:36:10 Starting application...<br>2015/12/13 05:36:10 Config file missing using defaults<br>2015/12/13 05:36:10 Overriding configuration from env vars.<br>2015/12/13 05:36:10 Creating missing data directory /var/lib/data<br>2015/12/13 05:36:10 Connecting to database at 203.0.113.10:3306<br>2015/12/13 05:36:40 dial tcp 203.0.113.10:3306: i/o timeout<br>2015/12/13 05:37:11 dial tcp 203.0.113.10:3306: i/o timeout</pre><blockquote>Notice in the above output that I’m not able to connect to the target database running at 203.0.113.10.</blockquote><p>After running the following command to grant access to the “mysql” database:</p><pre>$ gcloud sql instances patch mysql \<br>  --authorized-networks &quot;203.0.113.20/32&quot;</pre><p>The application is able to connect to the database and complete the startup process.</p><pre>2015/12/13 05:37:43 dial tcp 203.0.113.10:3306: i/o timeout<br>2015/12/13 05:37:46 Application started successfully.</pre><p>The code to make this happen looks like this:</p><pre>// Connect to database.<br>hostPort := net.JoinHostPort(config.Host, config.Port)</pre><pre>log.Println(&quot;Connecting to database at&quot;, hostPort)</pre><pre>dsn := fmt.Sprintf(&quot;%s:%s@tcp(%s)/%s?timeout=30s&quot;,<br>    config.Username, config.Password, hostPort, config.Database)<br>db, err = sql.Open(&quot;mysql&quot;, dsn)<br>if err != nil {<br>    log.Println(err)<br>}</pre><pre>var dbError error<br>maxAttempts := 20<br>for attempts := 1; attempts &lt;= maxAttempts; attempts++ {<br>    dbError = db.Ping()<br>    if dbError == nil {<br>        break<br>    }<br>    log.Println(dbError)<br>    time.Sleep(time.Duration(attempts) * time.Second)<br>}</pre><pre>if dbError != nil {<br>    log.Fatal(dbError)<br>}</pre><p>Nothing fancy here. I’m simply retrying the database connection and increasing the time between each attempt.</p><p>Finally, we wrap up the startup process with a friendly log message that the application has started correctly. Trust me, your sysadmin will thank you.</p><pre>log.Println(&quot;Application started successfully.&quot;)</pre><h3>Summary</h3><p>Everything in this post is about improving the deployment process for your applications, specifically those running in a Docker container, but these ideas should apply almost anywhere. On the surface it may seem like a good idea to push application bootstrapping tasks to custom wrapper scripts, but I urge you to reconsider. Deal with application bootstrapping tasks as close to the application as possible and avoid pushing this burden onto your users, which in the future could very well be you.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1080c73d481c" width="1" height="1" alt="">]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building Docker Images for Static Go Binaries]]></title>
            <link>https://medium.com/@kelseyhightower/optimizing-docker-images-for-static-binaries-b5696e26eb07?source=rss-9e783a6f12f6------2</link>
            <guid isPermaLink="false">https://medium.com/p/b5696e26eb07</guid>
            <dc:creator><![CDATA[Kelsey Hightower]]></dc:creator>
            <pubDate>Thu, 14 Aug 2014 14:52:28 GMT</pubDate>
            <atom:updated>2014-08-16T14:38:40.875Z</atom:updated>
            <content:encoded><![CDATA[<p>Building applications in Go enables the ability to easily produce statically linked binaries free of external dependencies. Statically linked binaries are much larger than their dynamic counterparts, but often weigh in at less than 10 MB for most real-world applications. The reason for such large binary sizes are because everything, including the Go runtime, is included in the binary. The large binary size is a tradeoff I’m willing to make since I gain the ability to deploy applications by copying a single binary into place and executing it.</p><p>So I can’t help but ask, If the process of building and deploying static binaries is so easy, why would I want to bring Docker into the mix? Well, Docker does offer the convenience of a standardized packaging format that makes it easy to share, discover, and install applications. I see Docker images being similar to rpms in concept, with Docker images having the advantage of packaging up my entire application in a single artifact. Hmm… this sounds familiar.</p><p>Deploying applications with Docker brings the benefits of Linux containers. I essentially gain the ability to leverage Linux namespaces and cgroups for free.</p><p>After being sold on the benefits of using Docker for Go apps, I started building containers for them. I was a bit surprised by the initial results. To help illustrate my journey I’ll use an example application called <a href="https://github.com/kelseyhightower/contributors">Contributors</a>. The Contributors app is a simple web frontend that lists the contributors, along with their avatar photos, for a given GitHub repository.</p><p>I, like many people, followed examples for building Docker images using a Dockerfile that looks something like this:</p><pre>FROM google/debian:wheezy<br>MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;<br>RUN apt-get update -y &amp;&amp; apt-get install —no-install-recommends -y -q curl build-essential ca-certificates git mercurial </pre><pre># Install Go<br># Save the SHA1 checksum from http://golang.org/dl<br>RUN echo &#39;9f9dfcbcb4fa126b2b66c0830dc733215f2f056e go1.3.src.tar.gz&#39; &gt; go1.3.src.tar.gz.sha1<br>RUN curl -O -s https://storage.googleapis.com/golang/go1.3.src.tar.gz<br>RUN sha1sum —check go1.3.src.tar.gz.sha1<br>RUN tar -xzf go1.3.src.tar.gz -C /usr/local<br>ENV PATH /usr/local/go/bin:$PATH<br>ENV GOPATH /gopath<br>RUN cd /usr/local/go/src &amp;&amp; ./make.bash —no-clean 2&gt;&amp;1</pre><pre>WORKDIR /gopath/src/github.com/kelseyhightower/contributors</pre><pre># Build the Contributors application<br>RUN mkdir -p /gopath/src/github.com/kelseyhightower/contributors<br>ADD . /gopath/src/github.com/kelseyhightower/contributors<br>RUN CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags &#39;-w&#39; .<br>RUN cp contributors /contributors</pre><pre>ENV PORT 80<br>EXPOSE 80</pre><pre>ENTRYPOINT [&quot;/contributors&quot;]</pre><p>The above Dockerfile combines the Docker image creation process with the application build process. While this workflow has its advantages I consider it unnecessary when working with statically linked binaries. I personally build my applications in CI and save the resulting binaries as artifacts of the build, which allows me to package my application in any format including a Docker image, rpm, or tarball.</p><p>Another drawback to building the application during the image creation process is the size of the resulting Docker image. The above Dockerfile produces a Docker image that checks in around 500 MB in size. Earlier I mentioned that sized does not matter, but at almost 500 MB we better start paying attention.</p><p>Why is the Docker image so big?</p><p>Building the Contributor app as part of the image creation process means we must set up a working build environment including all the tools required to download and build the Go runtime. This requirement locks us into choosing a base image capable of installing all those extra bits, and that base image usually starts around the 100 MB range. Then every additional RUN command in the Dockerfile increases the overall file size of the image. Keep in mind I’m doing all of this so I can produce the same binary built by my CI system.</p><p>While I could add logic to the Dockerfile to clean up unneeded files I decided to keep the application build process in CI, and treat the Docker image as another target for packaging. At this point I was able to shrink my Dockerfile to the following:</p><pre>FROM google/debian:wheezy<br>MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;<br>ADD contributors contributors<br>ENV PORT 80<br>EXPOSE 80<br>ENTRYPOINT [&quot;/contributors&quot;]</pre><p>Notice the addition of the ADD command which adds the contributors binary to the image. The build process now goes like this:</p><pre>CGO_ENABLED=0 GOOS=linux go build -a -tags netgo -ldflags &#39;-w&#39; .<br>docker build -t kelseyhightower/contributors .</pre><p>I’m now working with a much smaller Docker image, but at 120 MB the image is still too big. Especially since our binary doesn’t require any files in the image. The easy solution is to use a smaller base image such as busybox, considered tiny at 5 MB, or the absolute smallest image available, the <a href="https://docs.docker.com/articles/baseimages/#creating-a-simple-base-image-using-scratch">scratch image</a>:</p><pre>FROM scratch<br>MAINTAINER Kelsey Hightower &lt;kelsey.hightower@gmail.com&gt;<br>ADD contributors contributors<br>ENV PORT 80<br>EXPOSE 80<br>ENTRYPOINT [&quot;/contributors&quot;]</pre><p>Normally the scratch image will not work for typical Go applications because they are often built with cgo enabled, which results in a binary that dynamically links to a few dependencies. Also, some Go applications make external calls to SSL endpoints, which will fail with the following error when running from the scratch image:</p><pre>x509: failed to load system roots and no roots provided</pre><p>The reason for this is that on Linux systems the tls package reads the root CA certificates from /etc/ssl/certs/ca-certificates.crt, which is missing from the scratch image. The Contributors app gets around this problem by bundling a copy of the root CA certificates and configuring outbound calls to use them.</p><p>Bundling of the actual root CA certificates is pretty straightforward. The Contributor app takes a cert bundle, /etc/ssl/certs/ca-certificates.crt, from <a href="https://coreos.com/">CoreOS Linux</a> and assigns the text to a global variable named pemCerts. Then in the main init() the Contributor app initializes a cert pool and configures a HTTP client to use it.</p><pre>func init() {<br>    pool = x509.NewCertPool()<br>    pool.AppendCertsFromPEM(pemCerts)<br>    client = &amp;http.Client{<br>        Transport: &amp;http.Transport{<br>            TLSClientConfig: &amp;tls.Config{RootCAs: pool},<br>        },<br>    }<br>}</pre><p>From this point on all calls using the new HTTP client will work with SSL end-points. Checkout the <a href="https://github.com/kelseyhightower/contributors">source code</a> for more details on how the the root CA certificates are wired up.</p><p>Using the updated Dockerfile and re-running the build process I end up with a Docker image that is slightly larger than the Contributor app binary. We are now down to a 6MB Docker image.</p><p>At this point I have a Docker image optimized for size that is ready to run and share with others.</p><pre>docker run -d -P kelseyhightower/contributors</pre><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b5696e26eb07" width="1" height="1" alt="">]]></content:encoded>
        </item>
    </channel>
</rss>