
== Article 1
* Title: 'Colab Data Visualizations Made Easy'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/colab-data-visualizations-made-easy-5e1918e5234e?source=rss-14a828f24ca2------2'
* PublicationDate: 'Tue, 18 Jul 2023 16:40:52 GMT'
* Categories: matplotlib, data-visualization, colab

https://medium.com/media/9ebff7a2cb35faac7a83cd839708b013/hrefIf you’ve never found visualizing your data challenging, you can stop reading now. For everyone else, the Google Colab team is excited to announce automated generation of plots from Pandas DataFrames! One click gets you a palette of suggested plots for your DataFrame, and clicking on any plot inserts the corresponding code into your notebook.To get the auto-plotting button to display, run a code cell which finishes with a line that lists an existing DataFrame, which will then display this icon on the bottom right of your cell output:If you find a generated plot useful, the auto-plotting feature can provide the the supporting code and even insert it directly into a new cell in your notebook, allowing you to save and configure the plot to your liking.Here’s a sample notebook you can use to try the feature for yourself. Let us know how you’re using this feature in the comments below and keep on visualizing!Colab Data Visualizations Made Easy was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 2
* Title: 'Two New Ways to Manage Cell Execution'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/two-new-ways-to-manage-cell-execution-fbad61b40882?source=rss-14a828f24ca2------2'
* PublicationDate: 'Thu, 06 Jul 2023 14:24:34 GMT'
* Categories: 

We’ve recently introduced two new features to make it easier to control how and when to run your code cells:The first feature gives you the ability to run a collection of cells directly from the notebook table of contents, via a “Run cells in section” option in the context menu (under the triple dots), as shown in the image above.The second feature gives you the ability to assign certain cells as pre-requisites for a given notebook. This is manifested via an option in the notebook settings dialog (under the Edit menu) to “Automatically run the first cell or section” when opening a notebook (see image below). If your notebook starts with a single cell, that cell will be auto-executed whenever your notebook is opened. If the notebook starts with a section, the entire first section of cells will be auto-executed on notebook open.Hope you find these useful — let us know in the comments how you’re using them!Two New Ways to Manage Cell Execution was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'Noteworthy Notebooks #3 — Analyzing a Bank Failure with Colab'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/noteworthy-notebooks-3-analyzing-a-bank-failure-with-colab-d23b372de313?source=rss-14a828f24ca2------2'
* PublicationDate: 'Mon, 08 May 2023 18:11:25 GMT'
* Categories: economics, colab

Noteworthy Notebooks #3 — Analyzing a Bank Failure with ColabPhoto by Dmitry Demidko on UnsplashA well crafted notebook is a kind of living textbook and this is a great example. Professor Ashwin Rao of Stanford University has authored a fascinating Colab notebook explaining, at a high level, why Silicon Valley Bank (SVB) failed.Due to the nature of it’s clientele (largely tech startups), SVB enjoyed a strong cash position and invested much of that portfolio in short and long term bonds. Unfortunately, with post-pandemic rising interest rates, bond prices plunged, drastically reducing the value of those investments while, at the same time, increasing interest rates the bank owed to consumers and institutions above the level they earned on their own investments.There were other factors involved — Dr. Rao’s analysis is limited to the economic and pricing dynamics underlying this story. This notebook will teach you how bonds work and why, in the investment world, there’s no such thing as a sure thing.Check out the notebook here.Noteworthy Notebooks #3 — Analyzing a Bank Failure with Colab was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 4
* Title: '10 Minutes to Pandas — now in Colab!'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/10-minutes-to-pandas-now-in-colab-a7a629a630dc?source=rss-14a828f24ca2------2'
* PublicationDate: 'Tue, 02 May 2023 20:04:53 GMT'
* Categories: pandas, colab, python

10 Minutes to Pandas — in Colab!Photo by Yosuke Ota on UnsplashPandas is a popular open source Python package for data science, data engineering, analytics, and machine learning. It’s built on top of NumPy, which provides efficient support for numerical computation on multi-dimensional arrays.The Pandas project offers a helpful introductory tutorial called 10 Minutes to Pandas but it’s a read-only document. I like to learn by doing so I’ve taken the liberty of porting the ten minute Pandas tutorial to Colab so now you can enjoy an interactive version of this popular material.Here’s the notebook.10 Minutes to Pandas — now in Colab! was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 5
* Title: 'Colab + BigQuery — Perfect Together'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/colab-bigquery-perfect-together-827152026eb?source=rss-14a828f24ca2------2'
* PublicationDate: 'Thu, 12 Jan 2023 08:47:16 GMT'
* Categories: colab, analytics, google-cloud-platform, bigquery, cloud-computing

Colab + BigQuery — Perfect TogetherPhoto by benjamin lehman on UnsplashHave you heard about Google BigQuery? It’s hard to summarize all of the advantages in one sentence but I’m going to try anyway: BigQuery is a fully managed, cloud-native data warehouse that enables incredibly fast SQL queries using the processing power of Google’s infrastructure.If you haven’t had an opportunity to try BigQuery, today’s notebook, courtesy of Google Developer Advocate Alok Pattani, is a treat. In Alok’s notebook, you’ll learn how to use BigQuery to perform some basic data science tasks, including:setting up Colab and Google BigQuery within Colabreading data from BigQuery into Colabusing Python data science tools to do some analysis/curve fittingcreating some interactive outputsusing Python functionality “on top” of BigQuery to scale analysiswriting some analysis results back into BigQueryIn addition to mind-bending performance, BigQuery is scalable, serverless, cost effective, secure, and plays well with other Google Cloud services.Run the notebook and see how easy it is to use a world class data warehouse from the free, zero-friction Colab environment.Colab + BigQuery — Perfect Together was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 6
* Title: 'Make your dataframes come alive!'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/make-your-dataframes-come-alive-924c12e2e1d6?source=rss-14a828f24ca2------2'
* PublicationDate: 'Thu, 08 Dec 2022 18:22:54 GMT'
* Categories: data-visualization, exploratory-data-analysis, dataframes, colab, pandas

interactive table in actionPandas is a wonderful tool for analyzing data in Python but wouldn’t it be nice if your dataframes could be a bit more interactive? Imagine being able to easily sort and search your data without having to write additional code. Well, in Colab that feature is just a mouse click away.Let’s load a dataframe with some heart disease prediction data and display the first few rows:import pandas as pddata = pd.read_csv("https://raw.githubusercontent.com/kb22/Heart-Disease-Prediction/master/dataset.csv")data.head()As expected, Pandas renders the table nicely, but it also includes a little magic wand, as highlighted here:Clicking the wand re-renders the dataframe as an interactive table, like this:Now you can dynamically explore the data, for example, you can:reorder the rows by clicking on a column headerfilter the data based on arbitrarily ranges on one or more columnscopy your data to the clipboard in csv, markdown, or json formatsYou can also skip the mouse click and automatically render your dataframes in this style by default, by running this code near the top of your notebook:from google.colab import data_tabledata_table.enable_dataframe_formatter()This feature can make your exploratory data analysis a bit more efficient and fun — give it a try!Learn more about interactive tables in this notebook or see them live in this excellent video by my colleague Nate.Make your dataframes come alive! was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 7
* Title: 'Noteworthy Notebooks #2 — Winning Wordle'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/noteworthy-notebooks-2-winning-wordle-40793f94a054?source=rss-14a828f24ca2------2'
* PublicationDate: 'Fri, 02 Dec 2022 17:13:02 GMT'
* Categories: wordle, colab, puzzles-and-games, fun, python

Noteworthy Notebooks #2 — Winning WordlePhoto by Nils Huenerfuerst on UnsplashToday’s featured notebook answers three interesting questions about the viral word game sensation Wordle:If I win in two guesses, am I good or lucky?What is a guaranteed-winning strategy that I can easily memorize?What is a strategy that minimizes the number of guesses?This one comes from a delightful collection of Python notebooks by Peter Norvig. Check out Peter’s pytudes site for more interesting and fun puzzle solving and learning exercises, all of which can be run on Colab.Even if you’ve never written a line of Python, experimenting with this notebook will give you a feeling for how you can solve tricky problems using software.Here’s the notebook:Noteworthy Notebooks #2 — Winning Wordle was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Using BERT Models in TensorFlow'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/using-bert-models-in-tensorflow-213c25eaacc4?source=rss-14a828f24ca2------2'
* PublicationDate: 'Tue, 22 Nov 2022 21:51:14 GMT'
* Categories: machine-learning, tensorflow, bert, colab, nlp

Tensorflow Hub makes it easier than ever to use BERT models with preprocessing. Try it in Colab!BERT and other Transformer encoder architectures have been very successful in natural language processing (NLP) for computing vector space representations of text, both in advancing the state of the art in academic benchmarks as well as in large-scale applications like Google Search. BERT has been available for TensorFlow since it was created, but originally relied on non-TensorFlow Python code to transform raw text into model inputs.Nowadays, we can use BERT entirely within TensorFlow, thanks to pre-trained encoders and matching text preprocessing models available on TensorFlow Hub. BERT in TensorFlow can now run on text inputs with just a few lines of code:# Load BERT and the preprocessing model from TF Hub.preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')encoder = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3')# Use BERT on a batch of raw text inputs.input = preprocess(['Batch of inputs', 'TF Hub makes BERT easy!', 'More text.'])pooled_output = encoder(input)["pooled_output"]print(pooled_output)tf.Tensor([[-0.8384154  -0.26902363 -0.3839138  ... -0.3949695  -0.58442086  0.8058556 ] [-0.8223734  -0.2883956  -0.09359277 ... -0.13833837 -0.6251748   0.88950026] [-0.9045408  -0.37877116 -0.7714909  ... -0.5112085  -0.70791864  0.92950743]],shape=(3, 768), dtype=float32)These encoder and preprocessing models have been built with TensorFlow Model Garden’s NLP library and exported to TensorFlow Hub in the SavedModel format. Under the hood, preprocessing uses TensorFlow ops from the TF.text library to do the tokenization of input text — allowing you to build your own TensorFlow model that goes from raw text inputs to prediction outputs without Python in the loop. This accelerates the computation, removes boilerplate code, is less error prone, and enables the serialization of the full text-to-outputs model, making BERT easier to serve in production.To show in more detail how these models can help you, check out these tutorials:The beginner tutorial solves a sentiment analysis task and doesn’t need any special customization to achieve great model quality. It’s the easiest way to use BERT and a preprocessing model.The advanced tutorial solves NLP classification tasks from the GLUE benchmark, running on a TPU. It also shows how to use the preprocessing model in situations where you need multi-segment input.Choosing a BERT modelBERT models are pre-trained on a large corpus of text (for example, an archive of Wikipedia articles) using self-supervised tasks like predicting words in a sentence from the surrounding context. This type of training allows the model to learn a powerful representation of the semantics of the text without needing labeled data. However, it also takes significant computing resources to train — 4 days on 16 TPUs (as reported in the 2018 BERT paper). Fortunately, after completing this expensive pre-training, we can reuse this rich representation for many different tasks.TensorFlow Hub offers a variety of BERT and BERT-like models:Eight BERT models come with the trained weights released by the original BERT authors.24 Small BERTs have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.ALBERT: these are four different sizes of “A Lite BERT” that reduces model size (but not computation time) by sharing parameters between layers.The 8 BERT Experts all have the same BERT architecture and size but offer a choice of different pre-training domains and intermediate fine-tuning tasks, to align more closely with the target task.Electra has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).BERT with Talking-Heads Attention and Gated GELU [base, large] has two improvements to the core of the Transformer architecture.Lambert has been trained with the LAMB optimizer and several techniques from RoBERTa.MuRIL is Multilingual Representations for Indian Languages, pre-trained on 17 Indian languages (including English), and their transliterated counterparts.MobileBERT (english, multilingual), is a thin version of BERT, trained through distillation from a teacher BERT model on the Wikipedia, BooksCorpus.… and more to come.These models are BERT encoders. The links above take you to their documentation on TF Hub, which refers to the preprocessing model to use with each of them.Visit the model pages to learn more about the different applications targeted by each model. Thanks to their common interface, it’s easy to experiment and compare the performance of different encoders on your specific task by changing the URLs of the encoder model and its preprocessing.The Preprocessing ModelFor each BERT encoder, there’s a matching preprocessing model, which transforms raw text to numeric input tensors expected by the encoder, using the TF.text library. Unlike preprocessing with pure Python, this logic can be part of a TensorFlow model served directly from text inputs. Each preprocessing model from TF Hub is already configured with a vocabulary and its associated text normalization logic and needs no further set-up.We’ve already seen the simplest way of using the preprocessing model above. Let’s look again more closely:preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')input = preprocess(["This is an amazing movie!"]) {'input_word_ids': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=  array([[ 101, 2023, 2003, 2019, 6429, 3185,  999,  102,    0,  ...]])&gt;, 'input_mask': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=  array([[   1,    1,    1,    1,    1,    1,    1,    1,    0,  ...,]])&gt;, 'input_type_ids': &lt;tf.Tensor: shape=(1, 128), dtype=int32, numpy=  array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,  ...,]])&gt;}Calling preprocess()transforms raw text inputs into a fixed-length input sequence for the BERT encoder. You can see that it consists of a tensor input_word_ids with numerical ids for each tokenized input, including start, end and padding tokens, plus two auxiliary tensors: an input_mask (that tells non-padding from padding tokens) and input_type_ids for each token (that can distinguish multiple text segments per input, which we will discuss below).The same preprocessing SavedModel also offers a second, more fine-grained API, which supports putting one or two distinct text segments into one input sequence for the encoder. Let’s look at a sentence entailment task, in which BERT is used to predict if a premise entails a hypothesis or not:text_premises = ["The fox jumped over the lazy dog.",                 "Good day."]tokenized_premises = preprocess.tokenize(text_premises) &lt;tf.RaggedTensor  [[[1996], [4419], [5598], [2058], [1996], [13971], [3899], [1012]],  [[2204], [2154], [1012]]]&gt;  text_hypotheses = ["The dog was lazy.",  # Entailed.                   "Axe handle!"]        # Not entailed.tokenized_hypotheses = preprocess.tokenize(text_hypotheses) &lt;tf.RaggedTensor  [[[1996], [3899], [2001], [13971], [1012]],  [[12946], [5047], [999]]]&gt;The result of each tokenization is a RaggedTensor of numeric token ids, representing each of the text inputs in full. If some pairs of premise and hypothesis are too long to fit within the seq_length for BERT inputs in the next step, you can do additional preprocessing here, such as trimming the text segment or splitting it into multiple encoder inputs.The tokenized input then gets packed into a fixed-length input sequence for the BERT encoder:encoder_inputs = preprocess.bert_pack_inputs(   [tokenized_premises, tokenized_hypotheses],   seq_length=18)  # Optional argument, defaults to 128. {'input_word_ids': &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=  array([[  101,  1996,  4419,  5598,  2058,  1996, 13971,  3899,  1012,            102,  1996,  3899,  2001, 13971,  1012,   102,     0,     0],         [  101,  2204,  2154,  1012,   102, 12946,  5047,   999,   102,              0,     0,     0,     0,     0,     0,     0,     0,     0]])&gt;, 'input_mask': &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=  array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&gt;, 'input_type_ids': &lt;tf.Tensor: shape=(2, 18), dtype=int32, numpy=  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],         [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&gt;}The result of packing is the already-familiar dict of input_word_ids, input_mask and input_type_ids (which are 0 and 1 for the first and second input, respectively). All outputs have a common seq_length (128 by default). Inputs that would exceed seq_length are truncated to approximately equal sizes during packing.Accelerating model trainingTensorFlow Hub provides BERT encoder and preprocessing models as separate pieces to enable accelerated training, especially on TPUs.Tensor Processing Units (TPUs) are Google’s custom-developed accelerator hardware that excel at large scale machine learning computations such as those required to fine-tune BERT. TPUs operate on dense Tensors and expect that variable-length data like strings has already been transformed into fixed-size Tensors by the host CPU.The split between the BERT encoder model and its associated preprocessing model enables distributing the encoder fine-tuning computation to TPUs as part of model training, while the preprocessing model executes on the host CPU. The preprocessing computation can be run asynchronously on a dataset using tf.data.Dataset.map() with dense outputs ready to be consumed by the encoder model on the TPU. Asynchronous preprocessing like this can improve performance with other accelerators as well.Our advanced BERT tutorial can be run in a Colab runtime that uses a TPU worker and demonstrates this end-to-end.SummaryTensorFlow Hub makes available a large collection of pre-trained BERT encoders and text preprocessing models that are easy to use in just a few lines of code.Take a look at our interactive beginner and advanced tutorials to learn more about how to use the models for sentence and sentence-pair classification.Adapted from a Tensorflow Blog article by Arno Eigenwillig and Luiz GUStavo MartinsUsing BERT Models in TensorFlow was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 9
* Title: 'Welcome to the official Google Colab blog!'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/welcome-to-the-official-google-colab-blog-3a6d10f50d36?source=rss-14a828f24ca2------2'
* PublicationDate: 'Fri, 11 Nov 2022 12:24:07 GMT'
* Categories: announcements, colab

Click the Follow button to stay informed about product announcements, best practices, and featured notebooks. And let us know if you have a story or a notebook you’d like to share with our community.Welcome to the official Google Colab blog! was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Featured Notebook #1 — Digit Classifier Using Keras and TensorFlow'
* Author: 'Marc Cohen'
* URL: 'https://medium.com/google-colab/featured-notebook-1-digit-classifier-using-keras-and-tensorflow-e088e05c6ad2?source=rss-14a828f24ca2------2'
* PublicationDate: 'Wed, 09 Nov 2022 22:10:57 GMT'
* Categories: machine-learning, keras, colab, tensorflow, ai

Noteworthy Notebooks #1 — Digit Classifier Using Keras and TensorFlow.jsThis lab trains and evaluates a handwritten digit classification model using the MNIST dataset. It uses a Graphical Processing Unit (GPU) to speed up training and includes an interactive component that lets you test your model by drawing your own digits right inside this notebook.Here’s what we’ll do in this lab:Train an ML model on a GPUTest our model interactively using TensorFlow.jsExport our model for use in JavaScript and run an interactive web app to test the model interactively, with hand-drawn digits (as depicted in the animated gif above).Here’s the notebook: mco.fyi/mllab.Featured Notebook #1 — Digit Classifier Using Keras and TensorFlow was originally published in Google Colab on Medium, where people are continuing the conversation by highlighting and responding to this story.
