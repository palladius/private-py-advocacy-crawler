
== Article 1
* Title: 'Contact Tracing and Exposure Notification'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/contact-tracing-and-exposure-notification-1f70517cef6b?source=rss-ef484db59f33------2'
* PublicationDate: 'Tue, 26 May 2020 00:00:00 GMT'
* Categories: 

When Google and Apple announced in April that they would be working together on a contract tracing API, a lot of people got concerned about privacy. Today, I’m going to try to explain how these apps work so that people can make an informed decision about the technology. Personally, if an app using this technology becomes available for my area, I’ll install it.Dispelling MisconceptionsFirst, Apple and Google aren’t making apps. Instead, they are working together to build core technology to make it easier for public health authorities to build apps for their local area. By working together, they can ensure that everyone can get important health information no matter what phone they have.Second, the Google and Apple joint effort doesn’t use location data[1]. Some COVID related health apps use location data, but this project does not. To understand why location data via GPS isn’t ideal, think about how often the GPS in your phone is off by a block. Also, location data can’t differentiate between someone on the 1st floor of a building and the 15th floor of a skyscraper. That difference could be relevant for COVID-19 exposure.The Apple and Google effort uses Bluetooth. Bluetooth works well over the distances health experts say are relevant when determining COVID-19 exposure. Bluetooth also works in cell phone and GPS dead zones. Like subway stations, basements, parking garages, and offices and houses like mine that don’t have a strong cell or GPS signal. Also, the signal strength of the Bluetooth connection can be used to approximate the distance between two phones to determine if they are close enough that COVID transmission is likely.Finally, you must actively consent to have your data shared. By default, all the data stays on your device. Data is only shared if you get sick and you tell the app to share your data, which it does anonymously.If you are curious, here is Google’s Explanation of the API and Apple’s FAQ.So How Does It Work?So if these apps don’t use location data, how do they work?First, you have to install an app from your local health department. Your phone won’t get the exposure notification application automatically.Once you install the app, your phone starts broadcasting a random code via Bluetooth to anyone nearby who also has the app installed. This code changes a couple of times an hour as an additional privacy measure. Your phone keeps track of which codes it has broadcast.Your phone is also listening for any other phones nearby that are broadcasting codes. It records all the codes it “hears.”Once a day or so, the app on your phone contacts the cloud. It downloads a list of these random codes that were broadcast by the phones of people diagnosed with COVID-19. It compares that list to the list of codes it has “heard.” If there’s a match, the app shows you an alert saying you may have been exposed to COVID-19 and gives you instructions about how to proceed.If you get diagnosed with COVID-19, you can open up the app and volunteer to share all the codes your phone has broadcast for the last 14 days. If you volunteer this info, the list of codes gets uploaded to servers in the cloud. None of your personally identifying information is shared, just the codes.SummaryTo summarize the essential points:The exposure notification software Google and Apple are cooperating on does not use location data.No personally-identifying information is shared.No data leaves your phone without your consent.You choose whether or not to share your COVID-19 diagnosis, and if you do, it’s done so anonymously.The main downside I see to these apps is that they need as many people as possible to install them to be effective. My goal for this blog post is to explain the technology so that people can make an informed decision about whether to install the app for their region. I’ll be installing it, and I hope many others do as well.[1] The terms of service for the Exposure Notification API explicitly prevent apps from using location data. Apps also must be endorsed by a government health authority. Apps will not be available in the relevant store if they don’t meet terms of service.Originally published at https://thagomizer.com on May 26, 2020.

== Article 2
* Title: 'CatOps: Functions Framework, Cloud Tasks, and my cat'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/google-cloud/catops-functions-framework-cloud-tasks-and-my-cat-7b32a0b71db6?source=rss-ef484db59f33------2'
* PublicationDate: 'Wed, 22 Apr 2020 00:00:00 GMT'
* Categories: 

I have a relatively simple problem, my cat, Nick, can open our front door and let himself out. Since he’s a strictly indoor cat, this is a problem. In true computer nerd fashion, I way over-engineered a solution to lock the doors automatically after a specified delay to prevent Nick from escaping. My solution involves the Ruby Functions Framework, a container, Google Cloud Run, and Google Cloud Tasks.While this post is about CatOps, the solution is much more generalizable. If you want to call a webhook after a delay for a chatbot, send an email, or remind folks that they have items in their cart they haven’t purchased, you could do something similar to this CatOps project.ArchitectureThe hardware and triggers side of this is project is a Rube Goldberg of smart home applications and IFTTT. But the essential thing to know is that my doors have a simple HTTP interface. They send an HTTP request when unlocked and will lock if they receive a specific HTTP request.The interesting part of this project was writing some code that I could call when the door was unlocked, which would then lock it 10 minutes later. It sounds simple, but I ran into many challenges.First, I didn’t want to set up a server or build a Rails or Sinatra app. I knew that this should only be a few lines of code, and that felt like overkill. I also wanted to use Functions as a Service, which is advertised as ideal for simple scripts. Google’s FaaS solution doesn’t support Ruby, but Google just released the Ruby Functions Framework. The framework allows you to build and run functions anywhere you can run a container. I had a few choices for where to run my container, but I was curious about Cloud Run, and it promised to be simple to set up.The second issue I ran into was the 10-minute delay between opening the door and locking the door. My first attempt was to use sleep(600), but both IFTTT and the webhook interface for the door timed out waiting for a response. So I was forced to do something more elegant.Cloud Tasks was the right solution for the time out problem for several reasons. First, the HTTP Target task type sends an HTTP request when the task executes. I set the HTTP target to the webhook URL, which saved me from needing to writing code to call the webhook myself. Second, Cloud Tasks lets you schedule a task to execute at a specific time, so I could schedule my lock request to run exactly 10 minutes after the door was unlocked.The CodeThe Ruby Functions Framework is available as a gem, and you can include it in your project in the standard way using Bundler and the Gemfile.To write a function, you pass your code as a block to the http class method of the FunctionsFramework class.require "functions_framework"FunctionsFramework.http("my_function_name") do |request|  Do Stuff HereendCloud Tasks requires some setup on the server-side, which is explained well in the docs, so I won’t recreate it here. Once the initial setup is complete, creating tasks is straight forward. You create a hash that has the appropriate keys and call the create_task method.require "google/cloud/tasks"tasks_client = Google::Cloud::Tasks.newPARENT = tasks_client.queue_path(PROJECT, COMPUTE_REGION, QUEUE_NAME)task = {http_request: {http_method: "POST"}}task[:schedule_time] = {seconds: (Time.now() + 600).to_i}task[:http_request] = {url: URL_GOES_HERE}response = tasks_client.create_task(PARENT, task)The last thing I needed to do was add some configuration. I needed to know if the front door or the back door triggered the function, so I knew which door to lock after the delay. I set up an IFTTT trigger to send a parameter called “door” that could be either “front” or “back”. Since the request object is an instance of Rack::Request, you can access the parameters as usual, through the params method. Lastly, I extracted the URL that locks the doors into an environment variable for flexibility and privacy reasons.Below is the body of the function. To see the setup, you can look at the file on GitHub.FunctionsFramework.http("lock_door") do |request|  task = {http_request: {http_method: "POST"}}  door = request.params["door"]  if door == "back" then    task[:schedule_time] = {seconds: (Time.now() + DELAY_BACK).to_i}    task[:http_request] = {url: BACKDOOR}  elsif door == "front" then    task[:schedule_time] = {seconds: (Time.now() + DELAY_FRONT).to_i}    task[:http_request] = {url: FRONTDOOR}  end  begin    response = tasks_client.create_task(PARENT, task)  rescue Exception =&gt; e    FunctionsFramework.logger.error "Exception creating task"  end  FunctionsFramework.logger.info "Created task #{response.name}"  "Created task #{response.name}"endTestingThe Functions Framework gem comes with testing support that I was able to use. Since my code also uses Cloud Tasks, I needed to create a test double for those calls to prevent my tests from hitting my production task queue.Here’s the test double I created for Cloud Tasks Client.Response = Struct.new(:name) { }class TasksClientStub  attr_accessor :task_history, :project, :location, :queue  def initialize    @task_history = Hash.new { |h, k| h[k] = [] }  end  def create_task parent, task    @task_history[parent] &lt;&lt; task    Response.new("/#{task_history.length}")  end  def queue_path project, location, queue    @project = project    @location = location    @queue = queue    "projects/#{project}/locations/#{location}/queues/#{queue}"  endendThe double includes the methods I call in my function, initialize, create_task, and queue_path. Internally it represents task queues as a hash of arrays, so I can see the tasks that my function enqueues and verify they are correct. I also created a struct called response as a test double for Rack::Response because I don't need all the methods and fields in Rack.With the doubles created, I could write my tests. My code has separate branches for the front and back doors, so I wrote a test for each. Here’s the test for the front door. The test for the back door case is similar.require "functions_framework/testing"def test_function_creates_correct_task_for_front_door  task_stub = TasksClientStub.new  Google::Cloud::Tasks.stub :new, task_stub do    load_temporary "locker.rb" do      request = make_post_request "http://example.com:8080/", "door=front"      response = nil      _out, err = capture_subprocess_io do        response = call_http "lock_door", request      end      assert_equal 200, response.status      parent = task_stub.task_history.keys.first      assert_equal "projects/thagomizer-home-automation/locations/us-central1/queues/door-locker", parent      task = task_stub.task_history[parent].first      assert_match /front/, task[:http_request][:url]    end  endendOn line 6, I use Minitest’s stub method to make the function use the test double. Then on line 8, I load the function file. Inside the block, I use the function framework helper method make_post_request to build up a post request that includes the door=front parameter my function requires. To call the function, you use the call_http helper from the function framework testing package and capture_subprocess_io. Finally, the assertions verify that the function returned successfully, verify that the queue/parent is set correctly, and ultimately ensure that the task object created was correct for a front door request.Deployment and ConclusionsThe next post in this series will explain how to do the initial deployment to Google Cloud Run. It will also show you how to set up basic CI / CD on Cloud Build for any Ruby App. But if you are super excited to move forward, you can do an initial manual deploy by following the Readme for the Functions Framework.I want to close this post by being clear that the entire premise of this post is ridiculous. A much more straightforward, and likely cheaper, solution to having a problematic cat would have been to buy new doorknobs that he can’t open. But, I took joy in the pure ridiculousness of this scenario. I got to learn some new technologies in context too. I usually find that when I’m solving a problem I care about, I learn much better than if I’m copying and pasting code from a tutorial. Even if my particular use case is ridiculous, the underlying scenario of one event triggering a count down to another event is something I’ve run into over and over. There are a lot of serious business problems that can be solved using code similar to my CatOps project.Originally published at https://www.thagomizer.com on April 22, 2020.CatOps: Functions Framework, Cloud Tasks, and my cat was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 3
* Title: 'Cleaning Up Time Machine Local Snapshots'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/cleaning-up-time-machine-local-snapshots-a82719ebd397?source=rss-ef484db59f33------2'
* PublicationDate: 'Tue, 27 Mar 2018 00:00:00 GMT'
* Categories: apple, backup, time-machine

About a week ago my mac complained that it was running out of disk space. I deleted a bunch of old TV shows and GoPro videos and called it good. Today I went to do an external backup and saw that my drive was still full. Half an hour of investigation and I realized my 500 GB drive had 230 GB of local Time Machine backups. In fact, it was full enough that the Time Machine GUI wouldn’t start, I couldn’t run more backups, and several other basic utilities on my mac stopped working. It took about three hours of experimenting to get the backups under control and the disk back to 50% full. If you run into a similar situation here’s what worked for me.tmutilBefore OS X High Sierra there was a command you could run, tmutil disablelocal that would turn off local snapshots. It was a fast way to stop time machine from continuing to fill the disk while you debugged another problem. The High Sierra version of tmutil doesn’t have this command, so I had to try something different. I found listlocalsnapshotdates useful for seeing the scope of the problem.tmutil listlocalsnapshotdates \ /Volumes/com.apple.TimeMachine.localsnapshotsThat gives a list of the snapshots. My machine had 51 snapshots covering more than a month, with several days have eight or more snapshots. Once I had the list I started to delete the snapshots manually using this command:tmutil deletelocalsnapshots YYYY-MM-DD-HHMMSSThe first couple deletes went well, but after that, they stopped working. More reading of man pages and I found the command thinlocalsnapshots. The man page isn’t very verbose for this command, so I had to experiment a bit to find something that worked. According to the documentation, this command tries to free up a specified number of bytes from a given Time Machine mount. There’s also an optional urgency parameter which is a value between 1 and 4. I tried the first run with urgency 4 and found it took a long time and only thinned one snapshot. When I specified an urgency of 1, it freed the specified amount of space quickly.tmutil thinlocalsnapshots \ /Volumes/com.apple.TimeMachine.localsnapshots 20000000000 1After running this command and doing a couple of the steps in the next section, my disk was back to a more reasonable 57% full. Once there was space available, I was able to run a Time Machine backup to an external disk. That, in turn, cleaned up a few more local snapshots.Once the backup finished, I verified that I could start Time Machine and restore files once again. If you end up having to go down this path be aware that the first backup after your cleanup will probably take longer than usual. The documentation I could find on forums leads me to believe that the local snapshots are used to increase the speed of Time Machine’s preparation step. My first backup after fixing took nearly three hours (and copied 32 GB of data).I also found it helpful to disable automatic backups in the Time Machine UI while I was cleaning things up so that Time Machine wasn’t writing new snapshots as I deleted the old ones.I had to use a couple of tools to figure out that Time Machine local snapshots were using all the available disk space on my mac. The tools that OS X shows you when it warns you that you are low on disk space don’t make it clear what is going on. I saw that I had 220 GB of “purgeable” data (the backups), but I couldn’t figure out how to purge that data. In the end, a combination of the Unix utility df, realizing that the backups were in their own directory under \Volumes, and OmniDiskSweeper made me aware that I only had about 250 GB of real data and the rest was deletable backups.While solving the “disk full” problem, I realized I needed to update my Time Machine exclusions. I had about 40 GB of GoPro videos and screencasts that I was mistakenly including in my Time Machine backups. You can edit your exclusions under Time Machine options in the GUI. I added all the folders that don’t change often and are primarily media and that should help prevent this situation in the future.While I’ve decided it doesn’t make sense for me to backup my media via Time Machine, I do want to have it backed up somewhere offsite. I work at Google, so for me, the obvious solution was to use a Google Cloud Storage Bucket. I created a bucket that uses one of the lower priced and lower availability storage classes. It doesn’t make sense to pay for frequent read access to my backups. Once the bucket was set up, I wrote a simple bash script to use gsutil rsync to sync my media folders to GCS on a regular schedule. This doesn’t give me the versioning that I get with Time Machine, but it does give me offsite backups of my photos and other media files.The result of several hours of work is that my disk is no longer full, my backups are in a much better place now than they were yesterday, and my backup strategy includes offsite backups of both my personal files and my media. Now I just need to add regular checks to ensure that I can still restore and I’ll be all set.I want to thank Ryan Davis for helping me find the folders that contained media files that should be excluded from Time Machine and old mobile device backups that I could safely delete.03/27/18Originally published at www.thagomizer.com on March 27, 2018.

== Article 4
* Title: 'A Rubyist Learns List Comprehensions'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/a-rubyist-learns-list-comprehensions-b911f765f0e4?source=rss-ef484db59f33------2'
* PublicationDate: 'Wed, 21 Mar 2018 00:00:00 GMT'
* Categories: ruby, python, programming

Google Cloud supports three dynamic languages, JavaScript, Python, and Ruby. One of my goals is to become competent enough to debug and make small patches to libraries in all three. I love Ruby, and I’ve written Javascript professionally, so coming up to speed on Python has been my latest “side” project. I worked through the Python Koans in February, and folks had warned me that list comprehensions are confusing. It turns out that while they are odd to my Ruby sensibilities they are not hard. Even better, some things that are hard in Ruby are one-liners in Python with list comprehensions. So if you are a Rubyist trying to learn Python, here are some of the basics of list comprehensions explained side by side with equivalent Ruby.List Comprehensions?!?!In Python list comprehensions are used in places where a Rubyists would chain methods from Enumerable. For example, if I wanted to return the squares of all multiples of three less than 50 in Ruby, I would write something like this.(1...50).select { |x| x % 3 == 0 }.map { |x| x**2 }This code uses a range and two methods from Enumerable. First I use select to extract the multiples of three. Then I use map to square the resulting array. Using a list comprehension in Python, I can do this in one step.[x**2 for x in range(0, 50) if x % 3 == 0]First, let me give a quick syntax explanation. The square brackets indicate that this is a list comprehension. The first part of a list comprehension is what you want at the end, in this case, I want squares, so my list comprehension starts with x**2. After that, I tell Python what numbers to square by specifying a range and how to iterate over it with for x in range(0, 50). Finally, I need to filter those results with a postfix if x % 3 ==0.In my brain, the three pieces of a list comprehension map to “do this”, “for these values”, “in these situations”.I like that the Python list comprehension matches pretty well with English syntax. The Ruby version, translated into English would be something like, “Take the numbers from 1–50, choose only the multiples of 3, and then square each of those.” It is understandable but feels pretty “mathy.”But the best thing I learned about list comprehensions is that they are a fast way to generate all the pairwise combinations of two lists. I’m not sure how often you need to do that in production code it but comes up a lot in coding puzzles. Imagine you want all the combinations of one item from list A and one item from list B. If it helps pretend we’re making omelets.A = ["Ham", "Chicken", "Veggies"] B = ["Feta", "Cheddar"]I don’t know a one line way to do this in Ruby. The code below is close, but the result doesn’t have the correct nesting.A.map { |a| B.map { |b| [a, b] }}With a list comprehension in Python, it is straightforward.&gt;&gt;&gt; [[x, y] for x in a for y in b] [['Ham', 'Feta'], ['Ham', 'Cheddar'], ['Chicken', 'Feta'], ['Chicken', 'Cheddar'], ['Veggies', 'Feta'], ['Veggies', 'Cheddar']]For me, this is the killer feature of list comprehensions. I’m sure that there are community standards about how many layers deep is reasonable, but I like that I can take a concept that works on one list and seamlessly apply it to several lists or nested lists.So Pythonistas and Rubyists, what did I get wrong? Let me know in the comments.03/21/18Originally published at www.thagomizer.com on March 21, 2018.

== Article 5
* Title: 'Testing In Production'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/testing-in-production-65f164bf1850?source=rss-ef484db59f33------2'
* PublicationDate: 'Sun, 11 Mar 2018 00:00:00 GMT'
* Categories: production, devops, testing

In April I’m giving a talk at RailsConf titled “Testing in Production.” Today I thought I would write about a straightforward type of testing in production: running your existing tests against production.There’s a good chance you have some kinds of tests for your code. You probably have some unit tests and you may have higher level integration tests. Maybe your integration tests are written in something like cucumber or a record/replay test automation tool. Now, the hard question, do you run these tests against production? Most folks I talk to don’t. They think of testing as something that happens in pre-production environments. If you have a QA/test team working on your project you may even think of testing as something that delays the push from development to production. As a former tester, I’d like to propose that you run at least some of your tests against production.The StoryI started my career in QA doing both black box manual testing and automated testing using record/replay tools. While most record/replay tools create brittle tests, the program I was using at the time allowed you to parameterize the recording, so I was able to create a small set of tests that were pretty resilient. We used these tests whenever we pushed code, which was approximately once a month. Once we had confidence that the scripts worked, I got permission to run them against the live site on a spare computer I had in my office (I was using it partly as a space heater). I set up the tests to run every four hours and email me the results. Then I set up a filter in my email to file away all the successful runs. I set this up mostly as a way to know when the tests needed fixing. I didn’t expect to find actual bugs.These tests ran without incident for several months, but one day a failure came through. I assumed that the tests were broken so I manually retried the scenario. It turned out that the tests had detected an actual error. The site I was working on at the time relied on several external partners to provide inventory. The test detected that one of the partners was returning empty responses to our queries. The requests weren’t failing, so the monitoring systems hadn’t noticed the error. The website’s code was designed to handle poorly formed responses gracefully, so customers hadn’t noticed there was an issue. But there was an issue, and the test had found it before any other part of our system detected the outage. I notified the appropriate staff, and they notified the partner, and the issue was fixed quickly and with no impact on real users.The HowToolingRunning tests against production sounds easy but some test frameworks make it easier than others. Unit and functional testing frameworks often assume that the code under test and the tests themselves are running on the same computer. Some integration test suites allow you to specify a URL to run against, but others do not, so what is the best course of action? I believe that where possible the integration tests you run in production should be the same ones you run against staging and dev environments. When I haven’t been able to do that, I use a tool like mechanize, which is also available for at least Python, Node, and Perl, to script the production website interactions I want to verify.What To VerifyI try to limit my production test suite to the core workflows of the website. In the past, I’ve prioritized tests for authentication, purchase, and any other endpoints that drive a majority of the site traffic. I’ve also run tests in production for features that we wanted to demonstrate to potential investors or users. Like all testing, it is essential that your tests actually verify something concrete. If your tests just verify that the URL doesn’t return a 500, they aren’t particularly useful. I’ve always just picked critical text or links on the page and verified they appear in the HTML response. This doesn’t guarantee the page rendered successfully, but it should let you know that it was at least close and there were no major malfunctions.In my experience, it is helpful to have a way to distinguish test data from actual user data. This lets the team not include test data in reports about usage and lets you purge it from the database occasionally. There are many ways to flag test data. You can use a URL parameter, a database field, or a special naming scheme or even all three. The other way I avoid creating a bunch of test data in a production system is choosing tests that don’t create records or clean up after themselves. If I need to search for five products to cover the breadth of my site, I can do five search tests. Then I can write a separate purchase test, limiting the amount of data created. After that you can even use the transaction from the purchase test to run a cancellation test, cleaning up the purchase. Although I dislike interdependent tests, I make an exception in this case because the benefits of a test suite that cleans up after itself are more than the downsides of two well-documented but interdependent tests.Wrapping upThis is just one part of what people consider testing in production, but it is easy to implement. If you have an existing integration test suite, you can choose a few specific tests to run against your production environment right away. And this technique does find bugs. It is especially likely to find bugs in integrations, bugs that only appear at certain times, and bugs that result from configuration errors. I’m not an expert on testing, and I’ve only had a chance to try these techniques out on three different products during my career, so I’m curious if you’ve tried this and how it worked for you. Let me know in the comments.03/11/18Originally published at www.thagomizer.com on March 11, 2018.

== Article 6
* Title: 'The Toaster Parable'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/the-toaster-parable-5f5a45c36069?source=rss-ef484db59f33------2'
* PublicationDate: 'Thu, 18 Jan 2018 00:00:00 GMT'
* Categories: tech-culture, documentation, developer-relations

This is part of the Developer Relations series.The tech industry has a problem that I refer to as the “the toaster problem,” which is of course, best illustrated by “the parable of the toaster”. Suppose you are in the break room and a new employee says “Hey, can you help me with the toaster?” You spend the next 10 minutes explaining how the various knobs and dials work. You say that this toaster uses photoelectric sensors to detect browning instead of a thermostat or a timer. You teach them that the little red bits inside the toaster are filaments that have electric current run through them and that’s why you shouldn’t stick a knife into the toaster while it is plugged in.If the new employee were wondering “Hey, how does this fancy toaster work?” your explanation would be perfect. But if the employee was thinking “Hungry. Want toast.” they are probably hungrier than when they started, and they still don’t have toast. Of course, you were right to give them the full explanation of toasters. The new person will be safer understanding why they shouldn’t stick a fork into the toaster. Understanding that the toaster works on photoelectric sensors will help them understand why it works a bit differently on white bread versus brown. Since the new employee understands the toaster, they’ll be better equipped to debug toaster related issues in the future. But at the same time, most folks would say that you should have just shown them where the bread is kept and how to plug in the toaster.The problem with the tech industry is that we often explain the details of toaster design to folks who just want a piece of toast. We give them tons of details and expect that they’ll learn all about how toasters work. From there they’ll be able to make perfect toast. That logic is sound, except some percentage of people are going to get annoyed, wander off, and just have another snack instead.I don’t think giving the full explanation is a bad thing. This analogy came to me after a particularly frustrating meeting where the folks on the other side of the table kept asking “but how does it work,” and I kept replying “I don’t care, it just does.” I didn’t need to understand how the toaster worked to make toast, but the other folks did need that understanding. Our brains worked differently. Depending on context, we all fall somewhere on the “How do toasters work?” to “Hungry. Want toast.” continuum. To reach people on the other side of the toaster debate, I had to believe that understanding how toasters worked was important for them and then provide them the information they wanted. Understanding the opposite perspective is hard. I’m pretty biased toward “let’s just make toast” most of the time. After all, the failure cases for toast aren’t that bad. But the underlying tension of the toaster parable keeps coming up. No matter how many times I try to convince people they don’t need to understand the details some people truly do need that information. Whether you are a “toast, now” person or an “understanding the details” person seems to be at least partially a personality trait.The big issue with the toaster problem is that the solutions to “Hungry. Want toast.” are different than the solutions to “How do toasters work?” Neither concern is better than the other. There are multiple great solutions to both issues. But if you try to provide a “How do toasters work?” solution to “Hungry. Want toast.” it won’t go particularly well.So how does this apply to tech? Go look at the documentation for your favorite library, framework, or tool. Does it address “Hungry. Want toast.” or “How do toasters work?” Most of the documentation I’ve looked at skews one way or the other. Some companies are known for providing great “Hungry. Want toast.” documentation. Others are great at providing all the details about how the underlying technology works. Rightly or wrongly, many folks believe the SRE community is more on the “How does the toaster work?” end of the spectrum. Developers at startups often are on the “Hungry. Want toast.” end. If we want to help a wide group of people, we need to ensure we’re providing information from both of these perspectives.Addressing the toaster problem is straightforward, but it takes work. The first step is to acknowledge that the other perspective exists and is just as valid as your own. There’s a tendency in technical communities to look down on folks who don’t want to understand the inner workings of their kernel and are just happy that the gnomes in their computer keep returning correct answers. This isn’t an issue of better. This isn’t an issue of one group having the right priorities. In my experience, this is an issue of different people’s brains working differently. So back away from the frustration and the judgment and try to see the other side. I’ll be doing the same.Second, we need to look at our talks, our documentation, and our tutorials and make sure we aren’t only providing solutions to one of these problems. It is human nature to explain things in the way we’d want to learn them. Just looking at my blog I write a lot about “how to build things with “ and very little about “how works.” Once you’ve seen your bias, find ways to address it. You can try to create content from the other perspective, or if you are part of a team, you can find the folks on the team who tend the other way and use teamwork to achieve balance.Finally, we need to be better at figuring out the perspective of our audience. Providing a detailed schematic for the toaster isn’t helpful to someone who is hungry and handing a piece of toast to someone who needs to understand the inner workings to be successful is frustrating. When we are giving advice, we can ask about someone’s goals to get a better idea what type of information to provide them. When we are giving conference talks, we can use the abstract to help folks understand which problems we’ll be addressing. And we can use the introductions to blog posts, documentation, and tutorials to help people figure out if this content will address their current issue. It may take a couple more minutes and some awkward perspective shifting but by considering the toaster problem I’ve reduced conflict in design discussions and had more success reaching a wide audience.Have you run into the toaster problem in your work? I’d love to hear about it.01/18/18Originally published at www.thagomizer.com on January 18, 2018.

== Article 7
* Title: 'Security Advice for GCS Buckets'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/google-cloud/security-advice-for-gcs-buckets-6a6e9825d49f?source=rss-ef484db59f33------2'
* PublicationDate: 'Wed, 17 Jan 2018 00:00:00 GMT'
* Categories: devops, security, cloud-storage, google-cloud-platform

Almost every non-trivial website will have storage needs. As a Google Developer Advocate, I’ve been using Google Cloud Storage and talking to customers about their issues and difficulties with GCS. Security has come up a couple of times in these discussions. While I can’t tell you how to make your buckets and objects 100% secure, I can give you some basic advice that is pretty universally applicable. The vast majority of this information came from a week of research into the security of GCS buckets in a typical use case that I did in early December.Keep Your Credentials to YourselfI know it seems obvious that you shouldn’t share passwords, service accounts, API keys, or other secrets. Yet, leaked credentials are a common source of security issues. The accepted best practice is that each individual and each application should have their own credentials. This makes it easy to remove one person’s permissions without affecting any of the other users of the buckets. It also gives you visibility into who is doing what with your audit logging. But just assigning everyone an account isn’t sufficient. There have been high profile leaks because one engineer’s credentials got leaked. Ensure that credentials are getting stored securely enough for your use case. A piece of paper under your keyboard tray isn’t secure, but it is a lot more secure than checking credentials (or dotfiles) into a public source repository. And if you use password managers or check credentials into source then the credentials are only secure as the password manager or source repository.Bucket Names are DiscoverableIt wasn’t obvious to me when I started storing data in the cloud that bucket names are often discoverable by anyone. Complicating this is that in GCS and other cloud storage providers bucket names must be global unique. Under these constraints it feels like a good idea to put your project, company, application, or even personal credentials into the bucket name. I used to do this since it was easy to come up with a unique bucket name that had “aja-“ as a prefix. Once I realized that a sufficiently motivated individual could probe my buckets I took all personal information out of bucket names. Depending on you access the contents of a bucket the object name may also appear in URLs, and therefore in browser or firewall logs. Again, this is another good reason to not put secrets into object names. In the official storage best practices doc Google Cloud recommends using guids for bucket names if you need a lot of buckets.Double Check PermissionsMy last tip is to regularly audit permissions and ACLs. I’ve found misconfigured buckets when I double check permissions on my project. Maybe you were setting up the proof of concept and made your bucket globally readable while you sorted out the rest of the application. Most of us do stuff like that occasionally, but it becomes a problem if that bucket lives on into production with the same permissions. With GCS most projects use IAM permissions, but you may need to use ACLs lists if you need to have different permissions for objects in the same bucket. When I was digging into GCS permissions, I discovered that I had made incorrect assumptions about what different scopes meant. For example, I had assumed the “AllAuthenticatedUsers” scope meant all the users of my cloud project. When I read the documentation, I realized that it meant anyone with a Google account. If I want to limit access to just collaborators on my project I can do that with other scopes. Of course, many projects have valid reasons for making a bucket globally readable. Perhaps you are using the bucket to store assets for your web application, or maybe you are using a bucket to store a scientific data set that you are sharing with others. Just make sure that the permissions are appropriate for your use case and set a reminder to audit them every so often to ensure that your use case hasn’t changed.  This blog post only covers a few points. Before you take your app to production, I recommend reading more information on best practices for Google Cloud Storage on the Google Cloud site. That document is kept up to date, unlike a blog post.01/17/18Originally published at www.thagomizer.com on January 17, 2018.Security Advice for GCS Buckets was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 8
* Title: 'Case Statement Magic'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/case-statement-magic-127290532ce?source=rss-ef484db59f33------2'
* PublicationDate: 'Thu, 11 Jan 2018 00:00:00 GMT'
* Categories: programming, conditional, ruby

Earlier this week, a conversation on the Seattle.rb Slack reminded me of one of my favorite hidden Gems in Ruby, the case statement. Most languages have syntax for multiway branching. Java calls it switch. Lisp uses cond. Ruby calls it case which relates back to the mathematical roots of this construct. Ruby’s implementation of this feature is particularly flexible, and many folks don’t realize all the things you can do with it.Testing ValuesHere’s a straightforward example of a case statement the way we typically think of them.case my_var when 1   puts "one" when 2   puts "two" else  puts "too big!"endRuby evaluates the variable my_var against each of the conditions listed. If the condition is true, the associated code branch is executed. But you can put more than a single value after the when. In this example, I use several different values separated by commas to specify each branch.case my_var when 1, 3, 5  puts "odd" when 2, 4, 6  puts "even" else   puts "uh-oh" endAnd in this example, I use ranges instead of explicit values.case my_var when (1..5)  puts "Left Hand"when (5..10)  puts "Right Hand"endYou can also use regular expressions.case my_string when /[a-z]+/   puts "lower" when /[A-Z]+/   puts "UPPER" endAdding More FlexibilityThe above shouldn’t look too bizarre to folks who know Java or C. The syntax is a bit different, but the basic use case of testing a variable against several possible values is the same. This isn’t how I normally use a case statement in Ruby though. Most of my case statements look something like this:case when x % 3 == 0   "Multiple of 3" when y.odd?   "Y is Odd" when z.even?   "Z is Even" endIn Ruby the expression after the reserved word case is optional. When used like this the case statement is just a different way of doing multiple if/elsif/else lines. It is also analogous to the cond in Lisp which works the same way. I prefer to use case over if/elsif/else. I like that the whens and conditions line up vertically. In my brain, case means picking among more than two alternatives, and so semantically case makes more sense when I have a multiway conditional. Using case for multiway conditionals is so habitual for me that I have to think for a second about how Ruby spells “else if” if I’m using if for branching.Our Friend ===No discussion of case would be complete without discussing ===, often called “case equality”. === is also called the “is_a?” operator. mammal === pangolin would be true because a pangolin is a mammal. Note that the order is backwards of what we’d normally think in Ruby, it isn’t mammal.is_a? pangolin, but pangolin.is_a? mammal. The case statement uses === when determining which branch to follow and this allows for statements like this:case my_var when Numeric   puts "numbers" when String   puts "String" when Regexp   puts "Reg Ex"endBut === can be overridden by any class and some classes do interesting things. Range defines === as an alias for include?. Regexp defines === as an alias for match. While these don’t make sense with the ‘is_a?’ explanation they do make sense if you think in terms of sets. === is set inclusion. For example a === b is true when b is in the set defined by a. In that context Range#=== being defined as include? makes sense. when (1..5) is testing whether the variable is part of the set of things defined by (1..5).But there are some even more unusual definitions of === in the Ruby Standard Lib. In Brandon Weaver’s Triple Equals Black Magic I learned that Proc defines === to be call and IPAddr defines it to be subnet inclusion. This means we can do things like this:case my_var when Proc.new { |n| n.odd? }   puts "Odd number" when Proc.new { |n| n.even? }   puts "Even number" endI probably wouldn’t use this in production code, just like I won’t call next with a return value in production code. While both of these are valid Ruby, they aren’t idiomatic. Code that isn’t idiomatic is harder to understand and therefore maintain. So I’ll continue marveling at how awesome Ruby is but I’ll avoid using Procs in case statements unless the alternative is even less readable.01/11/18Originally published at thagomizer.com on January 11, 2018.

== Article 9
* Title: 'GCP Audit Logs'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/google-cloud/gcp-audit-logs-9dffe2f6ed75?source=rss-ef484db59f33------2'
* PublicationDate: 'Wed, 27 Dec 2017 00:00:00 GMT'
* Categories: logging, google-cloud-platform, audit-logs

Audit logs are boring. At least we hope they’re boring. If your audit logs are exciting, you are likely having a bad day. But audit logging of some sort is often a good idea, and many of us forget to set it up and verify that we understand the data on a regular basis. This post walks through setting up and using the audit logging capabilities of GCP. Operations and site reliability are not my areas of expertise. Also, I am not a security professional or a compliance lawyer. Please consider this blog post just one of the many things you should be doing regarding logging and monitoring when preparing your projects for the big scary internet.TerminologyBefore I dive into the how-to, I want to refine my terminology a bit. There are many things called audit logs. In this post, I’m focusing on two categories of logs: Admin Activity Logs and Data Access Logs. Admin Activity Logs record the actions of admins on your project. These may include spinning up new instances, altering the project metadata, enabling APIs, or deploying an application. Having good admin audit logs allows you to retroactively figure out how a given change was made and often who made it. Data Access Logs are what you’d expect, they record when users access data. Google Cloud Platform has Data Access Logs for Cloud Storage, Cloud Dataproc, Deployment Manager, Cloud SQL, Compute Engine, and several other products.Most people have some admin activity logging setup. Admin activity logs, like all logs, are only as useful as what you are logging. I hope no one is still doing this, but ten years ago it was pretty common practice to have a shared admin account used by multiple people. I worked on a system like this, and when configs were changed in production, we couldn’t track down the person responsible since all the logs showed up as root.Data Access Logs are less commonly used for a couple of reasons. First off, a lot of the data we store we want folks to access, and we don’t need to know who. Things like images, web assets, public data sets, and scripts are in this category. Second, data access logs can be extremely verbose. Knowing that someone listed the contents of your backups bucket is noise if your only goal is to find out when someone accesses “my-super-secret-file.txt.” And while noisy logs make some people feel safer, I find they make my job harder.Admin Activity LogsOn GCP most admin activity is logged by default. Here is a list of all the products that have admin activity logs and what release stage the logging support is at.There are a couple of places that you can view the recent admin actions. The first is on the activity tab of the GCP Console homepage. This view allows you to see admin actions for multiple projects in an organization. You can filter by the type of log or the service (GKE, App Engine, BigTable) and the type of event you are interested in. You can also filter logs by the actor if that is helpful.You can also view logs in Stackdriver Logging where there are additional querying and filtering capabilities. Stackdriver Logging also allows you to set-up alerts on your logs by creating a metric. I did a longer blog post about this earlier this year. There’s also a Cloud Minute video that demonstrates the steps to create a log-based alert.Data Access LogsData Access logs on GCP are not enabled by default (BigQuery is an exception to this). If you want to setup Data Access logs you must explicitly turn them on. All the documentation on enabling data access logs is here, but this post will walk you through the basics.Data Access logs are configured as part of the IAM policy for a project or organization. You also cannot turn them on or off in the UI; you must use the ‘gcloud’ command line tool to modify the IAM policy. Finally, be careful when editing the IAM policy. If you configured the IAM policy incorrectly, you could remove all access to a project or organization. One of my co-workers found this out by accident and rendered her project unusable.I find it easiest to use Cloud Shell to make changes like this. You can use your machine, but Cloud Shell has an up to date version of gcloud installed plus my favorite text editor.The first step is to pull down your existing IAM policy. You can do this with the following command:gcloud projects get-iam-policy [PROJECT_ID] &gt; policy.yamlNow you need to edit that file and add the data access log configuration at the top of the file. To enable all data access logs, add the following to the top of your file.auditConfigs:- auditLogConfigs:  - logType: ADMIN_READ  - logType: DATA_WRITE  - logType: DATA_READ  service: allServicesIf you have a publicly readable Cloud Storage bucket, it might make sense to enable data access logs just for writes. This would let you see when the bucket was updated but would eliminate log spam from millions of users. In that case, you would add the following to your policy file:auditConfigs:- auditLogConfigs:  - logType: DATA_WRITE  service: storage.googleapis.comOnce you have a policy file, you can apply it using the gcloud projects set-iam-policy [PROJECT_ID] ./policy.yaml. This will apply your new policy and turn on audit logging for your project.Learn MoreAudit logging is a complex topic. There are many opinions on how to do it “properly.” What works for one project may not work for another. To learn more about the audit logging support that GCP offers, please check out the audit logging page on the GCP website.12/27/17Originally published at www.thagomizer.com on December 27, 2017.GCP Audit Logs was originally published in Google Cloud - Community on Medium, where people are continuing the conversation by highlighting and responding to this story.

== Article 10
* Title: 'Metaprogramming and Currying'
* Author: 'Aja Hammerly'
* URL: 'https://medium.com/@thagomizer/metaprogramming-and-currying-f68e681d95a1?source=rss-ef484db59f33------2'
* PublicationDate: 'Tue, 19 Dec 2017 00:00:00 GMT'
* Categories: functional-programming, currying, advent-of-code, metaprogramming

Along with what feels like half of the internet, I’m participating in Advent of Code this December. The problems/puzzles in this exercise are the kind of small programming exercise that I enjoy. Incidentally, many of these problems, especially the early ones, look very similar to interview questions I’ve been given over the years.This year there have been many problems that require writing an interpreter for a simple language. My favorite of these has been day 16 which had you execute some “dance moves” that modify an array. One of the dance moves is “spin” which involves rotating the array by a constant. Another is “exchange X Y” where the elements in the array at X and Y change places. The moves themselves aren’t complicated. The parsing of the input file isn’t complicated either. The problem gets hard though when you are asked to execute the entire sequence of dance moves 999,999,999 times.In my first solution, I was using regexes to parse the instructions every time I made a move. This was incredibly slow. My next thought was that I should parse the file once and just store the code that needed to be executed in each step. This put me in the position that I needed to write code that could take the constants I was parsing out of the file and create a function that could perform the correct actions on the array later.For example, take the instruction x5/3. This is an exchange instruction that says the elements at position 5 and position 3 change places. To implement my “parse once, run the code multiple times” approach I needed to write a function that took in 5 and 3 and returned a function that would take the array and swap elements 5 and 3. In Ruby we metaprogram with lambdas and procs. After some trials and errors, I ended up with this function that wrote code to handle the exchange case.def make_exchange x, y  lambda { |ary| ary[x], ary[y] = ary[y], ary[x] }endThis code returns a function that takes an array and does the exchange. When I parse the x5/3, I pass 5 and 3 to this function and get a lambda back. When it is time to modify the array I pass the array to the lambda like this l.call ary. This works, and it is surprisingly fast, but I found the multiple levels of metaprogramming a bit confusing. What I wanted was to define a method exchange that took two locations and the array as parameters. To do this though I needed a way to supply the x and y while parsing the file and the array while running the array transformations.Supplying parameters to a function in stages reminded me of currying in languages like Scheme and Lisp. Out of curiosity I searched “Ruby currying” and found out that Proc#curry is in the standard lib. To use Proc#curry, you create a Proc object and then supply whatever parameters you have in square brackets. When the Proc has enough arguments, it runs and returns the result. Here’s an example of Proc#curry for addition.p = proc { |x, y| x + y }=&gt; #&lt;Proc:0x007f9fcf990108&gt;q = p.curry[3]=&gt; #&lt;Proc:0x007f9fd01b9ed8&gt;q.curry[5]=&gt; 8On the first line, I create a new proc named p that adds its two arguments. This returns a proc object. Then I call curry with the argument 3, supplied in square brackets. This returns a new proc object that I store in the variable q. Finally, I call q.curry with the argument 5 and that returns the sum 8.So how can this be used for my advent of code problem? Instead of defining a function that writes a lambda I can specify the lambda once and use curry to supply the arguments as I have access to them. Using this method, my code looks like this:exchange = lambda { |x, y, ary|                    ary[x], ary[y] = ary[y], ary[x]                  }# When parsing I run this codeinstruction = exchange.curry[$1.to_i, $2.to_i]# When executing the instructions, run thisinstruction.curry[ary]This solution works, but I won’t say it is a good idea. It is slower than the solution I described above. I can’t think of a case where I’d use currying instead of metaprogramming in production. But it was a fun diversion, and it provided an opportunity to explore using Ruby more functionally.12/19/17Originally published at www.thagomizer.com on December 19, 2017.
