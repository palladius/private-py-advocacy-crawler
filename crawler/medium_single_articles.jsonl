# short:
{"author_name": "Daniel Sanche", "published_time": "2018-05-14T00:50:43.680Z", "title": "Hardware"}
{"author_name": "Riccardo Carlesso", "published_time": "2023-12-19T10:11:19.056Z", "title": "Hey Bard, write a responsive Javascript Search Engine app for me.."}
{"author_name": "Romin Irani", "published_time": "2024-05-28T05:15:52.024Z", "title": "A Tale of Two Functions : Function calling in Gemini"}
---
{"title": "Hardware", "twitter_creator": null, "twitter_image": "https://miro.medium.com/v2/resize:fit:1200/1*uyMd-QxYaOk_APwtuScsOg.png", "content": [["Google Cloud - Community", "Listen", "Share", "Kubernetes is quickly becoming the new standard for deploying and managing software in the cloud. With all the power Kubernetes provides, however, comes a steep learning curve. As a newcomer, trying to parse the ", "can be overwhelming. There are many different pieces that make up the system, and it can be hard to tell which ones are relevant for your use case. This blog post will provide a simplified view of Kubernetes, but it will attempt to give a high-level overview of the most important components and how they fit together.", "First, lets look at how hardware is represented", "A ", " is the smallest unit of computing hardware in Kubernetes. It is a representation of a single machine in your cluster. In most production systems, a node will likely be either a physical machine in a datacenter, or virtual machine hosted on a cloud provider like ", ". Don\u2019t let conventions limit you, however; in theory, you can make a node out of ", " ", ".", "Thinking of a machine as a \u201cnode\u201d allows us to insert a layer of abstraction. Now, instead of worrying about the unique characteristics of any individual machine, we can instead simply view each machine as a set of CPU and RAM resources that can be utilized. In this way, any machine can substitute any other machine in a Kubernetes cluster.", "Although working with individual nodes can be useful, it\u2019s not the Kubernetes way. In general, you should think about the cluster as a whole, instead of worrying about the state of individual nodes.", "In Kubernetes, nodes pool together their resources to form a more powerful machine. When you deploy programs onto the cluster, it intelligently handles distributing work to the individual nodes for you. If any nodes are added or removed, the cluster will shift around work as necessary. It shouldn\u2019t matter to the program, or the programmer, which individual machines are actually running the code.", "If this kind of hivemind-like system reminds you of the ", ", you\u2019re not alone; \u201cBorg\u201d is the name for the ", " Kubernetes was based on.", "Because programs running on your cluster aren\u2019t guaranteed to run on a specific node, data can\u2019t be saved to any arbitrary place in the file system. If a program tries to save data to a file for later, but is then relocated onto a new node, the file will no longer be where the program expects it to be. For this reason, the traditional local storage associated to each node is treated as a temporary cache to hold programs, but any data saved locally can not be expected to persist.", "To store data permanently, Kubernetes uses ", ". While the CPU and RAM resources of all nodes are effectively pooled and managed by the cluster, persistent file storage is not. Instead, local or cloud drives can be attached to the cluster as a Persistent Volume. This can be thought of as plugging an external hard drive in to the cluster. Persistent Volumes provide a file system that can be mounted to the cluster, without being associated with any particular node.", "Programs running on Kubernetes are packaged as ", ". Containers are a widely accepted standard, so there are already many ", " that can be deployed on Kubernetes.", "Containerization allows you to create self-contained Linux execution environments. Any program and all its dependencies can be bundled up into a single file and then shared on the internet. Anyone can download the container and deploy it on their infrastructure with very little setup required. Creating a container can be done programmatically, allowing powerful ", " pipelines to be formed.", "Multiple programs can be added into a single container, but you should limit yourself to one process per container if at all possible. It\u2019s better to have many small containers than one large one. If each container has a tight focus, updates are easier to deploy and issues are easier to diagnose.", "Unlike other systems you may have used in the past, Kubernetes doesn\u2019t run containers directly; instead it wraps one or more containers into a higher-level structure called a ", ". Any containers in the same pod will share the same resources and local network. Containers can easily communicate with other containers in the same pod as though they were on the same machine while maintaining a degree of isolation from others.", "Pods are used as the unit of replication in Kubernetes. If your application becomes too popular and a single pod instance can\u2019t carry the load, Kubernetes can be configured to deploy new replicas of your pod to the cluster as necessary. Even when not under heavy load, it is standard to have multiple copies of a pod running at any time in a production system to allow load balancing and failure resistance.", "Pods can hold multiple containers, but you should limit yourself when possible. Because pods are scaled up and down as a unit, all containers in a pod must scale together, regardless of their individual needs. This leads to wasted resources and an expensive bill. To resolve this, pods should remain as small as possible, typically holding only a main process and its tightly-coupled helper containers (these helper containers are typically referred to as \u201cside-cars\u201d).", "Although pods are the basic unit of computation in Kubernetes, they are not typically directly launched on a cluster. Instead, pods are usually managed by one more layer of abstraction: the ", ".", "A deployment\u2019s primary purpose is to declare how many replicas of a pod should be running at a time. When a deployment is added to the cluster, it will automatically spin up the requested number of pods, and then monitor them. If a pod dies, the deployment will automatically re-create it.", "Using a deployment, you don\u2019t have to deal with pods manually. You can just declare the desired state of the system, and it will be managed for you automatically.", "Using the concepts described above, you can create a cluster of nodes, and launch deployments of pods onto the cluster. There is one last problem to solve, however: allowing external traffic to your application.", "By default, Kubernetes provides isolation between pods and the outside world. If you want to communicate with a service running in a pod, you have to open up a channel for communication. This is referred to as ingress.", "There are multiple ways to add ingress to your cluster. The most common ways are by adding either an ", " controller, or a ", ". The exact tradeoffs between these two options are out of scope for this post, but you must be aware that ingress is something you need to handle before you can experiment with Kubernetes.", "What\u2019s described above is an oversimplified version of Kubernetes, but it should give you the basics you need to start experimenting. Now that you understand the pieces that make up the system, it\u2019s time to use them to deploy a real app. Check out", " to get started.", "To experiment with Kubernetes locally, ", " will create a virtual cluster on your personal hardware. If you\u2019re ready to try out a cloud service ,", " has a collection of ", " to get you started.", "If you are new to the world of containers and web infrastructure, I suggest reading up on the", ". This describes some of the best practices to keep in mind when designing software to run in an environment like Kubernetes.", "Finally, for more content like this, make sure to follow me here on Medium and on "]]}
{"title": "Hey Bard, write a responsive Javascript Search Engine app for me..", "twitter_creator": "@sreccardo", "twitter_image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*GRWdYdfz_8efocft", "content": [["Listen", "Share", "I suck at Javascript. \ud83d\ude12 It\u2019s time to admit it. \ud83d\ude05 Let\u2019s see how ", " helped me overcome this limitation\u2026 and create ", "!", "I tried for years to cope and survive in a world which seems to be unable to exist without Javascript, and last weekend I succumbed. Time to ask your favorite JS developer ", " to point you at some good documentation and start reading about this weird language.", " unless? Wait, what if an ", " could code the app for me? Maybe I\u2019m not good to start the project myself, but what if ", " can do it for me, I should be able to tweak the code myself.", " If you don\u2019t want to read my Italian drama, ", " (or see the ", ").", "Two days ago, I was babysitting my kids; playing with my mobile phone while they were playing with Lego\u2026", ".. and I started asking Bard a question. But wait, ", " I was quite surprised by the answer.", "What\u2019s my idea? Say ", ". But you can\u2019t accept a 1\u20132 seconds latency from a typical server-side website search.", "I want a search bar where I can start typing stuff and the choice starts to diminish. Then I want to have a number of cards with the results (from a static JSON \u2014 I\u2019ll make it dynamic in a next episode!). I also want to have some rudimental clicking to narrow down the choice, say with a Boolean \u201cis this a technical article).", "My kids are delving into plenty of Legos, so I have plenty of time to try a verbose and well planned prompt. After all, I want it to write an app for me! Let\u2019s not save time and details.", "Prompt 1: \u201c", "\u201d", "I\u2019m so ignorant on JS that I\u2019m not even sure this is something you can do fully on client-side ( ", "+ ", ") or if you need some server-side effort (add ", "to the mix) to provide the JSON (although on the server-side I have 20-year expertise \u2014 this part I can do without help).", "Now, Bard started with a confident \u201c", "\u201d but I don\u2019t trust what I see. Plus, I don\u2019t have a computer with me, so I can\u2019t try the code. I asked him to do two things that Google is really good at doing:", "Look:", "When I was home I had plenty to choose from: my \ud83d\udcec GMail inbox, my \ud83d\udcc3 Google Docs link. what I didn\u2019t know is that the conversation is saved conveniently in a top left tab, and you can pin it \u2014 until you\u2019re done with your project.", "I paste the JS / HTML code and\u2026 it works FROM SCRATCH! Teh first tentative works! Wow I\u2019m impressed. I start putting the ", " for posterity and for making potentially breaking changes.", "Prompt2. \u201c", "Result:", "Note that Bard gives me the ", " code, the ", "code , and also a pre-baked JSON which responds to my requirements (a ", " boolean and a float ", ").", "Prompt 3. ", "This made the final visualization a bit better (not much).", "Prompt 4. ", "And this is the first time Bard makes a mistake. He gives me a HTML code which seems like ", " (", " ):", "The app is currently served by cloud run here: ", "This is the final result in GIF format:", "The functionalities it demonstrates are:", "As a person who can\u2019t even code a ", " in Javascript, I was able to create a simple / dumb \u2014 yet customized \u2014 website ", " on a Saturday afternoon (when my wife was babysitting).", " gave me good code from the beginning, and responded correctly to the next 2 questions. The first error was at the 4th prompt (!). Also, on the negatives, the CSS for cards was pretty ugly so I had to fix it myself (luckily I speak some rudimental ", ").", "I found Bard can be a pretty good companion if you have time to kill away from keyboard, and you want to go with the flow in some brainstorming phase, ready to copy the content when you\u2019re back to your workstation."]]}
{"title": "A Tale of Two Functions : Function calling in Gemini", "twitter_creator": "@iRomin", "twitter_image": "https://miro.medium.com/v2/resize:fit:516/0*fLheUnd_9lJrbzIH.png", "content": [["Google Cloud - Community", "Listen", "Share", "Building a Generative AI application comes with its own set of interesting challenges. This is especially true when you are trying to use the foundation models in combination with your own data. I am sure that you would agree if you could use the power of natural language understanding and generation of a foundation model and use that to infuse its responses with your own data served via your applications APIs, it would make for a very powerful combination.", ", as they call it, is the ability to tell the model to determine which of the functions should be called to meet the request that you have sent to it, via a prompt. I probably made a terrible attempt to describe the feature in my own words but let me augment that with what some of the vendors say:", "While there are several articles and tutorial on Function calling, my goal in this blog post is the following:", "This by no means is a definitive tutorial on this subject and my goal is to highlight how it works step by step and convince myself that it works too. All the code samples are in Java language (not Python). Its not about a language preference here but I\u2019d like to bump up more samples in Java language. If you choose any other language for which the Vertex AI client library is available, you should ideally be able to port the code accordingly.", "If you prefer to jump to the code, you can hop over to the Github repository on the same.", "github.com", "I have found it a bit difficult to wrap my head around Function calling, when articles define the feature instead of using an example straight up (which I have been guilty of doing in this article too :-)). So let\u2019s understand what we are trying to do first.", "Consider that I have an inventory API that has been designed, implemented and deployed for my distribution company. The inventory API provides me information on the following two high level entities:", "For simplicity, let us consider two magical functions (the implementation is not important here), that have the following signatures:", "Given this, we would like to create a Chatbot application for the organization, that allows anyone to use any of the following prompts to get the information on either the warehouse or specific inventory available at a specific warehouse location.", "Sample prompts that one can provide are given below:", "You would agree that if we simply give these prompts to a foundation model, it is going to respond saying that it is not able to answer this question or worse, it might even end up giving hallucinating.", "What we want instead is a way for the model to interpret the prompt and determine that our functions/tools/APIs (pick your terminology) need to be invoked and then the response needs to be formatted back and given back as the prompt response.", "Two points arise over here:", "More questions follow and for that let us consider the scenarios via the sample prompt.", "Where is warehouse ", " ", "?\u201d", "This prompt clearly indicates that the LLM needs to consider calling the ", " function with ", " as the value for the input parameter ", ".", "How ", " of ", " and ", " do we have in warehouse ", "?", "This would require ", "invocations of ", " function, once with ", " of ", "and ", " of ", " and next with ", " of ", "and ", " of ", ".", "Where is ", " ", " located and how ", " unit of ", " are there?", "This is an interesting scenario. This would require ", "function calls. 2 times for ", " and 1 time for ", "Still more questions arise.", "Interesting possibilities, isn\u2019t it? My colleague ", "What if you could give a prompt that says:", "Now that we are clear on what we are trying to do and the different combinations, let us revisit what Function Calling is.", "I will take the liberty of borrowing information from a couple of documentation sources here. First up is the essence of what Function calling is and I really like the ", " that Anthropic provides and I reproduce it here. You can ", " or Claude with your favorite foundation LLM that supports function calling. I have modified the steps a bit to keep them simple.", "Using tools with Claude involves the following steps:", "1. Provide Claude with tools and a user prompt: (API request)", "Define the set of tools you want Claude to have access to, including their names, descriptions, and input schemas.", "Provide a user prompt that may require the use of one or more of these tools to answer, such as ", "2. Claude uses a tool: (API response)", "Claude assesses the user prompt and decides whether any of the available tools would help with the user\u2019s query or task. If so, it also decides which tool(s) to use and with what inputs.", "Claude constructs a properly formatted tool use request.", "3. Extract tool input, run code, and return results: (API request)", "On the client side, ", " with a new ", " message containing a ", " content block.", "4. ", "After receiving the tool results, Claude will use that information to formulate its final response to the original user prompt.", "Steps (3) and (4) are optional \u2014 for some workflows, Claude using the tool is all the information you need, and you might not need to return tool results back to Claude.", "If you are a visual person, I reproduce the diagram from the official ", ".", "Let\u2019s try to answer some of the questions that we had earlier.", "The source code for these experiments is all available ", ".", "github.com", "This is a standard Maven project with a ", " in the root folder, so you should be able to take the dependencies from there, should you want to recreate it in the different way.", "There are two Java main programs available:", "1.", " and", "2. ", "To run the programs, you will need to do the following in the respective Java files:", "We will come to each of the above programs and understand them in the following respective two sections.", "The source code file for this is :", "When I started off with my experiments on Function calling, I was using the Gemini Pro 1.0 model. We will focus on this model in this section.", "We will then look at what Gemini Pro 1.5 model enabled (Parallel Function calling) in the next section. This will help us understand some additional questions that we had raised in an earlier section and which I reproduce here:", "Let\u2019s understand the source code first:", "For the prompt ", ", we get the following output. You can see that we are being asked by the model to invoke the ", " method twice. Once for ", " and the other for ", ":", "For the prompt ", ", we get the following output. You can see that we are being asked by the model to invoke the ", " method first with the warehouse location as ", " and then we are asked to invoke the ", " method with the ", " as ", " and the ", " as ", ":", "So as you can see over here, the Gemini 1.0 Pro model is doing great. However it asks us to invoke the Functions one after the other.", "Enter Parallel Function calling.", "You would have noticed that the model response gives us a sequence of function calls to make one after the other. But if you look at it, it could have just determined that the functions can be invoked in parallel and could have given us a list of functions to invoke in the first original response itself. For e.g. if we had provided the following prompt: ", ", it should have given us two function calls that we need to make to getWarehouseLocation, one with warehouse location as ", " and the other with warehouse location as ", ". In that case, we could have made both the API calls ourselves, collected the response and given it back to the model in one shot to form the final response.", "From Gemini 1.5 Pro and Gemini 1.5 Flash models, the ", ". This means that we will need to modify our code to expect not just one function call or multiple ones that we will then need to make before handing the API results from those function calls back to the model. The documentation does highlight a ", ".", "I have provided another Java program in ", " and you will notice that we use another model here: ", ".", "If you now run the following prompt in the sample, ", " , you will find the response output is now as follows:", "You can see that in the initial response itself, we have 2 parts that are function calls. We can now parse this response and make the function calls in parallel and return back the response to the model for the final output.", "The code also needed to be changed to iterate through each of the ", " responses and not just the first one. We iterate through the ", ", invoke the function, collect the response and then send the aggregated response from our function call back into the model for the final response.", "Hope this demonstrates the Function calling feature in LLMs, specifically in Gemini.", "I don\u2019t want to rehash the documentation over here since the page could get updated over time but there are some excellent best practices listed over here vis-a-vis Function calling. These range from giving clear descriptions of your functions, parameters and more. Check it ", ".", "cloud.google.com", "Since Function calling is available across major LLM vendors and they have been releasing models at a pace that is unprecendeted, surely there has to be a project that is tracking this capability across LLMs and benchmarking them? Yes \u2014 there is and it is the Berkeley Function Calling Leaderboard. Check out the project and how your favorite LLM model compares to others.", "The Berkeley Function Calling Leaderboard (also called Berkeley Tool Calling Leaderboard) evaluates the LLM\u2019s ability to call functions (aka tools) accurately. This leaderboard consists of real-world data and will be updated periodically.", "gorilla.cs.berkeley.edu"]]}
{"title": "RAG one-liners: curling a website with Gemma", "twitter_creator": "@sreccardo", "twitter_image": "https://miro.medium.com/v2/resize:fit:886/1*wPBSVZUUaFifvYxz_umYDA.png", "content": [["DevOps.dev", "Listen", "Share", "I recently did a ", " (", ") demo in Ruby (interested? Check my ", " and ", "). However, when I talk about RAG with my practitioners, they usually don\u2019t know where to start. I thought to myself (yes, I tend to speak to myself a lot, especially when I cycle or swim):", "\u2014 \u201cHey, RAG should be as easy as: ", "!\u201d (*)", " \u2014", "\u2014 \u201cIs it not obvious? ", "So here we are! I\u2019m going to show you a ", " which is useful in my triathlete daily routine, and hopefully is relateable to you:", "In a nutshell, LLMs have a two problems:", "How do you fix this? By injecting vetted/recent data to address (1) and (2) respectively.", "In a nutshell, you \u201cG", "\u201d and add it to the prompt. This seems complicated, but it\u2019s not. It\u2019s something like this:", " (", ") is just another prompt, where you inject your context somewhere in the middle (hoping the LLM will prioritize that knowledge over the previous one). And yes, the three dashes are really important (since prompt are in plain English, and not in JSON/YAML/", ").", "Let\u2019s say you you want to talk to a Website and ask a question.", "I\u2019m a swimmer and I love swimming in the lake in summer. However, water temperature is not advertised as well as air temperature via APIs. You have a bunch of German websites with some tables which change every year \u2014 so it doesn\u2019t seem smart to build a proper parser there. I\u2019ll let the LLM handle the complexity.", "A simple googling of \u201c", "\u201d fetches some websites:", "You don\u2019t speak German? Welcome to my world!", "My favourite \u201c", "\u201d (Swiss term for ", ") which are close to my home are Seebad Enge (with fancy and elegant vibe, evening party, .. opens at 8am) and Seebad Utoquai (for Ironmen and grannies, opens at 7am). They\u2019re in front of each other but, due to current, they often have 1\u00b0C difference (~half 1\u00b0F if you insist with that imperial nonsense).", "To run this, I will not use a Google Cloud product, for once, I\u2019ll just use:", "I\u2019ve tried Ollama on a long intercontinental flight to the Philippines and it changed my life: ", " Sign me up for this! And you can get open models from Facebook, Google, Microsoft, Mistral, OpenAI, .. I also love to create cheap embeddings with a model like ", " , my favorite.", "And you\u2019re good to go!", "So here\u2019s a prompt I refined after some trial and error (", "):", "I created some simple ", " wrapper in Ruby \ud83d\udc8e (you can of course use your own favorite language1 It\u2019s so small and simple you won\u2019t be able to resist porting in your fav language!) in ", ".", "Here\u2019s how the execution looks like for", " :", "Is it correct? Let\u2019s check:", "\ud83d\udc4d As you can see the output is as I expected. Note that if you prefer Fahrenheit, it\u2019s a trivial change to the prompt! remember:", "So you\u2019re a lazy developer! I like you, we should be friends!", "RAG can be as simple as Retrieve and Ask a question, so it can becomes as simple as: ", ":", "I would have broken it down to two lines for readability but \u2014 a challenge is a challenge! \ud83d\ude03", "Wait a minute \u2014 is it really 17\u00b0C? Or did they just hallucinate?", "Yup, seems like ", "! And TA-DA!"]]}
{"title": "Gemma is born!", "twitter_creator": "@sreccardo", "twitter_image": "https://miro.medium.com/v2/resize:fit:925/1*zktJLAqzgGa4ISoynpyMSQ.jpeg", "content": [["Google Cloud - Community", "Listen", "Share", "As a rubyist, I love gems.", "So imagine my delight and surprise when last week ", ", a family of open models available directly on `Kaggle`: ", ". Plus, it\u2019s ", "!", "If you\u2019re like me, a CLI kind of person, fear no more! I found a way to call Gemma with cURL (", " \u2014 said a wise man in Dodgeball \u2014 did he?).", "Don\u2019t believe me? Check out my ugly bash code here: ", "/", "I haven\u2019t tried that one out, I\u2019m sorry. It\u2019s in my todo list.", "As Barney would say ", "I got hooked by ", " by Mark Ryan which got me started.", "Please note that in this case you need ", " and have billing enabled. You need:", "1. A Gmail account (Google Account)", "2. A Google Cloud Project with ", ".", "If this doesn\u2019t scare you off, the easiest way is to:", "Once done, you should have something like this:", "Glad you asked!", "It took me one hour of digging but I can tell you, the JSON can be as simple as this:", "The output will look like this:", "So writing a shell wrapper is as easy as ", "plus a", "(latest code ", "):", "Deploying Gemma to Vertex AI and calling the model via curl is a ", ".", "And you, what are you waiting for?"]]}
